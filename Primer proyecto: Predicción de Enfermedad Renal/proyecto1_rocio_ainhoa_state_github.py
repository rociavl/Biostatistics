# -*- coding: utf-8 -*-
"""Proyecto1_Rocio_Ainhoa_state_GitHub.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DQy_0hcSD2zaax_iwC3y8MJs_SEpQv4y

# Primer proyecto: Predicci√≥n de Enfermedad Renal - An√°lisis, Modelado y Evaluaci√≥n

## 1. Introducci√≥n

La Enfermedad Renal Cr√≥nica (ERC) es un problema de salud p√∫blica global con una creciente prevalencia. Su detecci√≥n temprana es clave para mejorar la calidad de vida de los pacientes y optimizar los tratamientos. Este c√≥digo implementa un pipeline completo de ciencia de datos con el objetivo de desarrollar modelos predictivos eficientes para la identificaci√≥n y clasificaci√≥n de la ERC.

Para ello y para tener una mejor comprensi√≥n del contenido del c√≥digo, hemos decidido dividirlo en cinco puntos clave, facilitando as√≠ su an√°lisis y desarrollo. A lo largo del proyecto se aplicar√°n t√©cnicas avanzadas de an√°lisis exploratorio de datos (EDA), preprocesamiento, modelado y evaluaci√≥n de algoritmos de aprendizaje autom√°tico.

Por otro lado, siguiendo las directrices establecidas por el profesorado, a nuestro grupo se le ha asignado la tarea de regresi√≥n utilizando la variable "bu" (urea en sangre) como objetivo, lo que permitir√° analizar su relaci√≥n con otras variables cl√≠nicas relevantes. Asimismo, se nos ha encomendado la comparaci√≥n de tres modelos distintos para la clasificaci√≥n:
*   Linear Discriminant Analysis (LDA).
*   Quadratic Discriminant Analysis (QDA).
*   Support Vector Machine (SVM) con funci√≥n de activaci√≥n sigmoidal.

Finalmente, conforme a la asignaci√≥n, implementaremos un modelo basado en Boosting para ensamblar m√∫ltiples clasificadores y mejorar el rendimiento predictivo.

## 2. An√°lisis exploratorio de datos (EDA)

### 2.1. Procesamiento de datos
En esta secci√≥n, realizamos la carga y exploraci√≥n inicial de los datos relacionados con la enfermedad renal cr√≥nica (ERC). El objetivo es familiarizarnos con las caracter√≠sticas del conjunto de datos y garantizar que est√©n correctamente estructurados para su posterior an√°lisis.

Para ello, lo primero que debemos hacer es importar la biblioteca necesaria para cargar archivos en Google Colab y cargar el archivo de datos `kidney_disease.csv` desde nuestro almacenamiento local.
"""

from google.colab import files
datos = files.upload()
print(datos)

"""A continuaci√≥n instalamos todas las bibliotecas necesarias a lo largo del c√≥digo."""

!pip install pandas
!pip install tabulate
!pip install seaborn
!pip install matplotlib
!pip install missingno
!pip install missingpy

"""El siguiente paso es importar la biblioteca `Pandas`, esencial para el manejo de dataframes y an√°lisis de datos.

Una vez hecho esto, cargamos el dataset en un DataFrame de Pandas. Durante este proceso, especificamos el par√°metro `index_col='id'` para que la columna de identificaci√≥n no sea tratada como una variable regular dentro del an√°lisis.

Finalmente, una vez cargado el conjunto de datos, realizamos una exploraci√≥n inicial mostrando las primeras 18 filas del DataFrame. Esto nos permite obtener una visi√≥n general de su estructura, identificar posibles valores nulos o inconsistencias, y comenzar a comprender la distribuci√≥n de las variables.
"""

import pandas as pd

kidney_df = pd.read_csv('kidney_disease.csv', index_col='id')
kidney_df.head(18)

"""Observando el DataFrame, podemos apreciar que las caracter√≠sticas principales, que incluyen tanto mediciones cl√≠nicas como resultados de laboratorio, son las siguientes:

*   **age:** edad del paciente.
*   **bp:** tensi√≥n arterial (en mm/Hg).
*   **sg:** peso espec√≠fico de la orina.
*   **al:** niveles de alb√∫mina en orina.
*   **su:** niveles de az√∫car en orina.
*   **rbc:** gl√≥bulos rojos en orina (normal/anormal).
*   **pc:** c√©lulas de pus en orina (normal/anormal).
*   **pcc:** ac√∫mulos de c√©lulas de pus (presentes/no presentes).
*   **ba:** bacterias en orina (presente/no presente).
*   **bgr:** glucosa en sangre aleatoria (en mgs/dl).
*   **bu:** urea en sangre (en mgs/dl).
*   **sc:** creatinina s√©rica (en mgs/dl).
*   **sod:** niveles de sodio (en mEq/L).
*   **pot:** niveles de potasio (en mEq/L).
*   **hemo:** niveles de hemoglobina (en gms).
*   **pcv:** volumen celular empaquetado.
*   **wc:** recuento de gl√≥bulos blancos.
*   **rc:** recuento de gl√≥bulos rojos.
*   **htn:** hipertensi√≥n (s√≠/no).
*   **dm:** diabetes mellitus (s√≠/no).
*   **cad:** enfermedad coronaria (s√≠/no).
*   **appet:** apetito (bueno/deficiente).
*   **pe:** edema pedio (s√≠/no).
*   **ane:** anemia (s√≠/no).
*   **classification:** clase (ckd/notckd) que indica la presencia o ausencia de enfermedad renal cr√≥nica.


Adem√°s, durante esta primera exploraci√≥n de los datos, se pueden identifican valores faltantes (NaN) en varias variables. Esto se puede deber a registros incompletos o errores en la recopilaci√≥n de datos. El tratamiento de estos datos es esencial para garantizar la calidad del an√°lisis y el desempe√±o de los modelos predictivos.

### 2.2. An√°lisis exploratorio de las caracter√≠sticas del dataset

En este apartado, nos centraremos en comprender la estructura y propiedades del conjunto de datos. Este proceso nos puede ser √∫til para detectar patrones, identificar posibles problemas, y obtener informaci√≥n relevante sobre la distribuci√≥n de las variables.

El primer paso a realizar es importar la librer√≠a `tabulate`, que permite formatear y mostrar tablas de manera m√°s legible.
"""

from tabulate import tabulate

"""El codigo que encontramos en las celdas que hay a continuaci√≥n nos proporciona informaci√≥n clave sobre su estructura, tipos de variables y estad√≠sticas descriptivas.

Lo primero que hacemos es imprimir el n√∫mero de filas y columnas que contiene el dataset.
"""

print(f"\nüîπ Forma del dataset: {kidney_df.shape}")

"""Seguidamente, utilizando el m√©todo `.info()` obtenemos un resumen con los siguientes datos:

*   El n√∫mero total de entradas (filas).
*   El n√∫mero de valores no nulos en cada columna.
*   Los tipos de datos de cada variable.



"""

print("\nüîπ Informaci√≥n del dataset:")
kidney_df.info()

"""Para visualizar si las variables son num√©ricas, categ√≥ricas o de otro tipo, creamos un DataFrame temporal con los tipos de datos de cada columna y lo mostramos en un formato tabular usando `tabulate`."""

print("\nüîπ Tipos de datos de cada columna:")
dtypes_table = pd.DataFrame(kidney_df.dtypes, columns=["Tipo de Dato"])
print(tabulate(dtypes_table, headers="keys", tablefmt="pretty"))

"""Adem√°s, para generar las estad√≠sticas descriptivas de todas las columnas hemos utilizado el m√©todo `.describe(include='all')`, el cual nos devuelve:

*   Media, mediana y desviaci√≥n est√°ndar en variables num√©ricas.
*   Conteo de valores √∫nicos en variables categ√≥ricas.
*   Valores m√≠nimo y m√°ximo en variables num√©ricas.
*   Cuartiles (25%, 50%, 75%).

En el c√≥digo, la tabla se transpone para facilitar su lectura.



"""

print("\nüîπ Descripci√≥n estad√≠stica del dataset:")
describe_table = kidney_df.describe(include='all').transpose()
print(tabulate(describe_table, headers="keys", tablefmt="fancy_grid"))

"""En la siguiente celda, se trabaja con ambos grupos de datos, num√©ricos y categ√≥ricos, con el objetivo de obtener un mejor conocimiento de la estructura del dataset.

Lo primero que hacemos es convertir ciertas variables categ√≥ricas a formato num√©rico, ya que parece que algunas se han cargado como texto debido a errores en los datos. Para ello:

*   Se convierten las columnas "pcv" (volumen celular empaquetado), "wc" (recuento de gl√≥bulos blancos) y "rc" (recuento de gl√≥bulos rojos) a formato num√©rico con `pd.to_numeric()`.
*   El par√°metro `errors='coerce'` transforma valores inv√°lidos en NaN, evitando errores en el an√°lisis.

Seguidamente, identificamos ambos grupos de variables con las siguientes l√≠neas de c√≥digo:

*   `select_dtypes(include=['number'])` selecciona todas las columnas num√©ricas.
*   `select_dtypes(include=['object'])` selecciona todas las columnas categ√≥ricas.

Finalmente, calculamos el porcentaje de columnas num√©ricas con respecto al total de variables en el dataset. Esto permite conocer la proporci√≥n de datos num√©ricos y categ√≥ricos, lo que puede influir en la selecci√≥n y el entrenamiento de modelos.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convierte pcv, wc y rc en num√©ricos si actualmente son objetos
kidney_df['pcv'] = pd.to_numeric(kidney_df['pcv'], errors='coerce')
kidney_df['wc'] = pd.to_numeric(kidney_df['wc'], errors='coerce')
kidney_df['rc'] = pd.to_numeric(kidney_df['rc'], errors='coerce')
numerical_cols = kidney_df.select_dtypes(include=['number']).columns
categorical_cols = kidney_df.select_dtypes(include=['object']).columns
print("Variables num√©ricas:", numerical_cols.tolist())
print("Variables categ√≥ricas:", categorical_cols.tolist())

# Calcular porcentaje de variables num√©ricas
percentage_numerical = (len(numerical_cols) / len(kidney_df.columns)) * 100
print(f"\nüìä Porcentaje de variables num√©ricas: {percentage_numerical:.2f}%")

"""Para obtener √∫nicamente las estad√≠sticas descriptivas de las variables num√©ricas, empleamos `.describe().T` sobre las columnas num√©ricas (`numerical_cols`) previamente definidas, obteniendo estad√≠sticas como: media, mediana, desviaci√≥n est√°ndar, m√≠nimo, m√°ximo y cuartiles.

Adem√°s, para mejorar la visualizaci√≥n de los resultados, aplicamos los siguientes formatos:

*   `.style.format("{:,.2f}")` para formatear la tabla con dos decimales.
*   `.background_gradient(cmap="Blues")` a√±ade un degradado en tonos azules, resaltando valores m√°s altos o bajos.
*  `display(numerical_stats)` permite mostrar la tabla de manera m√°s visual, especialmente en Jupyter Notebook.
"""

pd.options.display.float_format = "{:,.2f}".format

# Tabla de datos estad√≠sticos
numerical_stats = kidney_df[numerical_cols].describe().T
numerical_stats = numerical_stats.style.format("{:,.2f}").background_gradient(cmap="Blues")
print("\nüîπ Estad√≠sticas descriptivas de las variables num√©ricas:")
display(numerical_stats)

"""### 2.3. Matriz de correlaci√≥n

Para identificar las relaciones entre las variables, utilizamos un gr√°fico de calor (heatmap). En este gr√°fico, las variables con valores de correlaci√≥n cercanos a 1 est√°n fuertemente correlacionadas positivamente, mientras que aquellas con valores cercanos a -1 est√°n fuertemente correlacionadas negativamente. En cambio, valores pr√≥ximos a 0 indican ausencia de correlaci√≥n.

La intensidad de la correlaci√≥n se representa con colores, en este caso:
*   Tonos azules indican una correlaci√≥n positiva fuerte.
*   Tonos rojos representan una correlaci√≥n negativa fuerte.
*   Colores intermedios sugieren una correlaci√≥n d√©bil o moderada.


Para generar este gr√°fico, utilizamos el comando `sns.heatmap()` de Seaborn, el cual nos permite visualizar la correlaci√≥n entre las variables num√©ricas. Adem√°s, aplicamos personalizaci√≥n para mejorar su interpretaci√≥n:

*   `kidney_df[numerical_cols].corr()` calcula la matriz de correlaci√≥n entre las variables.
*   `annot=True, fmt=".2f"` muestra los valores de correlaci√≥n con dos decimales dentro del heatmap.
*   `cmap="spectral"` utiliza una paleta de colores explicada anteriormente.
*   `linewidths=0.5, linecolor="gray"` a√±ade l√≠neas de separaci√≥n para mayor claridad.
*   `plt.title("...", fontsize=15, pad=20, fontweight='bold')` agrega un t√≠tulo con fuente m√°s grande, negrita y espaciado adecuado.
*   `plt.xticks(rotation=45)` y `plt.yticks(rotation=0)` ajustan la rotaci√≥n de las etiquetas para mejorar la legibilidad.
"""

plt.figure(figsize=(12, 8))
sns.heatmap(
    kidney_df[numerical_cols].corr(),
    annot=True, fmt=".2f", cmap="Spectral",
    linewidths=0.5, linecolor="gray", cbar=True
)
plt.title("Matriz de correlaci√≥n de variables num√©ricas", fontsize=15, pad=20, fontweight='bold')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.show()

"""Examinando la matriz de correlaci√≥n entre las variables num√©ricas del conjunto de datos, podemos identificar relaciones lineales relevantes entre ellas. Algunas de estas relaciones son:

1. Par√°metros hematol√≥gicos: se observ√≥ una fuerte correlaci√≥n positiva entre las variables `hemo`, `pcv` y `rc`, con coeficientes de correlaci√≥n superiores a 0.79. Esta asociaci√≥n es esperable desde un punto de vista fisiol√≥gico, dado que estos par√°metros suelen variar de forma conjunta en funci√≥n del estado hematol√≥gico del paciente.

2. Funci√≥n renal: la variable objetivo `bu` mostr√≥ una correlaci√≥n negativa moderada con los par√°metros hematol√≥gicos mencionados anteriormente (r ‚âà -0.6). Esta relaci√≥n sugiere que a medida que los niveles de urea aumentan (indicador t√≠pico de deterioro de la funci√≥n renal) tienden a disminuir los valores de estos marcadores hematol√≥gicos, lo cual podr√≠a reflejar la aparici√≥n de anemia asociada a enfermedad renal cr√≥nica. Por otro lado, `bu` se correlaciona positivamente con `sc` (r = 0.59), lo que respalda su validez como indicador del estado renal del paciente, ya que ambas variables aumentan com√∫nmente en presencia de disfunci√≥n renal.

3. Electrolitos: se evidenci√≥ una correlaci√≥n negativa significativa entre `sc` y `sod` (r = -0.69). Esta relaci√≥n puede estar vinculada a alteraciones hidroelectrol√≠ticas que suelen acompa√±ar a los trastornos renales, como la hiponatremia dilucional en el contexto de insuficiencia renal.

4. Otras variables: variables como `age`, `bp` y `pot` presentan correlaciones d√©biles o poco significativas con la mayor√≠a de las variables cl√≠nicas, aunque podr√≠an tener relevancia en modelos multivariantes, especialmente al interactuar con otras variables o bajo enfoques no lineales.

### 2.4. Valores corrompidos

En este fragmento de c√≥digo, nos centramos en el tratamiento de variables categ√≥ricas dentro del conjunto de datos. La limpieza y exploraci√≥n de estas variables es fundamental, ya que inconsistencias como espacios en blanco pueden afectar el an√°lisis y la modelizaci√≥n.

Por eso, en el primer bloque de c√≥digo nos centraremos en identificar los valores √∫nicos dentro de las variables categ√≥ricas del dataset. Los pasos para conseguirlo son:

*   Recorrer cada columna categ√≥rica dentro de `categorical_cols` con un bucle for.
*   Finalmente, `kidney_df[col].unique()` obtiene los valores √∫nicos de cada variable categ√≥rica y los imprime.
"""

for col in categorical_cols:
    unique_vals = kidney_df[col].unique()
    print(f"Unique values in {col}: {unique_vals}")

"""Al revisar los valores √∫nicos, observamos valores con espacios y tabulaciones en varias variables categ√≥ricas, como:

*   'ckd\t' en `classification`.
*   '\tno' en `cad`.
*   '\tyes', '  yes' y '\tno' en `dm`.

Estos errores pueden generar inconsistencias al analizar y modelar los datos, ya que 'ckd' y 'ckd\t' ser√°n tratados como valores distintos.

Para solucionar estos problemas, en el siguiente bloque de c√≥digo limpiamos las variables categ√≥ricas eliminando los espacios innecesarios al inicio o final de los valores. Para ello:

*   Definimos `categorical_cols_spac`, que almacena una lista con las variables categ√≥ricas de inter√©s: "cad", "classification" y "dm".
*   Hacemos una copia del DataFrame original para no modificarlo directamente.
*   Aplicamos `str.replace(r'^\s+|\s+$', '', regex=True)` a cada columna categ√≥rica en `categorical_cols_space`, donde:
  *   `^\s+` elimina espacios en blanco o tabulaciones al inicio del texto.
  *   `\s+$` elimina espacios en blanco o tabulaciones al final del texto.
  *   `regex=True` indica que estamos usando expresiones regulares.

Esto garantiza que los valores sean uniformes y no contengan caracteres ocultos. Despu√©s de limpiar los datos, verificamos si los valores han sido corregidos comparando las versiones originales y limpias de cada variable:

*   Recorremos las mismas columnas categ√≥ricas.
*   Imprimimos los valores √∫nicos antes y despu√©s de la limpieza para comparar.
*   Mostramos un fragmento del DataFrame limpio con `display(cleaned_kidney_df.head())` para corroborar que la limpieza se ha realizado correctamente.
"""

categorical_cols_space = ['cad', 'classification', 'dm']
cleaned_kidney_df = kidney_df.copy()

for col in categorical_cols_space:
    cleaned_kidney_df[col] = cleaned_kidney_df[col].str.replace(r'^\s+|\s+$', '', regex=True)

for col in categorical_cols_space:
    unique_vals_original = kidney_df[col].unique()
    unique_vals_cleaned = cleaned_kidney_df[col].unique()
    print(f"Original unique values in {col}: {unique_vals_original}")
    print(f"Cleaned unique values in {col}: {unique_vals_cleaned}\n")

display(cleaned_kidney_df.head())

"""Para concluir el an√°lisis de las variables categ√≥ricas, hemos decidido recopilar la informaci√≥n en una tabla resumen. Esta tabla nos permitir√° visualizar de manera estructurada las caracter√≠sticas principales de cada variable categ√≥rica en el dataset ya que nos proporciona:

*   Nombre de la variable
*   Valores √∫nicos: lista de los valores distintos que toma la variable.
*   Distribuci√≥n de valores: muestra la cantidad y porcentaje de registros que corresponden a cada categor√≠a.
"""

categorical_cols = cleaned_kidney_df.select_dtypes(include=['object']).columns
summary_df = pd.DataFrame(columns=['Variable', 'Valores √∫nicos', 'Distribuci√≥n de valores'])

for i, col in enumerate(categorical_cols):
    unique_vals = cleaned_kidney_df[col].unique()
    value_counts = cleaned_kidney_df[col].value_counts().sort_values(ascending=False)
    unique_values_str = ', '.join(map(str, unique_vals))

    total = len(cleaned_kidney_df)
    value_counts_str = '<br>'.join([
    f"{val}: {count} ({count/total:.2%})"
    for val, count in value_counts.items()
    ])

    summary_df.loc[i] = [
    col,
    unique_values_str,
    value_counts_str
    ]

styled_table = summary_df.style.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})

styled_table = styled_table.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

styled_table

"""### 2.5. Recursos visuales

En este apartado, nos enfocaremos en la creaci√≥n de visualizaciones variadas y efectivas para analizar y mostrar los hallazgos del conjunto de datos de manera clara y comprensible.

El uso de gr√°ficos nos permitir√° identificar patrones, tendencias y posibles relaciones entre las variables, facilitando as√≠ la interpretaci√≥n de los datos. Esto ser√° clave para detectar valores at√≠picos, evaluar la distribuci√≥n de los datos y obtener informaci√≥n valiosa que contribuir√° al desarrollo de modelos predictivos eficientes.

#### 2.5.1. Variables num√©ricas
A continuaci√≥n, se presenta un c√≥digo para generar diagramas de caja (boxplots) de todas las variables num√©ricas del dataset utilizando la funci√≥n `sns.boxplot()`. Estos diagramas son herramientas esenciales para el an√°lisis exploratorio de datos, ya que permiten visualizar la distribuci√≥n de las variables y detectar posibles anomal√≠as. Con ellos, podemos observar:

*   Mediana: representada por la l√≠nea dentro del cuadro, indica el valor central de la distribuci√≥n.
*   Media: representada por un punto naranja en nuestro caso, indica el valor promedio.
*   Cuartiles (Q1 y Q3): dividen los datos en cuatro partes iguales, proporcionando informaci√≥n sobre la dispersi√≥n de los valores.
*   Rango intercuartil (IQR = Q3 - Q1): mide la variabilidad de los datos, excluyendo valores extremos.
*   Valores at√≠picos (outliers): se representan como puntos fuera de los "bigotes" del diagrama y pueden indicar errores o casos excepcionales en los datos.
"""

num_vars = len(numerical_cols)
fig_height = max(12, num_vars * 0.5)
fig_width = 15
numerical_stats = kidney_df[numerical_cols].describe()
plt.figure(figsize=(fig_width, fig_height))

# Diagramas de caja individuales para cada columna num√©rica
for i, col in enumerate(numerical_cols, 1):
    plt.subplot((num_vars // 3) + 1, 3, i)
    sns.boxplot(data=kidney_df, x=col, orient='h', width=0.6, color="#588157")

    # T√≠tulo y etiquetas
    plt.title(f'Distribuci√≥n de "{col}"', fontsize=12, pad=15, fontweight='bold')
    plt.xlabel(f'{col}', fontsize=10)
    plt.ylabel('')
    plt.grid(axis='x', linestyle='--', alpha=0.6)

    mean_value = numerical_stats.loc['mean', col]
    plt.scatter(mean_value, 0, color='#fb8b24', zorder=10, label='Media', s=100, marker='o')
    if i == len(numerical_cols):
        plt.legend()

plt.tight_layout()
plt.show()

"""Para comprender mejor la distribuci√≥n de nuestras variables num√©ricas, hemos optado por representarlas mediante histogramas. Este tipo de gr√°fico nos permite analizar la frecuencia con la que aparecen ciertos valores dentro de los datos, proporcionando informaci√≥n valiosa sobre su forma y dispersi√≥n.

El siguiente bloque de c√≥digo genera histogramas, utilizando `sns.histplot()`, para todas las variables num√©ricas del dataset. Adem√°s, hemos decidido superponer una curva de densidad para visualizar mejor la distribuci√≥n.

Para hacer un mejor an√°lisis, hemos dibujamos l√≠neas verticales para la media (l√≠nea punteada naranja) y la mediana (l√≠nea solida roja), lo que permite comparar la simetr√≠a de la distribuci√≥n.
"""

mean_vals = kidney_df[numerical_cols].mean()
median_vals = kidney_df[numerical_cols].median()

plt.figure(figsize=(18, 14))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot((len(numerical_cols) // 3) + 1, 3, i)
    sns.histplot(kidney_df[col].dropna(), kde=True, bins=30, color="#588157", alpha=0.7)

    # Media y mediana
    plt.axvline(mean_vals[col], color='#fb8b24', linestyle='--', label=f'Media ({mean_vals[col]:.2f})')
    plt.axvline(median_vals[col], color='#9d0208', linestyle='-', label=f'Mediana ({median_vals[col]:.2f})')
    plt.title(f'Distribuci√≥n de "{col}"', fontsize=12, pad=15, fontweight='bold')
    plt.xlabel(f'{col}', fontsize=10)
    plt.legend(loc='upper right', fontsize=10)

plt.tight_layout()
plt.show()

"""#### 2.5.2. Variables categ√≥ricas
Ahora nos enfocaremos en las variables categ√≥ricas del dataset y, para comenzar, exploraremos su distribuci√≥n mediante gr√°ficos de barras (countplot). Este tipo de visualizaci√≥n nos permitir√° comprender la frecuencia con la que aparecen las distintas categor√≠as en cada variable, lo cual es fundamental para detectar posibles desequilibrios en los datos.

Para ello, utilizaremos el dataset limpio de inconsistencias y la funci√≥n `sns.countplot()`. Adem√°s, para mejorar la visualizaci√≥n, aplicaremos una paleta de colores personalizada, lo que permitir√° diferenciar claramente cada categor√≠a.
"""

categorical_cols_cleaned = cleaned_kidney_df.select_dtypes(include=['object']).columns # Datos limpios
palette_colour = ['#b23a48', '#0e9594']

plt.figure(figsize=(14, 10))
for i, col in enumerate(categorical_cols_cleaned, 1):
    plt.subplot((len(categorical_cols_cleaned) // 3) + 1, 3, i)
    sns.countplot(x=cleaned_kidney_df[col], hue=cleaned_kidney_df[col], palette=palette_colour, legend=False)
    plt.title(f'Distribuci√≥n de "{col}"', fontsize=12, pad=15, fontweight='bold')
    plt.xlabel('')
    plt.ylabel('Frecuencia', fontsize=10)
    plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

"""Como complemento a este an√°lisis, tambi√©n realizaremos gr√°ficos de pastel (pie charts) utilizando `plt.pie()`. Para mantener la coherencia visual, emplearemos la misma paleta de colores utilizada en los gr√°ficos de barras.

Los gr√°ficos de pastel nos ayudar√°n a representar la proporci√≥n de cada categor√≠a dentro de una variable, permitiendo identificar de forma intuitiva si alguna categor√≠a predomina sobre las dem√°s o si la distribuci√≥n es equilibrada.



"""

plt.figure(figsize=(18, 14))
for i, col in enumerate(categorical_cols_cleaned, 1):
    plt.subplot((len(categorical_cols_cleaned) // 3) + 1, 3, i)
    category_counts = cleaned_kidney_df[col].value_counts()
    plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=palette_colour)
    plt.title(f'Proporci√≥n de "{col}"', fontsize=12, pad=15, fontweight='bold')

plt.tight_layout()
plt.show()

"""Para finalizar la representaci√≥n de las variables categ√≥ricas, realizaremos un conteo de frecuencia de cada categor√≠a dentro de las variables categ√≥ricas del dataset limpio. Esto nos permitir√° identificar posibles desequilibrios en la distribuci√≥n de categor√≠as y detectar valores poco frecuentes que podr√≠an afectar el an√°lisis o el modelado.

Para lograrlo, utilizamos `apply(lambda x: x.value_counts())`, que calcula el n√∫mero de apariciones de cada categor√≠a en cada variable.
"""

categorical_counts = cleaned_kidney_df[categorical_cols_cleaned].apply(lambda x: x.value_counts())

print("\nüîπ Frecuencia de las variables categ√≥ricas (limpias):\n")
display(categorical_counts)

"""#### 2.5.3. Ausencia de datos
En esta secci√≥n, nos centraremos en la identificaci√≥n y cuantificaci√≥n de valores faltantes en el dataset. Este an√°lisis es fundamental, ya que la presencia de valores nulos puede afectar significativamente la calidad de los resultados y el rendimiento de los modelos predictivos.

Para ello, realizamos las siguientes acciones:

*   Calculamos los valores faltantes (`cleaned_kidney_df.isnull().sum()`) y su porcentaje (`missing_percentage = (missing_data / total_registros) * 100`).
*   Identificamos las columnas con y sin valores faltantes, que corresponden a  `columnas_con_faltantes` y `columnas_sin_faltantes` respectivamente.
*   Creamos un DataFrame resumen con la informaci√≥n detallada sobre cada variable, incluyendo:
    *   N√∫mero total de registros en el dataset.
    *   Cantidad y porcentaje de valores faltantes.
    *   Cantidad y porcentaje de valores v√°lidos (no nulos).
"""

# Calcular valores faltantes y porcentajes
total_registros = len(cleaned_kidney_df)
missing_data = cleaned_kidney_df.isnull().sum()
missing_percentage = (missing_data / total_registros) * 100

# Crear DataFrame de resumen
resumen_faltantes = pd.DataFrame({
    'Variable': missing_data.index,
    'Valores totales': total_registros,
    'Valores faltantes': [f"{val} ({pct:.2f}%)" for val, pct in zip(missing_data, missing_percentage)],
    'Valores v√°lidos': [f"{total_registros - val} ({(100 - pct):.2f}%)" for val, pct in zip(missing_data, missing_percentage)]
})

# Mostrar informaci√≥n resumida
print(f"\nTotal de columnas: {len(cleaned_kidney_df.columns)}")
print(f"Total de registros: {total_registros}")

columnas_con_faltantes = cleaned_kidney_df.columns[missing_data > 0]
print(f"\nN√∫mero de columnas con datos faltantes: {len(columnas_con_faltantes)}")
print(f"Columnas con datos faltantes: {columnas_con_faltantes.tolist()}")

columnas_sin_faltantes = cleaned_kidney_df.columns[missing_data == 0]
print(f"\nN√∫mero de columnas sin valores faltantes: {len(columnas_sin_faltantes)}")
print(f"Columnas sin valores faltantes: {columnas_sin_faltantes.tolist()}")

print("\nüîπ Resumen completo:\n")

styled_table2 = resumen_faltantes.style.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})
styled_table2 = styled_table2.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

styled_table2

"""Para complementar el an√°lisis de valores faltantes, utilizaremos diversas visualizaciones gr√°ficas que nos ayudar√°n a entender mejor la distribuci√≥n de los datos faltantes y detectar patrones de ausencia de los datos del dataset.

En este primer caso, crearemos un mapa de calor de valores faltantes utilizando `seaborn.heatmap()`. Este gr√°fico nos ayuda a visualizar la distribuci√≥n de los valores nulos, lo que facilita la detecci√≥n de patrones en los datos ausentes.

Adem√°s, hemos definido una paleta de colores personalizada para mejorar la interpretaci√≥n del gr√°fico:

*   Azul: representa los valores presentes en el dataset.
*   Amarillo: representa los valores faltantes (NaN).

"""

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Patch

sns.set(style="whitegrid")

paleta_colores = ['#0077b6', '#ffc43d']

plt.figure(figsize=(12, 8))
ax = sns.heatmap(cleaned_kidney_df.isnull(), cmap=paleta_colores, cbar=False, xticklabels=True, yticklabels=False)
legend_labels = [Patch(color='#0077b6', label='Valores presentes'),
                 Patch(color='#ffc43d', label='Valores faltantes')]

plt.legend(handles=legend_labels, bbox_to_anchor=(1.005, 1), loc='upper left', fontsize=12)
plt.title('Mapa de calor de datos faltantes', fontsize=18, pad=15, fontweight='bold')
plt.xlabel('Columnas', fontsize=14)
plt.ylabel('Filas', fontsize=14, labelpad=15)
plt.xticks(fontsize=12, rotation=45)
plt.show()

"""Al examinar el mapa de calor de los datos faltantes, podemos identificar patrones sistem√°ticos de ausencia de informaci√≥n, lo que indica que los datos no est√°n distribuidos de forma aleatoria. Este hallazgo tiene importantes implicancias para el an√°lisis posterior y la construcci√≥n de modelos predictivos.

1. Patrones por columnas: se observa una alta proporci√≥n de valores ausentes en variables como `rbc`, `pc`, `pcc`, `ba`, `sod`, `pot`, `hemo`, `pcv`, `wc` y `rc`. Estas variables est√°n asociadas principalmente a an√°lisis cl√≠nicos de laboratorio, tales como pruebas hematol√≥gicas, de orina y electrolitos. La coincidencia en los patrones de ausencia entre estas columnas sugiere que las pruebas correspondientes no fueron realizadas o registradas en ciertos pacientes. Las causas pueden ser diversas, incluyendo decisiones cl√≠nicas, limitaciones administrativas, o recursos disponibles en el momento de la atenci√≥n.

2. Patrones por filas: se identifican registros de pacientes con m√∫ltiples valores faltantes simult√°neamente, lo que apunta a una ausencia estructurada de informaci√≥n en determinados individuos. Es posible que estos pacientes no hayan sido sometidos a todas las evaluaciones m√©dicas por factores como el estado cl√≠nico, la fase de seguimiento, limitaciones log√≠sticas, o incluso la priorizaci√≥n de pruebas diagn√≥sticas en contextos de atenci√≥n m√©dica limitada.

3. Relaci√≥n potencial con otras variables: aunque el mapa de calor no permite establecer directamente la relaci√≥n con la variable objetivo `classification`, la concentraci√≥n de valores ausentes en variables cl√≠nicas sugiere que podr√≠a haber una relaci√≥n indirecta con el estado general del paciente. Por ejemplo, pacientes con enfermedad menos avanzada podr√≠an no haber requerido ciertas pruebas, mientras que en pacientes con mayor complejidad cl√≠nica se habr√≠an realizado estudios m√°s completos.

El patr√≥n de datos faltantes podria tener ciertas limitaciones:
*   Posible sesgo de selecci√≥n: la ausencia no aleatoria puede introducir sesgos en el an√°lisis, especialmente si se ignora la causa estructural de los datos faltantes. Los modelos predictivos podr√≠an sobreajustarse a subconjuntos no representativos de la poblaci√≥n.
*   P√©rdida de informaci√≥n valiosa: la eliminaci√≥n directa de filas o columnas con grandes cantidades de datos faltantes podr√≠a resultar en la p√©rdida de casos cl√≠nicamente relevantes o de variables predictoras clave.
*   Necesidad de estrategias de imputaci√≥n sofisticadas: dada la naturaleza estructurada de los datos faltantes, lo ideas ser√≠a aplicar t√©cnicas de imputaci√≥n inteligentes y contextualizadas, que consideren la naturaleza de la variable (num√©rica o categ√≥rica) y su relaci√≥n con otras columnas.

En resumen, el an√°lisis de los datos faltantes evidencia patrones sistem√°ticos vinculados a variables cl√≠nicas espec√≠ficas y a determinados subgrupos de pacientes. Debemos gestionar cuidadosamente esta ausencia no aleatoria para as√≠ minimizar sesgos y preservar la integridad de los an√°lisis posteriores.

Moviendonos a la siguiente secci√≥n, utilizaremos la biblioteca missingno (`msno`) para representar gr√°ficamente los valores nulos en el dataset.

En la siguinete celda realizaremos una matriz de nulidad (`msno.matrix()`). Este gr√°fico muestra los datos como una matriz binaria y nos puede ayudar a detectar patrones en los valores nulos. En este tipo de diagrama las l√≠neas blancas representan los valores faltantes y las negras los datos completos en las filas.
"""

from missingno import matrix
import missingno as msno
plt.figure(figsize=(12, 8))
msno.matrix(cleaned_kidney_df)
plt.title('Matriz de nulidad', fontsize=30, fontweight='bold')
plt.show()

"""A continuaci√≥n encontramos la creaci√≥n de un mapa de calor (`heatmap()`) en el que se muestra la correlaci√≥n entre los valores nulos de las variables. Su funcionamiento consiste en que:

*   Valores cercanos a 1 (amarillo) indican que cuando una columna tiene un valor nulo, la otra tambi√©n tiende a tenerlo.
*   Valores m√°s bajos (azules) indican poca relaci√≥n en la ausencia de datos.
"""

plt.figure(figsize=(12, 8))
msno.heatmap(cleaned_kidney_df, cmap='viridis')
plt.title('Mapa de calor de datos faltantes', fontsize=20, pad=20, fontweight='bold')
plt.show()

"""Seguidamente tenemos un gr√°fico de barras (`msno.bar()`) que representa la cantidad de valores no nulos en cada columna.

Las barras m√°s altas indican menos valores faltantes, mientras que las m√°s bajas muestran variables con m√°s datos ausentes.

Este tipo de diagrama nos puede ser √∫til para identificar variables que pueden requerir imputaci√≥n o eliminaci√≥n.
"""

plt.figure(figsize=(12, 8))
msno.bar(cleaned_kidney_df, color='#0077b6', figsize=(12, 8))
plt.title('Gr√°fico de barras de datos faltantes', fontsize=18, pad=20, fontweight='bold')
plt.show()

"""Finalmente, realizamos un dendrograma (`msno.dendrogram()`) que muestra una agrupaci√≥n jer√°rquica de las variables basada en la similitud en sus valores nulos. Gracias a el podemos entender qu√© variables tienen patrones similares de datos ausentes y pueden ser analizadas en conjunto.

Las agrupaciones en una misma rama indican que esas variables tienen un comportamiento similar respecto los valores faltantes.
"""

plt.figure(figsize=(10, 8))
msno.dendrogram(cleaned_kidney_df)
plt.title('Dendrograma de datos faltantes', fontsize=30, pad=30, fontweight='bold')
plt.show()

"""#### 2.5.4. Variable objetivo y relaciones entre variables

A continuaci√≥n, nos enfocaremos en analizar nuestra variable objetivo "bu", mencionada previamente en la introducci√≥n. Este an√°lisis descriptivo univariable nos permitir√° comprender su distribuci√≥n y sus caracter√≠sticas estad√≠sticas clave.

Para ello, realizaremos las siguientes acciones:
*   C√°lculo de medidas estad√≠sticas como media, mediana, moda, rango, varianza y curtosis.
*   An√°lisis de asimetr√≠a y curtosis para entender la forma de la distribuci√≥n.
*   Visualizaci√≥n de la distribuci√≥n de "bu" mediante histogramas, gr√°ficos de densidad y boxplots.



"""

import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

media = cleaned_kidney_df["bu"].mean()
mediana = cleaned_kidney_df["bu"].median()
moda = cleaned_kidney_df["bu"].mode()[0]
desviacion_tipica = cleaned_kidney_df["bu"].std()
var = cleaned_kidney_df["bu"].var()
min_val = cleaned_kidney_df["bu"].min()
max_val = cleaned_kidney_df["bu"].max()
rango = max_val - min_val
iqr = cleaned_kidney_df["bu"].quantile(0.75) - cleaned_kidney_df["bu"].quantile(0.25)
varianza = cleaned_kidney_df["bu"].var()
asimetria = stats.skew(cleaned_kidney_df["bu"].dropna())
curtosis = stats.kurtosis(cleaned_kidney_df["bu"].dropna())
q1 = cleaned_kidney_df["bu"].quantile(0.25)
q3 = cleaned_kidney_df["bu"].quantile(0.75)

# Mostrar las medidas
print(f'''
üìä An√°lisis estad√≠stico para "bu":\n
üîπ Medidas de tendencia central:
   ‚Ä¢ Media: {media:.3f}
   ‚Ä¢ Mediana: {mediana:.3f}
   ‚Ä¢ Moda: {moda:.3f}
üîπ Medidas de dispersi√≥n:
   ‚Ä¢ Desviaci√≥n est√°ndar: {desviacion_tipica:.3f}
   ‚Ä¢ Varianza: {var:.3f}
   ‚Ä¢ Rango: {rango:.3f}
   ‚Ä¢ Rango intercuart√≠lico (IQR): {iqr:.3f}
üîπ Valores extremos:
   ‚Ä¢ M√≠nimo: {min_val:.3f}
   ‚Ä¢ M√°ximo: {max_val:.3f}
   ‚Ä¢ Primer cuartil (Q1): {q1:.3f}
   ‚Ä¢ Tercer cuartil (Q3): {q3:.3f}
üîπ Forma de la distribuci√≥n:
   ‚Ä¢ Asimetr√≠a: {asimetria:.3f} ({'positiva' if asimetria > 0 else 'negativa' if asimetria < 0 else 'sim√©trica'})
   ‚Ä¢ Curtosis: {curtosis:.3f} ({'leptoc√∫rtica' if curtosis > 0 else 'platic√∫rtica' if curtosis < 0 else 'mesoc√∫rtica'})
''')

lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outliers = cleaned_kidney_df[(cleaned_kidney_df["bu"] < lower_bound) | (cleaned_kidney_df["bu"] > upper_bound)]["bu"]

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.tight_layout(pad=6.0)
plt.subplots_adjust(hspace=0.3)

# Histograma de la variable 'bu'
sns.histplot(cleaned_kidney_df['bu'], kde=True, color='#457b9d', bins=30, edgecolor='black', ax=axes[0, 0])
axes[0, 0].axvline(media, color='#fb8b24', linestyle='--', linewidth=1.5, label=f"Media: {media:.2f}")
axes[0, 0].axvline(mediana, color='#9d0208', linestyle='--', linewidth=1.5, label=f"Mediana: {mediana:.2f}")
axes[0, 0].legend(loc='upper right')
axes[0, 0].set_title('Distribuci√≥n de la variable (regresi√≥n)', fontsize=14, pad=11, fontweight='bold')
axes[0, 0].set_xlabel('Valor de "bu"', fontsize=12, labelpad=11)
axes[0, 0].set_ylabel('Frecuencia', fontsize=12, labelpad=11)
axes[0, 0].grid(True, linestyle='--', alpha=0.6)

# Gr√°fico de Densidad (KDE)
sns.kdeplot(cleaned_kidney_df['bu'], fill=True, color='#457b9d', linewidth=2, ax=axes[0, 1])
axes[0, 1].set_title('Distribuci√≥n de densidad (regresi√≥n)', fontsize=14, pad=11, fontweight='bold')
axes[0, 1].set_xlabel('Valor de "bu"', fontsize=12, labelpad=11)
axes[0, 1].set_ylabel('Densidad', fontsize=12, labelpad=11)
axes[0, 1].grid(True, linestyle='--', alpha=0.6)

# Boxplot para identificar valores at√≠picos
sns.boxplot(x=cleaned_kidney_df['bu'], color='#457b9d', fliersize=8, linewidth=1.5, ax=axes[1, 0])
axes[1, 0].set_title('Boxplot', fontsize=14, pad=11, fontweight='bold')
axes[1, 0].set_xlabel('Valor de "bu"', fontsize=12, labelpad=11)
axes[1, 0].grid(True, linestyle='--', alpha=0.6)

# Histograma con valores descriptivos
axes[1, 1].text(0.5, 0.5, f"""
Estad√≠sticas de "bu":\n
Media: {media:.3f}
Mediana: {mediana:.3f}
Moda: {moda:.3f}
Desviaci√≥n t√≠pica: {desviacion_tipica:.3f}
Rango: [{min_val:.1f}, {max_val:.1f}]
IQR: {iqr:.3f}
Varianza: {varianza:.3f}
Outliers: {len(outliers)} ({(len(outliers) / len(cleaned_kidney_df)) * 100:.1f}%)
""", horizontalalignment='center', verticalalignment='center', fontsize=12)
axes[1, 1].axis('off')  # Apagar los ejes ya que es solo texto

plt.suptitle(f'An√°lisis completo de la variable objetivo "bu"', fontsize=16, fontweight='bold', y=1)
plt.show()

"""Hacer este tipo de an√°lisis exploratorio no solo nos permite comprender el comportamiento estad√≠stico de nuestra variable objetivo, sino que resulta esencial para definir estrategias de imputaci√≥n, selecci√≥n de modelos y t√©cnicas de transformaci√≥n adecuadas en el flujo de trabajo anal√≠tico.

La comparaci√≥n entre la media (57.43) y la mediana (42.00) revela una distribuci√≥n claramente asim√©trica hacia la derecha, lo cual es corroborado por la presencia de valores extremos en el rango superior. Esta asimetr√≠a positiva sugiere que una proporci√≥n reducida de pacientes presenta niveles de urea considerablemente elevados, lo cual podr√≠a estar asociado a estados avanzados de insuficiencia renal.

La alta desviaci√≥n est√°ndar (50.50) y la amplitud del rango (de 1.5 a 391.0) indican una gran dispersi√≥n en los valores de la variable, lo cual podr√≠a afectar negativamente a modelos sensibles a la escala o a la normalidad de los datos. La detecci√≥n de outliers en un 9.5% de los casos refuerza esta observaci√≥n y subraya la necesidad de considerar transformaciones o t√©cnicas de manejo espec√≠ficas.

Despu√©s de haber analizado en profundidad nuestra variable objetivo "bu" y comprendido su distribuci√≥n, hemos decidido explorar de manera m√°s amplia las relaciones entre las distintas variables num√©ricas del conjunto de datos. Este an√°lisis visual nos permite identificar patrones, posibles relaciones lineales o no lineales, y detectar potenciales outliers que podr√≠an afectar el rendimiento de los modelos predictivos.

Para ello, hemos generado una serie de diagramas de dispersi√≥n que muestran las relaciones entre todos los pares posibles de variables num√©ricas presentes en el dataset. Esto incluye, por ejemplo, la relaci√≥n entre alb√∫mina y gravedad espec√≠fica de la orina, o entre edad y niveles de az√∫car en sangre, entre otras.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from itertools import combinations

# Obtener las variables num√©ricas
numeric_columns = cleaned_kidney_df.select_dtypes(include=[np.number]).columns.tolist()

# Generar todas las combinaciones posibles de pares
all_combinations = list(combinations(numeric_columns, 2))

# Calcular n√∫mero de filas y columnas necesarias para el subplots grid
n_plots = len(all_combinations)
n_cols = 4
n_rows = int(np.ceil(n_plots / n_cols))

# Crear la figura y los subplots
fig, axes = plt.subplots(n_rows, n_cols, figsize=(25, 4 * n_rows))
fig.tight_layout(pad=6.0)
plt.subplots_adjust(hspace=0.5, wspace=0.4)

# Aplanar el array de ejes
axes = axes.flatten()

# Crear cada gr√°fico de dispersi√≥n
for i, (var1, var2) in enumerate(all_combinations):
    if i < len(axes):
        ax = axes[i]

        sns.scatterplot(x=cleaned_kidney_df[var1], y=cleaned_kidney_df[var2], color='#9d4edd', ax=ax)
        ax.set_title(f'"{var1}" y "{var2}"', fontsize=10, pad=10, fontweight='bold')
        ax.set_xlabel(var1, fontsize=9, labelpad=10)
        ax.set_ylabel(var2, fontsize=9, labelpad=10)

# Ocultar los subplots vac√≠os
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.suptitle('Relaciones entre pares de variables', fontsize=16, fontweight='bold', y=0.95)
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Ajuste para que el t√≠tulo principal no se superponga
plt.show()

"""## 3. Limpieza de datos y preprocesamiento

### 3.1. Eliminaci√≥n de inconsistencias en las variables categ√≥ricas

En esta secci√≥n se describe el proceso de limpieza aplicado a ciertas variables categ√≥ricas del conjunto de datos, con el objetivo de mejorar la calidad y coherencia de los valores registrados que ser√°n utilizados en las siguientes etapas del an√°lisis. Tal como se explic√≥ en el apartado 2.2, esta fase resulta fundamental, ya que la presencia de inconsistencias o errores en los datos puede comprometer la correcta agrupaci√≥n de categor√≠as, generar valores redundantes y distorsionar los resultados del an√°lisis.

El enfoque principal consisti√≥ en identificar y corregir  variaciones sutiles en algunas columnas categ√≥ricas, en este caso: `'cad'`, `'classification'` y `'dm'`. Estas columnas presentaban variaciones no deseadas producto de espacios en blanco al inicio o final de las cadenas de texto. Este tipo de errores suele originarse por la introducci√≥n manual de datos o por inconsistencias al unificar informaci√≥n proveniente de diferentes fuentes.

Para abordar este problema, se implement√≥ una estrategia de normalizaci√≥n textual, utilizando expresiones regulares en combinaci√≥n con la funci√≥n `replace()` de pandas. En concreto, se aplic√≥ la eliminaci√≥n de espacios en blanco al principio y al final de cada valor categ√≥rico mediante la expresi√≥n `r'^\s+|\s+$'`.

### 3.2. Imputaci√≥n

En este apartado nos centraremos en el tratamiento de los valores faltantes presentes en el conjunto de datos, los cuales ya fueron identificados y analizados en detalle en el apartado 2.5. La presencia de valores nulos puede comprometer la calidad del an√°lisis y afectar negativamente la precisi√≥n de los modelos predictivos.

La imputaci√≥n de valores faltantes consiste en reemplazar dichos valores ausentes por estimaciones razonables, basadas en la informaci√≥n disponible en el conjunto de datos. El objetivo principal de esta t√©cnica es evitar la p√©rdida de informaci√≥n valiosa y preservar la estructura del dataset, minimizando los sesgos que podr√≠an generarse si se eliminaran directamente las observaciones incompletas.

Para llevar a cabo este proceso, se aplicar√°n distintas estrategias de imputaci√≥n, seleccionadas en funci√≥n del tipo de variable (categ√≥rica o num√©rica). Adem√°s, como veremos m√°s adelante, cada uno de los m√©todos presenta ventajas y limitaciones, tanto en t√©rminos de simplicidad como de impacto en la distribuci√≥n de los datos. Por ello, a lo largo de esta secci√≥n, se analizar√° la conveniencia de cada t√©cnica en funci√≥n de distintos par√°metros.

#### 3.2.1. Variables num√©ricas

Como primer paso en el proceso de imputaci√≥n, abordaremos las variables num√©ricas que presentan valores faltantes. Antes de decidir qu√© t√©cnica aplicar, es fundamental realizar un an√°lisis exploratorio que nos permita identificar qu√© columnas est√°n afectadas y en qu√© medida, para as√≠ cuantificar la magnitud del problema.

Para ello, mediante el siguiente fragmento de c√≥digo, se obtiene un resumen claro de la cantidad y el porcentaje de valores faltantes en cada una de las columnas num√©ricas del conjunto de datos.
"""

# Obtener las columnas num√©ricas con valores NA
numerical_cols_with_na = cleaned_kidney_df.select_dtypes(include=['number']).columns[
    cleaned_kidney_df.select_dtypes(include=['number']).isna().any()
].tolist()

# Obtener el total de registros (filas) en el DataFrame
total_registros = len(cleaned_kidney_df)

# Calcular el n√∫mero de valores NA por columna num√©rica
na_counts = cleaned_kidney_df[numerical_cols_with_na].isna().sum()
na_percentages = (na_counts / total_registros) * 100

# Crear DataFrame de resumen
resumen_faltantes = pd.DataFrame({
    'Variable': numerical_cols_with_na,
    'Valores totales': total_registros,
    'Valores faltantes': [f"{val} ({pct:.2f}%)" for val, pct in zip(na_counts, na_percentages)],
    'Valores v√°lidos': [f"{total_registros - val} ({(100 - pct):.2f}%)" for val, pct in zip(na_counts, na_percentages)]
})

print("üîπ Resumen completo de las variables num√©ricas:\n")

styled_table3 = resumen_faltantes.style.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})

styled_table3 = styled_table3.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

styled_table3

"""Una de las formas m√°s comunes y sencillas de abordar los valores faltantes en este tipo de variables es mediante la imputaci√≥n por media o mediana. Estas t√©cnicas consisten en sustituir los valores nulos por el valor medio o mediano de cada variable, respectivamente.

El siguiente bloque de c√≥digo imputa los valores faltantes en columnas num√©ricas utilizando las estrategias de media y mediana, ofreciendo adem√°s herramientas de validaci√≥n cruzada y visualizaci√≥n diagn√≥stica para evaluar la calidad de la imputaci√≥n.

Es por ello que se ha creado la funci√≥n `impute_with_strategy()` la cual generaliza la imputaci√≥n utilizando `SimpleImputer` de `sklearn`, y ofrece m√∫ltiples utilidades:
1.   Selecci√≥n de columnas num√©ricas con valores faltantes.
2.   Aplicaci√≥n de la estrategia seleccionada (`mean` o `median`).
3.   Visualizaci√≥n diagn√≥stica:
    *   Histogramas antes y despu√©s de la imputaci√≥n.
    *   Comparaci√≥n mediante boxplots.
4.   Evaluaci√≥n de la imputaci√≥n con validaci√≥n cruzada, midiendo el error cuadr√°tico medio (RMSE) y el coeficiente de determinaci√≥n (R¬≤).
5.   An√°lisis estad√≠stico de los cambios introducidos por la imputaci√≥n (media, desviaci√≥n, variaci√≥n porcentual, etc.).
6.   Control de errores y verificaci√≥n de condiciones.

Tambi√©n hemos creado la funci√≥n `run_both_imputations()`, que ejecuta la imputaci√≥n dos veces sobre el mismo dataset, cada una de ellas con una estrategia diferente.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score
import warnings
from tqdm.auto import tqdm

def impute_with_strategy(df, strategy='mean', diagnostic_plots=True,
                        cross_validation=False, n_splits=5, verbose=True,
                        random_state=42):
    """
    Imputaci√≥n utilizando SimpleImputer con visualizaci√≥n y validaci√≥n cruzada

    -----------
    Par√°metros:
    -----------
    df : DataFrame de entrada con valores faltantes.
    strategy : estrategia de imputaci√≥n ('mean', 'median', 'most_frequent', o 'constant').
    diagnostic_plots : si se generan gr√°ficos de diagn√≥stico.
    cross_validation : si se realiza validaci√≥n cruzada para evaluar precisi√≥n.
    n_splits : n√∫mero de divisiones para la validaci√≥n cruzada.
    verbose : si se imprime la informaci√≥n de progreso.
    random_state : semilla aleatoria para reproducibilidad.

    --------
    Devuelve:
    --------
    dict: contiene los resultados de imputaci√≥n y los metadatos.
    """

    np.random.seed(random_state)

    # Validar el Dataframe de entrada
    if df is None or df.empty:
        raise ValueError("Input dataframe cannot be None or empty")

    # Validar estrategia
    valid_strategies = ['mean', 'median', 'most_frequent', 'constant']
    if strategy not in valid_strategies:
        raise ValueError(f"Strategy must be one of {valid_strategies}")

    # Seleccionar columnas num√©ricas para la imputaci√≥n
    numerical_columns = df.select_dtypes(include=['number']).columns

    if len(numerical_columns) == 0:
        raise ValueError("No numerical columns found for imputation")

    X = df[numerical_columns].copy()

    # Comprobar columnas con todos los valores faltantes
    all_missing = X.columns[X.isna().all()].tolist()
    if all_missing:
        if verbose:
            print(f"Columnas con todos los valores faltantes: {all_missing}")
            print("Estas columnas se excluir√°n de la imputaci√≥n.")
        X = X.drop(columns=all_missing)
        numerical_columns = X.columns.tolist()

    # Comprueba si faltan valores
    if not X.isna().any().any():
        if verbose:
            print("No se encontraron valores faltantes en las columnas num√©ricas. No se requiere imputaci√≥n.")
        return {
            'imputed_data': X,
            'imputation_metadata': {},
            'message': 'No es necesaria ninguna imputaci√≥n.'
        }

    imputation_metadata = {}

    # Guardar X original antes de imputaci√≥n para referencias futuras
    X_original = X.copy()

    # Guardar las m√°scaras de valores faltantes antes de imputar
    missing_masks = {}
    for column in numerical_columns:
        missing_masks[column] = X[column].isna()

    # Implementar SimpleImputer para la imputaci√≥n
    imputer = SimpleImputer(strategy=strategy, missing_values=np.nan)

    # Ajustar el imputador y transformar los datos
    X_imputed_array = imputer.fit_transform(X)

    # Convertir el array de nuevo a DataFrame
    X_imputed = pd.DataFrame(X_imputed_array, columns=X.columns, index=X.index)

    # Guardar los valores de imputaci√≥n
    fill_values = {}
    for i, column in enumerate(X.columns):
        fill_values[column] = imputer.statistics_[i]

    # Preparar gr√°ficos de diagn√≥stico si se solicita
    if diagnostic_plots:
        # Contar solo columnas con valores faltantes
        missing_cols = [col for col in numerical_columns if X_original[col].isna().any()]
        n_rows = len(missing_cols)

        if n_rows == 0:
            diagnostic_plots = False
        else:
            fig, axes = plt.subplots(n_rows, 2, figsize=(16, 4*n_rows))
            title = f'Diagn√≥stico de imputaci√≥n por {strategy.capitalize()}'
            fig.suptitle(title, fontsize=20, fontweight='bold', y=1)
            # Asegurar que axes sea siempre un array, incluso cuando n_rows es 1
            if n_rows == 1:
                axes = np.array([axes])
            plot_idx = 0

    # Validaci√≥n cruzada para la evaluaci√≥n de la calidad de la imputaci√≥n si se solicita
    cv_results = {}
    if cross_validation and any(X_original[col].isna().sum() > 10 for col in X_original.columns):
        if verbose:
            print(f"\nRealizando validaci√≥n cruzada para evaluar la calidad de la imputaci√≥n por {strategy}.")

        # Columnas a validar
        columns_to_validate = [col for col in X_original.columns if X_original[col].isna().sum() > 0 and len(X_original[col].dropna()) >= 20]
        columns_iterator = tqdm(columns_to_validate) if verbose else columns_to_validate

        for col in columns_iterator:
            # Establecer descripci√≥n de la barra de progreso si est√° activada
            if verbose and isinstance(columns_iterator, tqdm):
                columns_iterator.set_description(f"Validando {col}")

            # Realizar el CV s√≥lo en columnas con suficientes valores no ausentes
            non_missing_idx = X_original[col].dropna().index
            original_values = X_original.loc[non_missing_idx, col].values

            cv_scores = {'rmse': [], 'r2': []}
            kf = KFold(n_splits=min(n_splits, len(non_missing_idx)//5), shuffle=True, random_state=random_state)

            for train_idx, test_idx in kf.split(non_missing_idx):
                # Crear una copia con valores perdidos introducidos artificialmente
                X_cv = X_original.copy()
                test_indices = non_missing_idx[test_idx]
                X_cv.loc[test_indices, col] = np.nan

                # Imputar utilizando SimpleImputer
                train_indices = non_missing_idx[train_idx]
                train_data = X_cv.loc[train_indices]

                cv_imputer = SimpleImputer(strategy=strategy)
                X_cv_imputed = X_cv.copy()

                # Ajustar el imputador solo con datos de entrenamiento
                for column in X_cv.columns:
                    # Si la columna tiene valores ausentes, impute utilizando entrenamiento
                    if X_cv[column].isna().any():
                        col_imputer = SimpleImputer(strategy=strategy)
                        col_imputer.fit(train_data[[column]].dropna())

                        # Aplicar la imputaci√≥n solo a los valores ausentes
                        missing_mask = X_cv[column].isna()
                        if missing_mask.any():
                            missing_values = col_imputer.transform(X_cv.loc[missing_mask, [column]])
                            X_cv_imputed.loc[missing_mask, column] = missing_values.flatten()

                # Evaluar la imputaci√≥n en el conjunto de pruebas
                true_values = X_original.loc[test_indices, col].values
                predicted_values = X_cv_imputed.loc[test_indices, col].values

                rmse = np.sqrt(mean_squared_error(true_values, predicted_values))
                r2 = r2_score(true_values, predicted_values)

                cv_scores['rmse'].append(rmse)
                cv_scores['r2'].append(r2)

            cv_results[col] = {
                'mean_rmse': np.mean(cv_scores['rmse']),
                'std_rmse': np.std(cv_scores['rmse']),
                'mean_r2': np.mean(cv_scores['r2']),
                'std_r2': np.std(cv_scores['r2'])
            }

    # Procesar los resultados de la imputaci√≥n y generar diagn√≥sticos
    missing_columns = [col for col in numerical_columns if X_original[col].isna().any()]
    if verbose:
        print(f"\nüìä Resumen de imputaci√≥n por {strategy.upper()} para {len(missing_columns)} columnas con valores faltantes.")

    plot_idx = 0
    for idx, column in enumerate(numerical_columns):
        missing_mask = missing_masks.get(column, X_original[column].isna())
        n_missing = missing_mask.sum()

        if n_missing == 0:
            if verbose:
                print(f"No faltan valores en '{column}'. No es necesaria la imputaci√≥n.")
            continue

        if len(X_original[column].dropna()) < 5:
            if verbose:
                print(f"Datos insuficientes en '{column}' para una imputaci√≥n fiable. Los resultados pueden ser poco fiables.")

        # Obtener valores imputados
        imputed_values = pd.Series([fill_values[column]] * n_missing)
        # Datos originales sin valores ausentes
        original_data = X_original[column].dropna()

        # Protecci√≥n contra divisi√≥n por cero o datos vac√≠os
        percent_change = np.nan
        if not original_data.empty and original_data.mean() != 0:
            percent_change = (imputed_values.mean() - original_data.mean()) / original_data.mean() * 100

        imputation_metadata[column] = {
            'missing_count': n_missing,
            'missing_percent': n_missing/len(X) * 100,
            'imputed_value': fill_values[column],
            'imputed_mean': imputed_values.mean(),
            'imputed_median': imputed_values.median(),
            'imputed_std': imputed_values.std() if len(imputed_values) > 1 else 0,
            'imputed_range': (imputed_values.min(), imputed_values.max()) if len(imputed_values) > 0 else (np.nan, np.nan),
            'original_mean': original_data.mean(),
            'original_std': original_data.std(),
            'percent_change_mean': percent_change
        }

        # Generar gr√°ficos de diagn√≥stico si se solicita
        if diagnostic_plots and n_missing > 0:
            try:
                impute_color_map = {
                    'mean': '#000000',
                    'median': '#000000',
                    'most_frequent': '#000000',
                }
                impute_color = impute_color_map.get(strategy)
                impute_label = f'Imputado ({strategy})'
                completed_color = '#bc4749'  # Color para la distribuci√≥n despu√©s de imputar

                # 1. Distribuci√≥n original vs. post-imputaci√≥n
                ax1 = axes[plot_idx, 0]

                # Histograma de datos originales
                sns.histplot(original_data, kde=True, ax=ax1,
                        color='#606c38', alpha=0.4, label='Datos originales')

                # Histograma de datos completos despu√©s de imputaci√≥n
                sns.histplot(X_imputed[column], kde=True, ax=ax1,
                        color=completed_color, alpha=0.3, label='Valores imputados')

                # A√±adir l√≠nea vertical para el valor imputado
                if len(imputed_values) > 0:
                    ax1.axvline(x=fill_values[column], color=impute_color, linestyle='--',
                                linewidth=2.05, label=impute_label)

                ax1.set_title(f'Comparaci√≥n de la distribuci√≥n de "{column}"', pad=12, fontweight='bold')
                ax1.set_xlabel(column)
                ax1.set_ylabel('Densidad', labelpad=12)
                ax1.legend()

                # 2. Comparaci√≥n de boxplots: original y despu√©s de imputaci√≥n
                ax2 = axes[plot_idx, 1]

                # Crear un DataFrame para la comparaci√≥n de boxplots
                comparison_df = pd.DataFrame({
                    'Original': original_data,
                    'Imputado': X_imputed[column]
                })

                # Boxplot para datos originales y despu√©s de imputaci√≥n
                sns.boxplot(data=comparison_df, ax=ax2, palette=['#606c38', completed_color], orient='h')

                # A√±adir marcador para valor imputado
                if fill_values[column] is not None:
                    ax2.axvline(x=fill_values[column], color=impute_color,
                                linestyle='--', linewidth=2.05, label=impute_label)

                ax2.set_title(f'Comparaci√≥n boxplot de "{column}"', fontsize=12, pad=15, fontweight='bold')
                ax2.set_xlabel(f'Valor de "{column}"', labelpad=12)
                ax2.legend()

                plot_idx += 1
            except Exception as e:
                if verbose:
                    print(f"Error en la generaci√≥n de gr√°ficos para la columna '{column}': {str(e)}")

        if verbose:
            print(f'\nüîπ COLUMNA "{column}":')
            print(f"   Valores faltantes: {n_missing} ({n_missing/len(X):.1%})")
            if not original_data.empty:
                print(f"   Datos originales: Media = {original_data.mean():.2f}, Mediana = {original_data.median():.2f}, Desviaci√≥n t√≠pica = {original_data.std():.2f}")
            print(f"   Valor imputado ({strategy}): {fill_values[column]:.2f}")
            print(f"   Datos despu√©s de imputaci√≥n: Media = {X_imputed[column].mean():.2f}, Mediana = {X_imputed[column].median():.2f}, Desviaci√≥n t√≠pica = {X_imputed[column].std():.2f}")
            if not pd.isna(percent_change):
                print(f"   Cambio respecto a media original: {percent_change:.1f}%")
            if column in cv_results:
                print(f"   Resultados de la validaci√≥n cruzada: RMSE = {cv_results[column]['mean_rmse']:.3f} ¬± {cv_results[column]['std_rmse']:.3f}, R¬≤ = {cv_results[column]['mean_r2']:.3f} ¬± {cv_results[column]['std_r2']:.3f}")

    if diagnostic_plots and plot_idx > 0:
        try:
            plt.tight_layout(h_pad=4.0, w_pad=3.0)
            plt.subplots_adjust(top=0.98)
            plt.show()
        except Exception as e:
            if verbose:
                print(f"Error en la visualizaci√≥n de gr√°ficos: {str(e)}")

    # Para las columnas del dataframe original que se imputaron, las copiamos de nuevo
    result_df = df.copy()
    for col in numerical_columns:
        if col in X_imputed.columns:
            result_df[col] = X_imputed[col]

    # Preparamos el diccionario de devoluci√≥n
    result = {
        'imputed_data': result_df,
        'imputation_metadata': imputation_metadata,
        'fill_values': fill_values
    }

    if cross_validation and cv_results:
        result['cv_results'] = cv_results

    return result

def run_both_imputations(df, diagnostic_plots=True, cross_validation=True,
                       n_splits=5, verbose=True, random_state=42):
    """
    Ejecuta imputaci√≥n por media y mediana.

    -----------
    Parametros:
    -----------
    df : DataFrame de entrada con valores faltantes.
    diagnostic_plots : si se generan gr√°ficos de diagn√≥stico.
    cross_validation : si se realiza validaci√≥n cruzada para evaluar precisi√≥n.
    n_splits : n√∫mero de divisiones para la validaci√≥n cruzada.
    verbose : si se imprime la informaci√≥n de progreso.
    random_state : semilla aleatoria para reproducibilidad.

    --------
    Devuelve:
    --------
    dict: contiene los resultados de imputaci√≥n por media y mediana.
    """

    mean_results = impute_with_strategy(
        df=df,
        strategy='mean',
        diagnostic_plots=diagnostic_plots,
        cross_validation=cross_validation,
        n_splits=n_splits,
        verbose=verbose,
        random_state=random_state
    )

    median_results = impute_with_strategy(
        df=df,
        strategy='median',
        diagnostic_plots=diagnostic_plots,
        cross_validation=cross_validation,
        n_splits=n_splits,
        verbose=verbose,
        random_state=random_state
    )

    return {
        'mean_imputation': mean_results,
        'median_imputation': median_results
    }

imputation_results = run_both_imputations(
     df=cleaned_kidney_df,
     diagnostic_plots=True,
     cross_validation=True,
     verbose=True
)

"""Para garantizar la calidad y consistencia de los datos imputados, hemos decidido realizar una verificaci√≥n posterior al proceso de imputaci√≥n de cada estrategia. En este bloque de c√≥digo se eval√∫a si los m√©todos aplicados para rellenar valores faltantes han cumplido correctamente su objetivo: eliminar por completo los valores NaN de las columnas num√©ricas.

El c√≥digo realiza los siguientes pasos para cada m√©todo de imputaci√≥n:
*   Se extraen √∫nicamente las columnas num√©ricas del DataFrame resultante de cada m√©todo de imputaci√≥n, esto se hace con `.select_dtypes(include=['number'])`, para centrarnos √∫nicamente en aquellas columnas qque han tenido cambios.
*   Se muestra una vista r√°pida de las primeras 10 filas con `.head(10)` para inspeccionar manualmente los resultados y verificar que los valores han sido imputados razonablemente.
*   Se aplica `.isna().sum()` sobre las columnas num√©ricas para contar cu√°ntos valores faltantes siguen presentes. Un resultado de ceros indica que todos los valores NaN fueron reemplazados exitosamente por los m√©todos de imputaci√≥n correspondientes.
"""

numeric_columns_Mean = imputation_results['mean_imputation']['imputed_data'].select_dtypes(include=['number'])
numeric_columns_Mean.head(10)

numeric_columns_Mean.isna().sum()

numeric_columns_Median = imputation_results['median_imputation']['imputed_data'].select_dtypes(include=['number'])
numeric_columns_Median.head(10)

numeric_columns_Median.isna().sum()

"""A continuaci√≥n se implementa un sistema de imputaci√≥n utilizando el algoritmo K-Nearest Neighbors (KNN), una t√©cnica que permite estimar valores ausentes bas√°ndose en la similitud con otros registros. A diferencia de enfoques tradicionales que utilizan un valor fijo de k para todo el conjunto de datos, aqu√≠ se introduce una versi√≥n adaptativa, capaz de encontrar el valor √≥ptimo de k para cada columna con valores faltantes, maximizando la precisi√≥n de la imputaci√≥n a trav√©s de validaci√≥n cruzada.

En el c√≥digo se implementa una funci√≥n llamada `AdaptiveKNNImputer`, que tiene las siguientes caracter√≠sticas y funcionalidades pricipales:
1.   Selecci√≥n de columnas num√©ricas con valores faltantes.
2.   Para cada columna, se eval√∫an diferentes valores de k (n√∫mero de vecinos) y se utiliza validaci√≥n cruzada (KFold) para medir el rendimiento de cada valor de k mediante m√©tricas como RMSE y R¬≤. Finalmente, se selecciona el k que minimiza el error de imputaci√≥n.
3.   Cada columna se imputa de forma independiente con su k √≥ptimo y se preservan las estad√≠sticas originales para as√≠ calcular el impacto del cambio (porcentaje de variaci√≥n en la media).
4.   Se ofrece una comparaci√≥n gr√°fica de la distribuci√≥n antes y despu√©s de la imputaci√≥n, incluyendo histogramas y boxplots.

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.base import BaseEstimator, TransformerMixin
from tqdm.auto import tqdm

class AdaptiveKNNImputer(BaseEstimator, TransformerMixin):
    """
    Implementaci√≥n de KNNImputer adaptativo que encuentra el mejor valor k para cada columna.
    Utiliza validaci√≥n cruzada para optimizar el par√°metro k usando los patrones naturales
    de valores faltantes en los datos.

    Par√°metros:
    -----------
    max_k : n√∫mero m√°ximo de vecinos a considerar.
    cv : n√∫mero de pliegues para la validaci√≥n cruzada.
    random_state : semilla aleatoria para reproducibilidad.
    verbose : si es True, muestra informaci√≥n sobre el proceso de optimizaci√≥n.
    plot : si es True, grafica la distribuci√≥n antes y despu√©s de la imputaci√≥n.
    """

    def __init__(self, max_k=10, cv=5, random_state=42, verbose=False, plot=True):
        self.max_k = max_k
        self.cv = cv
        self.random_state = random_state
        self.verbose = verbose
        self.plot = plot
        self.optimal_k_dict = {}
        self.imputers = {}
        self.cv_results = {}

    def fit(self, X, y=None):
        """
        Encuentra el valor √≥ptimo de k para cada columna con valores faltantes.
        Utiliza los patrones naturales de valores faltantes para la validaci√≥n.

        -----------
        Par√°metros:
        -----------
        X : DataFrame con los valores faltantes a optimizar.
        y : no utilizado, presente por compatibilidad con la API de scikit-learn.

        --------
        Retorna:
        --------
        self : objeto
        """

        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X)

        # Seleccionar s√≥lo columnas num√©ricas
        numerical_columns = X.select_dtypes(include=['number']).columns
        X_numerical = X[numerical_columns].copy()

        # Buscar columnas con valores faltantes
        self.cols_with_missing = [col for col in numerical_columns if X_numerical[col].isna().sum() > 0]

        if not self.cols_with_missing:
            if self.verbose:
                print("No se encontraron columnas con valores faltantes.")
            return self

        # Usar tqdm para mostrar una barra de progreso si verbose=True
        columns_iterator = tqdm(self.cols_with_missing) if self.verbose else self.cols_with_missing

        # Selecci√≥n de K para cada columna con valores faltantes
        for column in columns_iterator:
            # Establecer descripci√≥n de la barra de progreso si est√° activada
            if self.verbose and isinstance(columns_iterator, tqdm):
                columns_iterator.set_description(f"Optimizando k para {column}")

            # Separar datos completos e incompletos para esta columna
            X_complete = X_numerical[X_numerical[column].notna()].copy()

            # Manejar caso con muy pocos datos completos
            if len(X_complete) < self.cv + 1:
                optimal_k = min(3, len(X_complete))
                self.optimal_k_dict[column] = optimal_k
                if self.verbose:
                    print(f"Columna {column} tiene muy pocos datos completos. Se utilizar√° k={optimal_k}.")
                continue

            # Optimizar k usando validaci√≥n cruzada en los datos completos
            # Sin simular valores faltantes, sino usando el m√©todo de hold-out
            k_errors = []
            k_r2_scores = []

            for k in range(1, min(self.max_k + 1, len(X_complete))):
                cv_errors = []
                cv_r2_scores = []

                # Utilizamos KFold para validaci√≥n cruzada
                kf = KFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)

                for train_idx, val_idx in kf.split(X_complete):
                    # Dividir en entrenamiento y validaci√≥n
                    X_train, X_val = X_complete.iloc[train_idx], X_complete.iloc[val_idx]

                    # Guardar valores reales para comparar despu√©s
                    val_true_values = X_val[column].copy()

                    # Crear set de validaci√≥n con valores faltantes
                    X_val_missing = X_val.copy()
                    X_val_missing[column] = np.nan

                    # Combinar para entrenamiento e imputaci√≥n
                    X_combined = pd.concat([X_train, X_val_missing])

                    # Imputar usando KNN con el valor k actual
                    imputer = KNNImputer(n_neighbors=k)
                    X_imputed = pd.DataFrame(
                        imputer.fit_transform(X_combined),
                        columns=X_combined.columns,
                        index=X_combined.index
                    )

                    # Extraer valores imputados
                    val_imputed_values = X_imputed.loc[X_val.index, column]

                    # Calcular m√©tricas de error
                    error = mean_squared_error(val_true_values, val_imputed_values)
                    r2 = r2_score(val_true_values, val_imputed_values)

                    cv_errors.append(error)
                    cv_r2_scores.append(r2)

                # Promediar errores para este valor k
                k_errors.append(np.mean(cv_errors))
                k_r2_scores.append(np.mean(cv_r2_scores))

            # Seleccionar k √≥ptimo (error m√°s bajo)
            if k_errors:
                optimal_k = np.argmin(k_errors) + 1
                self.optimal_k_dict[column] = optimal_k

                # Guardar resultados de validaci√≥n cruzada
                self.cv_results[column] = {
                    'mean_rmse': np.sqrt(k_errors[optimal_k-1]),
                    'std_rmse': np.std([np.sqrt(e) for e in cv_errors]) if cv_errors else 0,
                    'mean_r2': k_r2_scores[optimal_k-1],
                    'std_r2': np.std(cv_r2_scores) if cv_r2_scores else 0
                }

                # Crear y guardar el imputador con k √≥ptimo para esta columna
                self.imputers[column] = KNNImputer(n_neighbors=optimal_k)

        return self

    def transform(self, X):
        """
        Aplica la imputaci√≥n utilizando los valores k √≥ptimos encontrados.

        -----------
        Par√°metros:
        -----------
        X : DataFrame con los valores faltantes a imputar.

        --------
        Retorna:
        --------
        X_imputed : DataFrame con los valores imputados.
        """

        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X)

        # Seleccionar s√≥lo columnas num√©ricas
        numerical_columns = X.select_dtypes(include=['number']).columns
        X_numerical = X[numerical_columns].copy()

        # Si no hay columnas con valores faltantes o no se entren√≥ el modelo
        if not hasattr(self, 'cols_with_missing') or not self.cols_with_missing:
            return X.copy()  # Retornar una copia para consistencia

        # Imputar cada columna por separado para mantener la integridad de los datos
        X_imputed = X_numerical.copy()

        # Guardar estad√≠sticas antes/despu√©s de la imputaci√≥n para cada columna
        self.imputation_stats = {}

        # Usar tqdm para mostrar una barra de progreso si verbose=True
        columns_iterator = tqdm(self.cols_with_missing) if self.verbose else self.cols_with_missing

        for column in columns_iterator:
            # Establecer descripci√≥n de la barra de progreso si est√° activada
            if self.verbose and isinstance(columns_iterator, tqdm):
                columns_iterator.set_description(f"Imputando {column}")

            if column in X_numerical.columns and X_numerical[column].isna().sum() > 0:
                # Guardar estad√≠sticas antes de la imputaci√≥n
                n_missing = X_numerical[column].isna().sum()
                original_data = X_numerical[column].dropna()

                if column in self.imputers:
                    # Clonar X_numerical para la imputaci√≥n de esta columna
                    X_to_impute = X_numerical.copy()

                    # Imputar utilizando el imputador entrenado
                    X_col_imputed = pd.DataFrame(
                        self.imputers[column].fit_transform(X_to_impute),
                        columns=X_to_impute.columns,
                        index=X_to_impute.index
                    )

                    # Actualizar solo los valores faltantes de esta columna
                    missing_mask = X_imputed[column].isna()
                    X_imputed.loc[missing_mask, column] = X_col_imputed.loc[missing_mask, column]

                    # Guardar estad√≠sticas despu√©s de la imputaci√≥n
                    imputed_values = X_imputed.loc[missing_mask, column]

                    # Calcular el cambio porcentual en la media
                    if len(original_data) > 0:
                        percent_change = ((imputed_values.mean() - original_data.mean()) / original_data.mean()) * 100
                    else:
                        percent_change = np.nan

                    # Guardar todas las estad√≠sticas
                    self.imputation_stats[column] = {
                        'missing_count': n_missing,
                        'missing_percent': n_missing / len(X) * 100,
                        'original_mean': original_data.mean() if len(original_data) > 0 else np.nan,
                        'original_median': original_data.median() if len(original_data) > 0 else np.nan,
                        'original_std': original_data.std() if len(original_data) > 0 else np.nan,
                        'imputed_mean': imputed_values.mean(),
                        'imputed_median': imputed_values.median(),
                        'imputed_std': imputed_values.std(),
                        'percent_change_mean': percent_change,
                        'imputed_range': (imputed_values.min(), imputed_values.max())
                    }

        # Crear un DataFrame final con todas las columnas originales
        result = X.copy()
        for col in X_imputed.columns:
            result[col] = X_imputed[col]

        # Mostrar resumen de imputaci√≥n si se solicit√≥
        if self.verbose:
            self.print_summary()

        # Visualizaci√≥n si se solicit√≥
        if self.plot and hasattr(self, 'cols_with_missing') and self.cols_with_missing:
            self._plot_distributions(X, result)

        return result

    def fit_transform(self, X, y=None):
        """
        Ajusta el modelo y aplica la transformaci√≥n.

        -----------
        Par√°metros:
        -----------
        X : DataFrame con los valores faltantes a imputar.
        y : No utilizado, presente por compatibilidad con la API de scikit-learn.

        --------
        Retorna:
        --------
        dict: Diccionario con los datos imputados y metadatos.
        """
        self.fit(X)
        imputed_data = self.transform(X)

        # Crear un diccionario de resultados similar al formato de MICE
        result = {
            'imputed_data': imputed_data,
            'imputation_metadata': self.imputation_stats
        }

        # A√±adir resultados de validaci√≥n cruzada si existen
        if hasattr(self, 'cv_results') and self.cv_results:
            result['cv_results'] = self.cv_results

        return result

    def get_optimal_k(self):
        """
        Devuelve el diccionario con los valores k √≥ptimos para cada columna.

        --------
        Retorna:
        --------
        optimal_k_dict : diccionario con los valores k √≥ptimos para cada columna.
        """
        return self.optimal_k_dict

    def print_summary(self):
        """
        Imprime un resumen de la imputaci√≥n realizada.
        """
        print(f"\nüìä Resumen de imputaci√≥n para {len(self.cols_with_missing)} columnas con valores faltantes.")

        for column in self.cols_with_missing:
            if column in self.imputation_stats:
                stats = self.imputation_stats[column]

                print(f'\nüîπ COLUMNA "{column}":')
                print(f"   Valores faltantes: {stats['missing_count']} ({stats['missing_percent']:.1f}%)")

                if not np.isnan(stats['original_mean']):
                    print(f"   Datos originales: Media = {stats['original_mean']:.2f}, "
                          f"Mediana = {stats['original_median']:.2f}, "
                          f"Desviaci√≥n t√≠pica = {stats['original_std']:.2f}")

                print(f"   Datos imputados: Media = {stats['imputed_mean']:.2f}, "
                      f"Mediana = {stats['imputed_median']:.2f}, "
                      f"Desviaci√≥n t√≠pica = {stats['imputed_std']:.2f}")

                if not np.isnan(stats['percent_change_mean']):
                    print(f"   Cambio medio: {stats['percent_change_mean']:.1f}%")

                if column in self.cv_results:
                    print(f"   Resultados de la validaci√≥n cruzada: "
                          f"RMSE = {self.cv_results[column]['mean_rmse']:.3f} ¬± {self.cv_results[column]['std_rmse']:.3f}, "
                          f"R¬≤ = {self.cv_results[column]['mean_r2']:.3f} ¬± {self.cv_results[column]['std_r2']:.3f}")

                print(f"   K √≥ptimo: {self.optimal_k_dict.get(column, 'N/A')}")

    def _plot_distributions(self, X_original, X_imputed):
        """
        Visualiza la distribuci√≥n de los datos antes y despu√©s de la imputaci√≥n.

        -----------
        Par√°metros:
        -----------
        X_original : DataFrame original con valores faltantes.
        X_imputed : DataFrame con valores imputados.
        """
        cols_to_plot = self.cols_with_missing
        n_cols = len(cols_to_plot)

        if n_cols == 0:
            return

        print("\n")

        # Crear figura con n√∫mero de filas igual al n√∫mero de columnas a graficar
        fig, axes = plt.subplots(n_cols, 2, figsize=(14, 4 * n_cols))
        fig.suptitle('Distribuci√≥n antes y despu√©s de la imputaci√≥n KNN adaptativa',
                     fontsize=16, fontweight='bold', y=1)

        # Asegurar que axes sea bidimensional incluso con una sola columna
        if n_cols == 1:
            axes = axes.reshape(1, 2)

        for idx, column in enumerate(cols_to_plot):
            # Obtener los ejes para esta columna
            ax1, ax2 = axes[idx]

            # Datos originales (sin imputaci√≥n)
            original_data = X_original[column].dropna()

            # Crear m√°scara para identificar valores que fueron imputados
            missing_mask = X_original[column].isna()
            imputed_values = X_imputed.loc[missing_mask, column]

            # Crear un DataFrame para el boxplot
            combined = pd.DataFrame({
                'Original': original_data,
                'Imputado': imputed_values
            })

            # 1. Distribuci√≥n original vs. distribuci√≥n imputada
            sns.histplot(original_data, kde=True, ax=ax1,
                        color='#606c38', alpha=0.4, label='Datos originales')
            if len(imputed_values) > 0:
                sns.histplot(imputed_values, kde=True, ax=ax1,
                            color='#bc4749', alpha=0.6, label='Valores imputados')

            k_value = self.optimal_k_dict.get(column, "N/A")
            ax1.set_title(f'Comparaci√≥n de la distribuci√≥n de "{column}" (k={k_value})', pad=12, fontweight='bold')
            ax1.set_xlabel(column)
            ax1.set_ylabel('Frecuencia', labelpad=12)
            ax1.legend()

            # 2. Comparaci√≥n de boxplot
            sns.boxplot(data=combined, ax=ax2, palette=['#606c38', '#bc4749'])
            ax2.set_title(f'Comparaci√≥n boxplot de "{column}"', fontsize=12, pad=15, fontweight='bold')
            ax2.set_ylabel(f'Valor de "{column}"', labelpad=12)

        plt.tight_layout(h_pad=4.0, w_pad=3.0)
        plt.subplots_adjust(top=0.985)
        plt.show()


imputer = AdaptiveKNNImputer(max_k=10, cv=5, verbose=True)
KNN_result = imputer.fit_transform(cleaned_kidney_df)
imputed_df_KNN = KNN_result['imputed_data']
metadata_KNN = KNN_result['imputation_metadata']

numeric_columns_KNN = imputed_df_KNN.select_dtypes(include=['number'])
numeric_columns_KNN.head(10)

numeric_columns_KNN.isna().sum()

"""La √∫ltima t√©cnica que hemos decidido utilizar para imputar este tipo de variables es MICE (Multiple Imputation by Chained Equations), una estrategia iterativa que permite modelar cada variable con valores perdidos como una funci√≥n de las dem√°s. Esta es especialmente √∫til cuando hay m√∫ltiples variables con datos faltantes y relaciones complejas entre ellas.

MICE funciona imputando secuencialmente los valores ausentes de cada variable utilizando modelos de regresi√≥n, ajustados a partir del resto de variables como predictores. Este proceso se repite durante varias iteraciones, refinando las estimaciones en cada paso hasta lograr la convergencia.

Para implementar este m√©todo, incluyendo diagn√≥stico gr√°fico y evaluaci√≥n mediante validaci√≥n cruzada, hemos creado la funci√≥n `impute_missing_values_mice`. Sus funcionalidades principales son las siguientes:
*   Permite elegir el estimador (por defecto, `BayesianRidge`, aunque se puede cambiar por modelos como `RandomForestRegressor`).
*   Escala el conjunto de datos con `StandardScaler`, reemplazando temporalmente los NaN por la media para evitar errores, y luego restaur√°ndolos antes de la imputaci√≥n. Esto se realiza para estabilizar el proceso y facilitar la convergencia del algoritmo.
*   Realiza imputaciones iterativas utilizando `IterativeImputer` de `Scikit-learn`. Este recorre las columnas num√©ricas, imputando una por una en un ciclo, y repite el proceso varias veces hasta que los valores imputados se estabilizan.
*   Permite visualizar gr√°ficos comparativos entre las distribuciones originales y los valores imputados para cada variable con datos faltantes, mediante histogramas y diagramas de caja (boxplots).
*   Permite comprobar la calidad del proceso mediante validaci√≥n cruzada: se simula la imputaci√≥n en subconjuntos de datos con valores ocultos y se eval√∫an m√©tricas como RMSE y R¬≤ para cada variable.
*   Muestra un resumen final para cada columna imputada, en el que se detalla:
    *   Cu√°ntos valores fueron imputados.
    *   La media, mediana y desviaci√≥n est√°ndar de los datos originales e imputados.
    *   El cambio porcentual en la media.
    *   Los resultados de la validaci√≥n cruzada, si se aplic√≥.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import BayesianRidge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
import warnings
import logging
from tqdm.auto import tqdm

def impute_missing_values_mice(df, max_iterations=20, random_state=42, estimator=None,
                              diagnostic_plots=True, return_convergence=False,
                              cross_validation=False, n_splits=5, verbose=True):
    """
    Imputaci√≥n MICE con visualizaci√≥n avanzada, diagn√≥stico y validaci√≥n cruzada

    -----------
    Parametros:
    -----------
    df : DataFrame de entrada con valores faltantes.
    max_iterations : n√∫mero m√°ximo de iteraciones de imputaci√≥n.
    random_state : semilla aleatoria para reproducibilidad.
    estimator : estimador personalizado (predeterminado: BayesianRidge).
    diagnostic_plots si se generan gr√°ficos de diagn√≥stico.
    return_convergence : si se devuelve informaci√≥n de convergencia.
    cross_validation : si se realiza una validaci√≥n cruzada para evaluar la precisi√≥n de la imputaci√≥n.
    n_splits : n√∫mero de divisiones para la validaci√≥n cruzada.
    verbose : si se imprime la informaci√≥n de progreso.

    --------
    Devuelve:
    --------
    dict: contiene los resultados de imputaci√≥n y los metadatos.
    """

    # Suprimir los mensajes de log de IterativeImputer
    logging.getLogger('sklearn.impute._iterative').setLevel(logging.ERROR)

    np.random.seed(random_state)

    # Validar el Dataframe de entrada
    if df is None or df.empty:
        raise ValueError("Input dataframe cannot be None or empty")

    # Seleccionar columnas num√©ricas para la imputaci√≥n
    numerical_columns = df.select_dtypes(include=['number']).columns

    if len(numerical_columns) == 0:
        raise ValueError("No numerical columns found for imputation")

    X = df[numerical_columns].copy()

    # Comprobar columnas con todos los valores faltantes
    all_missing = X.columns[X.isna().all()].tolist()
    if all_missing:
        if verbose:
            print(f"Columnas con todos los valores faltantes: {all_missing}")
            print("Estas columnas se excluir√°n de la imputaci√≥n.")
        X = X.drop(columns=all_missing)
        numerical_columns = X.columns.tolist()  # Convertimos a lista para mantener coherencia

    # Comprueba si faltan valores
    if not X.isna().any().any():
        if verbose:
            print("No se encontraron valores faltantes en las columnas num√©ricas. No se requiere imputaci√≥n.")
        return {
            'imputed_data': X,
            'imputation_metadata': {},
            'message': 'No es necesaria ninguna imputaci√≥n.'
        }

    imputation_metadata = {}
    convergence_data = {}

    # Preparar gr√°ficos de diagn√≥stico si se solicita
    if diagnostic_plots:
        n_cols = len(numerical_columns)
        n_rows = sum(X[col].isna().any() for col in numerical_columns)

        if n_rows == 0:
            diagnostic_plots = False
        else:
            fig, axes = plt.subplots(n_rows, 2, figsize=(16, 4*n_rows))
            fig.suptitle('Diagn√≥stico de imputaci√≥n MICE', fontsize=20, fontweight='bold', y=1)
            if n_rows == 1:
                axes = np.array([axes])
            plot_idx = 0

    # Estandarizaci√≥n para una mejor convergencia
    scaler = StandardScaler()
    # Manejar el caso donde algunas columnas podr√≠an ser todas NaN
    X_mean_filled = X.copy()
    for col in X.columns:
        if X[col].isna().any():
            X_mean_filled[col] = X[col].fillna(X[col].mean() if not pd.isna(X[col].mean()) else 0)

    X_scaled = pd.DataFrame(
        scaler.fit_transform(X_mean_filled),
        columns=X.columns,
        index=X.index
    )

    # Restablecer los valores NaN despu√©s de escalar
    for col in X.columns:
        X_scaled.loc[X[col].isna(), col] = np.nan

    # Inicializar el imputador MICE con el estimador apropiado
    if estimator is None:
        estimator = BayesianRidge(max_iter=300)

    mice_imputer = IterativeImputer(
        estimator=estimator,
        max_iter=max_iterations,
        random_state=random_state,
        tol=1e-5,
        imputation_order='ascending',
        sample_posterior=True,
        verbose=0,  # Siempre usar 0 para evitar mensajes de "Completing matrix"
        skip_complete=True
    )

    # Seguimiento de la convergencia si se solicita
    if return_convergence:
        for col in numerical_columns:
            if X[col].isna().any():
                convergence_data[col] = {'iterations': [], 'rmse': []}

    # Realizar la imputaci√≥n
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        X_imputed_scaled = mice_imputer.fit_transform(X_scaled)

    # Volver a la escala original
    X_imputed = pd.DataFrame(
        scaler.inverse_transform(X_imputed_scaled),
        columns=X.columns,
        index=X.index
    )

    # Validaci√≥n cruzada para la evaluaci√≥n de la calidad de la imputaci√≥n si se solicita
    cv_results = {}
    if cross_validation and any(X[col].isna().sum() > 10 for col in X.columns):
        if verbose:
            print("\nRealizando validaci√≥n cruzada para evaluar la calidad de la imputaci√≥n.")

        # Usar tqdm para mostrar una barra de progreso si verbose=True, en lugar de mensajes por columna
        columns_to_validate = [col for col in X.columns if X[col].isna().sum() > 0 and len(X[col].dropna()) >= 20]
        columns_iterator = tqdm(columns_to_validate) if verbose else columns_to_validate

        for col in columns_iterator:
            # Establecer descripci√≥n de la barra de progreso si est√° activada
            if verbose and isinstance(columns_iterator, tqdm):
                columns_iterator.set_description(f"Validando {col}")

            # Realizar el CV s√≥lo en columnas con suficientes valores no ausentes
            non_missing_idx = X[col].dropna().index
            original_values = X.loc[non_missing_idx, col].values

            cv_scores = {'rmse': [], 'r2': []}
            kf = KFold(n_splits=min(n_splits, len(non_missing_idx)//5), shuffle=True, random_state=random_state)

            for train_idx, test_idx in kf.split(non_missing_idx):
                # Crear una copia con valores perdidos introducidos artificialmente
                X_cv = X.copy()
                test_indices = non_missing_idx[test_idx]
                X_cv.loc[test_indices, col] = np.nan

                # Normalizar e imputar
                X_cv_mean_filled = X_cv.copy()
                for c in X_cv.columns:
                    if X_cv[c].isna().any():
                        X_cv_mean_filled[c] = X_cv[c].fillna(X_cv[c].mean() if not pd.isna(X_cv[c].mean()) else 0)

                X_cv_scaled = pd.DataFrame(
                    scaler.transform(X_cv_mean_filled),
                    columns=X_cv.columns,
                    index=X_cv.index
                )

                # Restablecer valores NaN tras el escalado
                for c in X_cv.columns:
                    X_cv_scaled.loc[X_cv[c].isna(), c] = np.nan

                # Usar siempre verbose=0 en el imputer para evitar mensajes
                X_cv_imputed_scaled = mice_imputer.transform(X_cv_scaled)
                X_cv_imputed = pd.DataFrame(
                    scaler.inverse_transform(X_cv_imputed_scaled),
                    columns=X_cv.columns,
                    index=X_cv.index
                )

                # Evaluar la imputaci√≥n en el conjunto de pruebas
                true_values = X.loc[test_indices, col].values
                predicted_values = X_cv_imputed.loc[test_indices, col].values

                rmse = np.sqrt(mean_squared_error(true_values, predicted_values))
                r2 = r2_score(true_values, predicted_values)

                cv_scores['rmse'].append(rmse)
                cv_scores['r2'].append(r2)

            cv_results[col] = {
                'mean_rmse': np.mean(cv_scores['rmse']),
                'std_rmse': np.std(cv_scores['rmse']),
                'mean_r2': np.mean(cv_scores['r2']),
                'std_r2': np.std(cv_scores['r2'])
            }

    # Procesar los resultados de la imputaci√≥n y generar diagn√≥sticos
    missing_columns = [col for col in numerical_columns if X[col].isna().any()]
    if verbose:
        print(f"\nüìä Resumen de imputaci√≥n para {len(missing_columns)} columnas con valores faltantes.")

    for idx, column in enumerate(numerical_columns):
        missing_mask = X[column].isna()
        n_missing = missing_mask.sum()

        if n_missing == 0:
            if verbose:
                print(f"No faltan valores en '{column}'. No es necesaria la imputaci√≥n.")
            continue

        if len(X[column].dropna()) < 5:
            if verbose:
                print(f"Datos insuficientes en '{column}' para una imputaci√≥n fiable. Los resultados pueden ser poco fiables.")

        # Almacenar metadatos de imputaci√≥n
        imputed_values = X_imputed.loc[missing_mask, column]
        original_data = X[column].dropna()

        # Protecci√≥n contra divisi√≥n por cero o datos vac√≠os
        percent_change = np.nan
        if not original_data.empty and original_data.mean() != 0:
            percent_change = (imputed_values.mean() - original_data.mean()) / original_data.mean() * 100

        imputation_metadata[column] = {
            'missing_count': n_missing,
            'missing_percent': n_missing/len(X) * 100,
            'imputed_mean': imputed_values.mean(),
            'imputed_median': imputed_values.median(),
            'imputed_std': imputed_values.std(),
            'imputed_range': (imputed_values.min(), imputed_values.max()),
            'original_mean': original_data.mean(),
            'original_std': original_data.std(),
            'percent_change_mean': percent_change
        }

        # Generar gr√°ficos de diagn√≥stico si se solicita
        if diagnostic_plots and n_missing > 0:
            try:
                # 1. Distribuci√≥n original vs. distribuci√≥n imputada
                if not original_data.empty:
                    sns.histplot(original_data, kde=True, ax=axes[plot_idx, 0], color='#606c38', alpha=0.4, label='Datos originales')
                if len(imputed_values) > 0:
                    sns.histplot(imputed_values, kde=True, ax=axes[plot_idx, 0], color='#bc4749', alpha=0.6, label='Valores imputados')
                axes[plot_idx, 0].set_title(f'Comparaci√≥n de la distribuci√≥n de "{column}"', fontsize=12, pad=15, fontweight='bold')
                axes[plot_idx, 0].legend()
                axes[plot_idx, 0].set_ylabel('Frecuencia', labelpad=12)

                # 2. Comparaci√≥n de boxplot
                data_to_plot = {}
                if not original_data.empty:
                    data_to_plot['Original'] = original_data
                data_to_plot['Imputado'] = imputed_values

                if data_to_plot:
                    combined = pd.DataFrame(data_to_plot)
                    sns.boxplot(data=combined, ax=axes[plot_idx, 1], palette=['#606c38', '#bc4749'])
                    axes[plot_idx, 1].set_title(f'Comparaci√≥n boxplot de "{column}"', fontsize=12, pad=15, fontweight='bold')
                    axes[plot_idx, 1].set_ylabel(f'Valor de "{column}"', labelpad=12)

                plot_idx += 1
            except Exception as e:
                if verbose:
                    print(f"Gr√°ficos de generaci√≥n de errores para la columna '{column}': {str(e)}")

        if verbose:
            print(f'\nüîπ COLUMNA "{column}":')
            print(f"   Valores faltantes: {n_missing} ({n_missing/len(X):.1%})")
            if not original_data.empty:
                print(f"   Datos originales: Media = {original_data.mean():.2f}, Mediana = {original_data.median():.2f}, Desviaci√≥n t√≠pica = {original_data.std():.2f}")
            print(f"   Datos imputados: Media = {imputed_values.mean():.2f}, Mediana = {imputed_values.median():.2f}, Desviaci√≥n t√≠pica = {imputed_values.std():.2f}")
            if not pd.isna(percent_change):
                print(f"   Cambio medio: {percent_change:.1f}%")
            if column in cv_results:
                print(f"   Resultados de la validaci√≥n cruzada: RMSE = {cv_results[column]['mean_rmse']:.3f} ¬± {cv_results[column]['std_rmse']:.3f}, R¬≤ = {cv_results[column]['mean_r2']:.3f} ¬± {cv_results[column]['std_r2']:.3f}")

    if diagnostic_plots and plot_idx > 0:
        try:
            print("\n")
            plt.tight_layout(h_pad=4.0, w_pad=3.0)
            plt.subplots_adjust(top=0.98)
            plt.show()
        except Exception as e:
            if verbose:
                print(f"Error en la visualizaci√≥n de gr√°ficos: {str(e)}")

    # Para las columnas del dataframe original que se imputaron, las copiamos de nuevo
    result_df = df.copy()
    for col in numerical_columns:
        result_df[col] = X_imputed[col]

    # Preparamos el diccionario de devoluci√≥n
    result = {
        'imputed_data': result_df,
        'imputation_metadata': imputation_metadata
    }

    if cross_validation and cv_results:
        result['cv_results'] = cv_results

    if return_convergence:
        result['convergence_data'] = convergence_data

    return result

MICE_result = impute_missing_values_mice(
     df=cleaned_kidney_df,
     max_iterations=15,
     diagnostic_plots=True,
     cross_validation=True,
     verbose=True
)

imputed_df_MICE = MICE_result['imputed_data']
metadata_MICE = MICE_result['imputation_metadata']

numeric_columns_MICE = imputed_df_MICE.select_dtypes(include=['number'])
numeric_columns_MICE.head(10)

numeric_columns_MICE.isna().sum()

"""Para finalizar este apartado y obtener una comprensi√≥n m√°s profunda sobre las t√©cnicas de imputaci√≥n utilizadas, hemos decidido realizar una comparaci√≥n exhaustiva entre los distintos m√©todos aplicados.

El objetivo es evaluar, desde un punto de vista estad√≠stico y visual, c√≥mo se comporta cada t√©cnica respecto a los datos originales. Para ello, hemos desarrollado la funci√≥n `compare_imputation_with_stats`, que automatiza este an√°lisis de manera estructurada, incorporando tanto pruebas estad√≠sticas como m√©tricas de validaci√≥n cruzada, en caso de estar disponibles.

Esta funci√≥n toma como entrada los dataframes resultantes de cada m√©todo de imputaci√≥n, junto con el conjunto original antes del tratamiento de los valores perdidos, y genera una tabla que resume todas las m√©tricas que veremos a continuaci√≥n, facilitando la comparaci√≥n directa entre los m√©todos. Las funcionalidades principales de la funci√≥n son:

*   Seleccionar las variables num√©ricas con datos ausentes en el dataset original.
*   Recuperar los valores imputados por cada t√©cnica y compararlos con los valores originales, calculando:
    *   Media y desviaci√≥n est√°ndar.
    *   Diferencias absolutas respecto a las estad√≠sticas originales.
*   Realizar un conjunto de pruebas estad√≠sticas:
    *   Kolmogorov-Smirnov (KS): eval√∫a si las distribuciones originales e imputadas provienen de la misma distribuci√≥n.
    *   t-test: contrasta si las medias de los grupos originales e imputados son significativamente distintas.
    *   Shapiro-Wilk: analiza la normalidad de los datos, tanto originales como imputados.
*   Evaluar los resultados de validaci√≥n cruzada, calculando la media y desviaci√≥n est√°ndar del RMSE y del R¬≤ para cada t√©cnica y variable imputada.
*   Aplicar varios estilos a los distintos grupos de columnas (medias, desviaciones, tests, m√©tricas) para resaltar los valores m√°s relevantes en la tabla resultante. Esto facilita la interpretaci√≥n de los resultados y la posterior toma de decisiones. Estas funciones de estilo son:
    *   `highlight_min_diff`: resalta las celdas con las diferencias m√°s bajas en las columnas relacionadas con las diferencias.
    *   `highlight_min_rmse`: resalta las celdas con el valor m√°s bajo de RMSE.
    *   `highlight_max_r2`: resalta las celdas con el valor m√°s alto de R¬≤.
    *   `highlight_significant_pvalues`: resalta las celdas con valores p significativos (menores a 0,05).
    *   `highlight_min_var_diff`: resalta las celdas con las diferencias m√°s bajas en la desviaci√≥n est√°ndar.
    *   `highlight_mean_mean_diff`: resalta espec√≠ficamente la columna `Mean_mean_diff`, ya que su valor siempre ser√° 0.
    *   `highlight_info_cells`: resalta las celdas que contienen un mensaje, tenemmos dos tipos:
        *   Gris claro para advertencias o mensajes informativos, como valores "insuficientes", "no v√°lidos" o "no disponibles".
        *   Gris oscuro para errores expl√≠citos, es decir, cuando la celda contiene la palabra "error".
"""

def compare_imputation_with_stats(knn_imputed, mice_imputed, mean_imputation_data, median_imputation_data, original_df, cv_results=None):
    """
    Comparar m√©todos de imputaci√≥n con m√©tricas estad√≠sticas y visualizaciones

    -----------
    Par√°metros:
    -----------
    knn_imputed: Dataframe con valores imputados por KNN.
    mice_imputed: Dataframe con valores imputados por MICE.
    mean_imputation_data: Dataframe con valores imputados por la media (de impute_with_strategy).
    median_imputation_data: Dataframe con valores imputados por la mediana (de impute_with_strategy).
    original_df: Dataframe original antes de la imputaci√≥n.
    cv_results: resultados de validaci√≥n cruzada para RMSE y R¬≤ (opcional).

    --------
    Devuelve:
    --------
    dict: contiene la comparaci√≥n estad√≠stica y visualizaciones.
    """
    import numpy as np
    import pandas as pd
    from scipy import stats
    import warnings

    numerical_columns = original_df.select_dtypes(include=['number']).columns
    cols_with_missing = [col for col in numerical_columns if original_df[col].isna().sum() > 0]

    if not cols_with_missing:
        print("No columns with missing values found.")
        return {'stats_comparison': pd.DataFrame(), 'plots': {}}

    results = {
        'stats_comparison': pd.DataFrame(),
        'plots': {}
    }

    stats_data = []

    # Calcular estad√≠sticas para cada columna
    for column in cols_with_missing:
        original_complete = original_df[column].dropna()
        missing_mask = original_df[column].isna()

        # Obtener valores imputados
        knn_values = knn_imputed.loc[missing_mask, column]
        mice_values = mice_imputed.loc[missing_mask, column]
        mean_values = mean_imputation_data.loc[missing_mask, column]
        median_values = median_imputation_data.loc[missing_mask, column]

        # Inicializar variables para m√©tricas estad√≠sticas con valores que indican el motivo del fallo
        # Usaremos strings especiales para indicar la raz√≥n del fallo
        ks_stat_original_knn = ks_pvalue_original_knn = "Informaci√≥n insuficiente"
        ks_stat_original_mice = ks_pvalue_original_mice = "Informaci√≥n insuficiente"
        ks_stat_original_mean = ks_pvalue_original_mean = "Informaci√≥n insuficiente"
        ks_stat_original_median = ks_pvalue_original_median = "Informaci√≥n insuficiente"

        t_stat_original_knn = t_pvalue_original_knn = "Informaci√≥n insuficiente"
        t_stat_original_mice = t_pvalue_original_mice = "Informaci√≥n insuficiente"
        t_stat_original_mean = t_pvalue_original_mean = "Informaci√≥n insuficiente"
        t_stat_original_median = t_pvalue_original_median = "Informaci√≥n insuficiente"

        shapiro_stat_original = shapiro_pvalue_original = "Informaci√≥n insuficiente"
        shapiro_stat_knn = shapiro_pvalue_knn = "Informaci√≥n insuficiente"
        shapiro_stat_mice = shapiro_pvalue_mice = "Informaci√≥n insuficiente"
        shapiro_stat_mean = shapiro_pvalue_mean = "Informaci√≥n insuficiente"
        shapiro_stat_median = shapiro_pvalue_median = "Informaci√≥n insuficiente"

        with np.errstate(all='ignore'):
            # Calcular KS test (original vs imputados) usando ks_2samp
            if len(original_complete) > 0 and len(original_complete) > 5:
                try:
                    # Verificar que las distribuciones no sean constantes antes de ejecutar KS test
                    if len(knn_values) > 5 and knn_values.nunique() > 1:
                        ks_stat_original_knn, ks_pvalue_original_knn = stats.ks_2samp(
                            original_complete, knn_values
                        )
                    else:
                        ks_stat_original_knn = "Datos insuficientes"
                        ks_pvalue_original_knn = "Datos insuficientes"

                    if len(mice_values) > 5 and mice_values.nunique() > 1:
                        ks_stat_original_mice, ks_pvalue_original_mice = stats.ks_2samp(
                            original_complete, mice_values
                        )
                    else:
                        ks_stat_original_mice = "Datos insuficientes"
                        ks_pvalue_original_mice = "Datos insuficientes"

                    if len(mean_values) > 5 and mean_values.nunique() > 1:
                        ks_stat_original_mean, ks_pvalue_original_mean = stats.ks_2samp(
                            original_complete, mean_values
                        )
                    else:
                        ks_stat_original_mean = "Datos insuficientes"
                        ks_pvalue_original_mean = "Datos insuficientes"

                    if len(median_values) > 5 and median_values.nunique() > 1:
                        ks_stat_original_median, ks_pvalue_original_median = stats.ks_2samp(
                            original_complete, median_values
                        )
                    else:
                        ks_stat_original_median = "Datos insuficientes"
                        ks_pvalue_original_median = "Datos insuficientes"

                except Exception as e:
                    error_msg = str(e)[:20] + "..." if len(str(e)) > 20 else str(e)
                    ks_stat_original_knn = ks_pvalue_original_knn = f"Error: {error_msg}"
                    ks_stat_original_mice = ks_pvalue_original_mice = f"Error: {error_msg}"
                    ks_stat_original_mean = ks_pvalue_original_mean = f"Error: {error_msg}"
                    ks_stat_original_median = ks_pvalue_original_median = f"Error: {error_msg}"

                # Calcular t-test (original vs imputados)
                try:
                    # A√±adir condiciones para evitar problemas de precisi√≥n num√©rica
                    if (len(knn_values) > 1 and knn_values.std() > 1e-10 and
                        original_complete.std() > 1e-10):
                        t_stat_original_knn, t_pvalue_original_knn = stats.ttest_ind(
                            original_complete, knn_values, equal_var=False, nan_policy='omit'
                        )
                    else:
                        t_stat_original_knn = "Varianza insuficiente"
                        t_pvalue_original_knn = "Varianza insuficiente"

                    if (len(mice_values) > 1 and mice_values.std() > 1e-10 and
                        original_complete.std() > 1e-10):
                        t_stat_original_mice, t_pvalue_original_mice = stats.ttest_ind(
                            original_complete, mice_values, equal_var=False, nan_policy='omit'
                        )
                    else:
                        t_stat_original_mice = "Varianza insuficiente"
                        t_pvalue_original_mice = "Varianza insuficiente"

                    if (len(mean_values) > 1 and mean_values.std() > 1e-10 and
                        original_complete.std() > 1e-10):
                        t_stat_original_mean, t_pvalue_original_mean = stats.ttest_ind(
                            original_complete, mean_values, equal_var=False, nan_policy='omit'
                        )
                    else:
                        t_stat_original_mean = "Varianza insuficiente"
                        t_pvalue_original_mean = "Varianza insuficiente"

                    if (len(median_values) > 1 and median_values.std() > 1e-10 and
                        original_complete.std() > 1e-10):
                        t_stat_original_median, t_pvalue_original_median = stats.ttest_ind(
                            original_complete, median_values, equal_var=False, nan_policy='omit'
                        )
                    else:
                        t_stat_original_median = "Varianza insuficiente"
                        t_pvalue_original_median = "Varianza insuficiente"

                except Exception as e:
                    error_msg = str(e)[:20] + "..." if len(str(e)) > 20 else str(e)
                    t_stat_original_knn = t_pvalue_original_knn = f"Error: {error_msg}"
                    t_stat_original_mice = t_pvalue_original_mice = f"Error: {error_msg}"
                    t_stat_original_mean = t_pvalue_original_mean = f"Error: {error_msg}"
                    t_stat_original_median = t_pvalue_original_median = f"Error: {error_msg}"

                # Shapiro-Wilk test para normalidad
                try:
                    # Verificar que las distribuciones no sean constantes antes de Shapiro-Wilk
                    if (len(original_complete) <= 5000 and len(original_complete) >= 3 and
                        original_complete.min() != original_complete.max()):
                        with warnings.catch_warnings():
                            warnings.filterwarnings("ignore", category=UserWarning,
                                                  message="Input data has range zero")
                            shapiro_stat_original, shapiro_pvalue_original = stats.shapiro(original_complete)
                    else:
                        shapiro_stat_original = "Rango no v√°lido"
                        shapiro_pvalue_original = "Rango no v√°lido"

                    if (3 <= len(knn_values) <= 5000 and knn_values.min() != knn_values.max()):
                        with warnings.catch_warnings():
                            warnings.filterwarnings("ignore", category=UserWarning,
                                                  message="Input data has range zero")
                            shapiro_stat_knn, shapiro_pvalue_knn = stats.shapiro(knn_values)
                    else:
                        shapiro_stat_knn = "Rango no v√°lido"
                        shapiro_pvalue_knn = "Rango no v√°lido"

                    if (3 <= len(mice_values) <= 5000 and mice_values.min() != mice_values.max()):
                        with warnings.catch_warnings():
                            warnings.filterwarnings("ignore", category=UserWarning,
                                                  message="Input data has range zero")
                            shapiro_stat_mice, shapiro_pvalue_mice = stats.shapiro(mice_values)
                    else:
                        shapiro_stat_mice = "Rango no v√°lido"
                        shapiro_pvalue_mice = "Rango no v√°lido"

                    if (3 <= len(mean_values) <= 5000 and mean_values.min() != mean_values.max()):
                        with warnings.catch_warnings():
                            warnings.filterwarnings("ignore", category=UserWarning,
                                                  message="Input data has range zero")
                            shapiro_stat_mean, shapiro_pvalue_mean = stats.shapiro(mean_values)
                    else:
                        shapiro_stat_mean = "Rango no v√°lido"
                        shapiro_pvalue_mean = "Rango no v√°lido"

                    if (3 <= len(median_values) <= 5000 and median_values.min() != median_values.max()):
                        with warnings.catch_warnings():
                            warnings.filterwarnings("ignore", category=UserWarning,
                                                  message="Input data has range zero")
                            shapiro_stat_median, shapiro_pvalue_median = stats.shapiro(median_values)
                    else:
                        shapiro_stat_median = "Rango no v√°lido"
                        shapiro_pvalue_median = "Rango no v√°lido"

                except Exception as e:
                    error_msg = str(e)[:20] + "..." if len(str(e)) > 20 else str(e)
                    shapiro_stat_original = shapiro_pvalue_original = f"Error: {error_msg}"
                    shapiro_stat_knn = shapiro_pvalue_knn = f"Error: {error_msg}"
                    shapiro_stat_mice = shapiro_pvalue_mice = f"Error: {error_msg}"
                    shapiro_stat_mean = shapiro_pvalue_mean = f"Error: {error_msg}"
                    shapiro_stat_median = shapiro_pvalue_median = f"Error: {error_msg}"

        # Inicializar variables para RMSE y R¬≤
        rmse_knn = rmse_std_knn = r2_knn = r2_std_knn = "No disponible"
        rmse_mice = rmse_std_mice = r2_mice = r2_std_mice = "No disponible"
        rmse_mean = rmse_std_mean = r2_mean = r2_std_mean = "No disponible"
        rmse_median = rmse_std_median = r2_median = r2_std_median = "No disponible"

        # Extraer datos de validaci√≥n cruzada de otros m√©todos
        if cv_results is not None:
            # Para KNN
            if 'KNN' in cv_results:
                # Si el objeto KNN tiene cv_results como atributo
                if hasattr(knn_imputed, 'cv_results') and column in knn_imputed.cv_results:
                    knn_data = knn_imputed.cv_results[column]
                    if isinstance(knn_data, dict):
                        rmse_knn = knn_data.get('mean_rmse', "No disponible")
                        rmse_std_knn = knn_data.get('std_rmse', "No disponible")
                        r2_knn = knn_data.get('mean_r2', "No disponible")
                        r2_std_knn = knn_data.get('std_r2', "No disponible")

                # Si cv_results tiene la estructura anidada
                elif column in cv_results['KNN']:
                    knn_data = cv_results['KNN'][column]
                    if isinstance(knn_data, dict):
                        rmse_knn = knn_data.get('mean_rmse', "No disponible")
                        rmse_std_knn = knn_data.get('std_rmse', "No disponible")
                        r2_knn = knn_data.get('mean_r2', "No disponible")
                        r2_std_knn = knn_data.get('std_r2', "No disponible")

            # Para MICE
            if 'MICE' in cv_results:
                # Si el objeto KNN tiene cv_results como atributo
                if hasattr(mice_imputed, 'cv_results') and column in mice_imputed.cv_results:
                    mice_data = mice_imputed.cv_results[column]
                    if isinstance(mice_data, dict):
                        rmse_mice = mice_data.get('mean_rmse', "No disponible")
                        rmse_std_mice = mice_data.get('std_rmse', "No disponible")
                        r2_mice = mice_data.get('mean_r2', "No disponible")
                        r2_std_mice = mice_data.get('std_r2', "No disponible")

                # Si cv_results tiene la estructura anidada
                elif column in cv_results['MICE']:
                    mice_data = cv_results['MICE'][column]
                    if isinstance(mice_data, dict):
                        rmse_mice = mice_data.get('mean_rmse', "No disponible")
                        rmse_std_mice = mice_data.get('std_rmse', "No disponible")
                        r2_mice = mice_data.get('mean_r2', "No disponible")
                        r2_std_mice = mice_data.get('std_r2', "No disponible")

            # Para Mean
            if 'Mean' in cv_results and cv_results['Mean']:
                mean_data = None
                if column in cv_results['Mean']:
                    mean_data = cv_results['Mean'][column]

                if isinstance(mean_data, dict):
                    rmse_mean = mean_data.get('mean_rmse', "No disponible")
                    rmse_std_mean = mean_data.get('std_rmse', "No disponible")
                    r2_mean = mean_data.get('mean_r2', "No disponible")

            # Para Median
            if 'Median' in cv_results and cv_results['Median']:
                median_data = None
                if column in cv_results['Median']:
                    median_data = cv_results['Median'][column]

                if isinstance(median_data, dict):
                    rmse_median = median_data.get('mean_rmse', "No disponible")
                    rmse_std_median = median_data.get('std_rmse', "No disponible")
                    r2_median = median_data.get('mean_r2', "No disponible")
                    r2_std_median = median_data.get('std_r2', "No disponible")

        stats_data.append({
            'Variable': column,
            'N_imputed': len(knn_values),
            'Original_mean': original_complete.mean(),
            'KNN_mean': knn_values.mean(),
            'MICE_mean': mice_values.mean(),
            'Mean_mean': mean_values.mean(),
            'Median_mean': median_values.mean(),
            'KNN_mean_diff': abs(knn_values.mean() - original_complete.mean()),
            'MICE_mean_diff': abs(mice_values.mean() - original_complete.mean()),
            'Mean_mean_diff': abs(mean_values.mean() - original_complete.mean()),
            'Median_mean_diff': abs(median_values.mean() - original_complete.mean()),
            'Original_std': original_complete.std(),
            'KNN_std': knn_values.std(),
            'MICE_std': mice_values.std(),
            'Mean_std': mean_values.std(),
            'Median_std': median_values.std(),
            'Std_diff_KNN': abs(knn_values.std() - original_complete.std()),
            'Std_diff_MICE': abs(mice_values.std() - original_complete.std()),
            'Std_diff_Mean': abs(mean_values.std() - original_complete.std()),
            'Std_diff_Median': abs(median_values.std() - original_complete.std()),
            'KS_stat_Original_KNN': ks_stat_original_knn,
            'KS_stat_Original_MICE': ks_stat_original_mice,
            'KS_stat_Original_Mean': ks_stat_original_mean,
            'KS_stat_Original_Median': ks_stat_original_median,
            'KS_pvalue_Original_KNN': ks_pvalue_original_knn,
            'KS_pvalue_Original_MICE': ks_pvalue_original_mice,
            'KS_pvalue_Original_Mean': ks_pvalue_original_mean,
            'KS_pvalue_Original_Median': ks_pvalue_original_median,
            'Ttest_pvalue_Original_KNN': t_pvalue_original_knn,
            'Ttest_pvalue_Original_MICE': t_pvalue_original_mice,
            'Ttest_pvalue_Original_Mean': t_pvalue_original_mean,
            'Ttest_pvalue_Original_Median': t_pvalue_original_median,
            'Shapiro_stat_Original': shapiro_stat_original,
            'Shapiro_stat_KNN': shapiro_stat_knn,
            'Shapiro_stat_MICE': shapiro_stat_mice,
            'Shapiro_stat_Mean': shapiro_stat_mean,
            'Shapiro_stat_Median': shapiro_stat_median,
            'Shapiro_pvalue_KNN': shapiro_pvalue_knn,
            'Shapiro_pvalue_MICE': shapiro_pvalue_mice,
            'Shapiro_pvalue_Mean': shapiro_pvalue_mean,
            'Shapiro_pvalue_Median': shapiro_pvalue_median,
            'RMSE_KNN': rmse_knn,
            'RMSE_MICE': rmse_mice,
            'RMSE_Mean': rmse_mean,
            'RMSE_Median': rmse_median,
            'R¬≤_KNN': r2_knn,
            'R¬≤_MICE': r2_mice,
            'R¬≤_Mean': r2_mean,
            'R¬≤_Median': r2_median,
        })

    stats_df = pd.DataFrame(stats_data)
    stats_df.set_index('Variable', inplace=True)

    # Funci√≥n para aplicar estilos m√∫ltiples
    def apply_styles(stats_df):
        # Definir los grupos de columnas
        column_groups = {
            'intro': ['Variable', 'N_imputed'],
            'mean': ['Original_mean', 'KNN_mean', 'MICE_mean', 'Mean_mean', 'Median_mean'],
            'mean_diff': ['KNN_mean_diff', 'MICE_mean_diff', 'Mean_mean_diff', 'Median_mean_diff'],
            'std': ['Original_std', 'KNN_std', 'MICE_std', 'Mean_std', 'Median_std'],
            'std_diff': ['Std_diff_KNN', 'Std_diff_MICE', 'Std_diff_Mean', 'Std_diff_Median'],
            'ks_test_stat': ['KS_stat_Original_KNN', 'KS_stat_Original_MICE', 'KS_stat_Original_Mean','KS_stat_Original_Median'],
            'ks_test_pvalue': ['KS_pvalue_Original_KNN', 'KS_pvalue_Original_MICE', 'KS_pvalue_Original_Mean', 'KS_pvalue_Original_Median'],
            't_test': ['Ttest_pvalue_Original_KNN', 'Ttest_pvalue_Original_MICE', 'Ttest_pvalue_Original_Mean', 'Ttest_pvalue_Original_Median'],
            'shapiro_test_stat': ['Shapiro_stat_Original', 'Shapiro_stat_KNN', 'Shapiro_stat_MICE', 'Shapiro_stat_Mean','Shapiro_stat_Median'],
            'shapiro_test_pvalue': ['Shapiro_pvalue_KNN', 'Shapiro_pvalue_MICE','Shapiro_pvalue_Mean', 'Shapiro_pvalue_Median'],
            'rmse': ['RMSE_KNN', 'RMSE_MICE', 'RMSE_Mean', 'RMSE_Median'],
            'r2': ['R¬≤_KNN', 'R¬≤_MICE', 'R¬≤_Mean', 'R¬≤_Median']
        }

        # 1. Resaltar las celdas con valores de diferencia m√°s bajos
        def highlight_min_diff(row):
            diff_cols = ['KNN_mean_diff', 'MICE_mean_diff', 'Median_mean_diff']
            valid_vals = []

            for c in diff_cols:
                if c in row.index and not isinstance(row[c], str) and not pd.isna(row[c]):
                    valid_vals.append((c, row[c]))

            if not valid_vals:
                return [''] * len(row)

            min_col, min_val = min(valid_vals, key=lambda x: x[1])
            return ['background-color: #468faf' if c == min_col else '' for c in row.index]

        # 2. Resaltar las celdas con valores de RMSE m√°s bajos
        def highlight_min_rmse(row):
            rmse_cols = ['RMSE_KNN', 'RMSE_MICE', 'RMSE_Mean', 'RMSE_Median']
            valid_vals = []

            for c in rmse_cols:
                if c in row.index and not isinstance(row[c], str) and not pd.isna(row[c]):
                    valid_vals.append((c, row[c]))

            if not valid_vals:
                return [''] * len(row)

            min_col, min_val = min(valid_vals, key=lambda x: x[1])
            return ['background-color: #6a994e' if c == min_col else '' for c in row.index]

        # 3. Resaltar las celdas con valores de R¬≤ m√°s altos
        def highlight_max_r2(row):
            r2_cols = ['R¬≤_KNN', 'R¬≤_MICE', 'R¬≤_Mean', 'R¬≤_Median']
            valid_vals = []

            for c in r2_cols:
                if c in row.index and not isinstance(row[c], str) and not pd.isna(row[c]):
                    valid_vals.append((c, row[c]))

            if not valid_vals:
                return [''] * len(row)

            max_col, max_val = max(valid_vals, key=lambda x: x[1])
            return ['background-color: #f9a620' if c == max_col else '' for c in row.index]

        # 4. Resaltar valores p bajos (< 0.05) para las pruebas estad√≠sticas
        def highlight_significant_pvalues(row):
            pvalue_cols = [c for c in row.index if 'pvalue' in c.lower()]
            result = []

            for c, v in row.items():
                if c in pvalue_cols and not isinstance(v, str) and not pd.isna(v) and 0 < v < 0.05:
                    result.append('background-color: #bc4749')
                else:
                    result.append('')

            return result

        # 5. Resaltar las celdas con valores de diferencia de desviaci√≥n est√°ndar m√°s bajos
        def highlight_min_var_diff(row):
            var_diff_cols = ['Std_diff_KNN', 'Std_diff_MICE', 'Std_diff_Mean', 'Std_diff_Median']
            valid_vals = []

            for c in var_diff_cols:
                if c in row.index and not isinstance(row[c], str) and not pd.isna(row[c]):
                    valid_vals.append((c, row[c]))

            if not valid_vals:
                return [''] * len(row)

            min_col, min_val = min(valid_vals, key=lambda x: x[1])
            return ['background-color: #9d4edd' if c == min_col else '' for c in row.index]

        # 6. Resaltar espec√≠ficamente la columna Mean_mean_diff
        def highlight_mean_mean_diff(s):
            is_mean_mean_diff = s.name == 'Mean_mean_diff'
            return ['background-color: #0077b6' if is_mean_mean_diff else '' for _ in s]

        # 7. Marcar celdas con mensajes de error y advertencia con un color de fondo
        def highlight_info_cells(df):
            info_background = 'background-color: #6c757d'
            error_background = 'background-color: #212529'

            # Inicializar matriz de estilos del mismo tama√±o que el DataFrame
            styled = pd.DataFrame('', index=df.index, columns=df.columns)

            # Detectar mensajes de informaci√≥n y error en el DataFrame
            for i in df.index:
                for c in df.columns:
                    val = df.loc[i, c]
                    if isinstance(val, str):
                        if "insuficiente" in val.lower() or "no v√°lido" in val.lower() or "no disponible" in val.lower():
                            styled.loc[i, c] = info_background
                        elif "error" in val.lower():
                            styled.loc[i, c] = error_background

            return styled

        # Aplicar separadores entre grupos de columnas
        def add_separators(df):
            # Encontrar las columnas de borde (√∫ltima columna de cada grupo)
            border_columns = []
            for group_cols in column_groups.values():
                for col in group_cols:
                    if col in df.columns:
                        border_columns.append(col)
                        break

            # Aplicar borde izquierdo a las columnas de borde
            styles = []
            for col in border_columns:
                if col in df.columns:
                    styles.append({
                        'selector': f'td.col{list(df.columns).index(col)}',
                        'props': [('border-left', '3px solid #000000')]
                    })

            return styles

        # Aplicar los estilos en secuencia
        styled = stats_df.style.apply(highlight_min_diff, axis=1)
        styled = styled.apply(highlight_min_rmse, axis=1)
        styled = styled.apply(highlight_max_r2, axis=1)
        styled = styled.apply(highlight_significant_pvalues, axis=1)
        styled = styled.apply(highlight_min_var_diff, axis=1)
        styled = styled.apply(highlight_mean_mean_diff, axis=0)

        # Aplicar estilos a celdas con mensajes informativos
        info_styles = highlight_info_cells(stats_df)
        for i in info_styles.index:
            for c in info_styles.columns:
                if info_styles.loc[i, c]:
                    styled.set_properties(**{'background-color': info_styles.loc[i, c].replace('background-color: ', '')},
                                       subset=pd.IndexSlice[i, c])

        # A√±adir separadores
        styled = styled.set_table_styles(add_separators(stats_df))

        return styled

    styled_df = apply_styles(stats_df)

    results['stats_comparison'] = stats_df
    results['styled_comparison'] = styled_df

    print("\nüîπ Comparaci√≥n estad√≠stica de m√©todos de imputaci√≥n:\n")
    try:
        from IPython.display import display
        display(styled_df)
    except (NameError, ImportError):
        print("Tabla de comparaci√≥n estad√≠stica generada")
        print(stats_df)

    return results

def run_comparison_analysis(original_df, knn_imputed, mice_imputed, imputation_results, cv_results=None):
    """
    Ejecuta el an√°lisis comparativo entre varios m√©todos de imputaci√≥n

    -----------
    Parametros:
    -----------
    original_df : DataFrame original con valores faltantes
    knn_imputed : DataFrame con valores imputados por KNN
    mice_imputed : DataFrame con valores imputados por MICE
    imputation_results : resultados de las imputaciones por media y mediana
    cv_results : resultados de validaci√≥n cruzada para RMSE y R¬≤ (opcional)

    --------
    Returns:
    --------
    dict: resultados del an√°lisis comparativo
    """
    # Obtener los DataFrames de los resultados de imputaci√≥n por media y mediana
    mean_imputation_data = imputation_results['mean_imputation']['imputed_data']
    median_imputation_data = imputation_results['median_imputation']['imputed_data']

    # Preparar CV results de manera m√°s robusta
    prepared_cv_results = {
        'KNN': {},
        'MICE': {},
        'Mean': {},
        'Median': {}
    }

    # Extraer CV results de AdaptiveKNNImputer si est√° disponible
    if hasattr(knn_imputed, 'cv_results'):
        prepared_cv_results['KNN'] = knn_imputed.cv_results

    # Extraer resultados de MICE si est√°n disponibles
    mice_cv_results = {}
    if hasattr(mice_imputed, 'cv_results'):
        mice_cv_results = mice_imputed.cv_results

    # O usar cv_results si se proporciona expl√≠citamente
    if cv_results is not None:
        if 'KNN' in cv_results and cv_results['KNN']:
            # S√≥lo sobrescribir si no se ha obtenido de knn_imputed
            if not prepared_cv_results['KNN'] and cv_results['KNN']:
                prepared_cv_results['KNN'] = cv_results['KNN']

        if 'MICE' in cv_results and cv_results['MICE']:
            prepared_cv_results['MICE'] = cv_results['MICE']

        if 'Mean' in cv_results and cv_results['Mean']:
            prepared_cv_results['Mean'] = cv_results['Mean']

        if 'Median' in cv_results and cv_results['Median']:
            prepared_cv_results['Median'] = cv_results['Median']

    # Extraer resultados de imputaci√≥n por media/mediana si est√°n disponibles
    if 'cv_results' in imputation_results.get('mean_imputation', {}):
        prepared_cv_results['Mean'] = imputation_results['mean_imputation']['cv_results']

    if 'cv_results' in imputation_results.get('median_imputation', {}):
        prepared_cv_results['Median'] = imputation_results['median_imputation']['cv_results']

    # Ejecutar la comparaci√≥n
    comparison_results = compare_imputation_with_stats(
        knn_imputed=knn_imputed,
        mice_imputed=mice_imputed,
        mean_imputation_data=mean_imputation_data,
        median_imputation_data=median_imputation_data,
        original_df=original_df,
        cv_results=prepared_cv_results
    )

    return comparison_results

def compare_all_methods(cleaned_kidney_df, numeric_columns_KNN, numeric_columns_MICE, imputation_results):
    """
    Funci√≥n de nivel superior para ejecutar la comparaci√≥n de todos los m√©todos.

    -----------
    Par√°metros:
    -----------
    cleaned_kidney_df: DataFrame original con valores faltantes
    numeric_columns_KNN: DataFrame con valores imputados por KNN
    numeric_columns_MICE: DataFrame con valores imputados por MICE
    imputation_results: resultados de imputaciones por media/mediana

    --------
    Retorna:
    --------
    dict: resultados de la comparaci√≥n
    """

    # Extraer resultados CV directamente de los objetos si es posible
    knn_cv_results = {}
    mice_cv_results = {}

    # Verificar si numeric_columns_KNN es un objeto AdaptiveKNNImputer o tiene cv_results
    if hasattr(numeric_columns_KNN, 'cv_results'):
        knn_cv_results = numeric_columns_KNN.cv_results
    elif isinstance(numeric_columns_KNN, dict) and 'cv_results' in numeric_columns_KNN:
        knn_cv_results = numeric_columns_KNN['cv_results']

    # Verificar si numeric_columns_MICE tiene cv_results
    if hasattr(numeric_columns_MICE, 'cv_results'):
        mice_cv_results = numeric_columns_MICE.cv_results
    elif isinstance(numeric_columns_MICE, dict) and 'mice_imputation' in numeric_columns_MICE:
        if 'cv_results' in numeric_columns_MICE['mice_imputation']:
            mice_cv_results = numeric_columns_MICE['mice_imputation']['cv_results']

    # Preparar el diccionario de resultados CV
    cv_results = {
        'KNN': knn_cv_results,
        'MICE': mice_cv_results,
        'Mean': imputation_results.get('mean_imputation', {}).get('cv_results', {}),
        'Median': imputation_results.get('median_imputation', {}).get('cv_results', {})
    }

    comparison_results = compare_imputation_with_stats(
        knn_imputed=numeric_columns_KNN,
        mice_imputed=numeric_columns_MICE,
        mean_imputation_data=imputation_results['mean_imputation']['imputed_data'],
        median_imputation_data=imputation_results['median_imputation']['imputed_data'],
        original_df=cleaned_kidney_df,
        cv_results=cv_results
    )

    return comparison_results

comparison_results = compare_imputation_with_stats(
    knn_imputed=imputed_df_KNN,
    mice_imputed=imputed_df_MICE,
    mean_imputation_data=imputation_results['mean_imputation']['imputed_data'],
    median_imputation_data=imputation_results['median_imputation']['imputed_data'],
    original_df=cleaned_kidney_df,
    cv_results={
        'KNN': KNN_result.get('cv_results', {}),
        'MICE': MICE_result.get('cv_results', {}),
        'Mean': imputation_results.get('mean_imputation', {}).get('cv_results', {}),
        'Median': imputation_results.get('median_imputation', {}).get('cv_results', {})
    }
)

"""Para la imputaci√≥n de variables num√©ricas se evaluaron cuatro enfoques: imputaci√≥n por media, por mediana, mediante K-Nearest Neighbors (KNN) y utilizando el modelo de imputaci√≥n iterativa MICE.

La comparaci√≥n se realiz√≥ con base en m√∫ltiples m√©tricas estad√≠sticas: diferencias en media y desviaci√≥n est√°ndar, pruebas de hip√≥tesis (t de Student, Kolmogorov-Smirnov, Shapiro-Wilk), y m√©tricas de error como RMSE y R¬≤. A continuaci√≥n, se presenta un an√°lisis detallado por cada m√©trica:

1. Diferencia en medias: se calcularon las diferencias absolutas entre la media original y la media resultante tras la imputaci√≥n con cada m√©todo. Esto permite evaluar qu√© tan bien cada t√©cnica preserva el valor central original de la variable. Por ejemplo, en la variable "age":
    *   KNN tuvo una desviaci√≥n media de 3.81 unidades respecto al valor original.
    *   MICE present√≥ una desviaci√≥n algo mayor, de 5.74 unidades.
    *   El m√©todo de la media, por definici√≥n, no mostr√≥ desviaci√≥n.
    *   La mediana imputada difiri√≥ en 3.51 unidades.

 Dependiendo de la variable las distribuciones de las diferencias cambian. Aun as√≠ observando la tabla, se sugiere que KNN suele preservar mejor la media original, aunque puede depender de la simetr√≠a de la distribuci√≥n subyacente.

2. Diferencia en la desviaci√≥n est√°ndar: se compar√≥ la desviaci√≥n est√°ndar antes y despu√©s de la imputaci√≥n, y se calcul√≥ la diferencia para evaluar c√≥mo cada m√©todo afecta la dispersi√≥n de los datos. En el caso de la variable "bp":
    *   La diferencia de la desviaci√≥n est√°ndar se redujo 6.89 unidades con KNN.
    *   Con MICE, pr√°cticamente se mantuvo sin cambios (0.019).

 Este patr√≥n se repiti√≥ en la mayor√≠a de las variables, lo que sugiere que KNN tiende a subestimar la variabilidad, mientras que MICE logra conservarla de forma mucho m√°s fiel.

3. Prueba de Kolmogorov-Smirnov (KS): se aplic√≥ la prueba de KS para comparar la distribuci√≥n completa de los datos originales vs. imputados. Por ejemplo, para la variable "sg":
    *   El estad√≠stico KS fue de 0.51 con KNN.
    *   Mientras que con MICE fue de 0.29.

 En ambos casos, los valores p fueron inferiores a 0.01, indicando diferencias estad√≠sticamente significativas con respecto a la distribuci√≥n original. Sin embargo, los valores del estad√≠stico en MICE apuntan a una mejor aproximaci√≥n a la distribuci√≥n original en comparaci√≥n con KNN.

4. Prueba t de Student: se utiliz√≥ la prueba t para contrastar las medias de los datos imputados frente a los datos originales (sin valores perdidos). Por ejemplo, en la variable "bu":
    *   KNN present√≥ un p-valor de 0.54.
    *   MICE obtuvo un p-valor de 0.75.

 Ambos valores sugieren que no existe una diferencia estad√≠sticamente significativa en las medias, aunque MICE mostr√≥ mayor consistencia en tambi√©n otras variables.

5. Prueba de normalidad (Shapiro-Wilk): se evalu√≥ si los datos imputados segu√≠an una distribuci√≥n normal, lo cual puede ser relevante en algunos modelos estad√≠sticos. En el caso de la variable "sc":
    *   MICE obtuvo un estad√≠stico de 0.725 y un p-valor cercano a 0, lo que indica una clara desviaci√≥n de la normalidad.
    *   KNN mostr√≥ un estad√≠stico de 0.978 y un p-valor 0.169, lo que sugiere una distribuci√≥n m√°s cercana a la normal.

 Este resultado evidencia que, si bien MICE preserva mejor la distribuci√≥n original, no garantiza que los datos imputados sigan una distribuci√≥n normal, lo cual es esperable en muchos conjuntos de datos reales.

6. M√©tricas de error: se calcularon el Error Cuadr√°tico Medio (RMSE) y el coeficiente de determinaci√≥n (R¬≤) para estimar el ajuste entre los valores imputados y los verdaderos. Para la variable "sod":
    *   RMSE fue similar entre m√©todos, 10.27 para KNN y 9.8 para MICE.
    *   MICE alcanz√≥ un R¬≤ de -0.71, mientras que para KNN fue de -0.014.

 MICE tiende a presentar valores de RMSE m√°s bajos que KNN en la mayor√≠a de las variables. Esto indica que, en t√©rminos absolutos, los errores de imputaci√≥n de MICE son m√°s peque√±os. Sin embargo, KNN suele mostrar valores de R¬≤ m√°s altos (o menos negativos), lo que sugiere retener mejor la tendencia general de los datos, capturando mejor la varianza de la variable original.

Algunas de las pruebas estad√≠sticas no pudieron aplicarse en las imputaciones realizadas por media y mediana a causa de tener datos y varianza insuficiente, lo que limita la capacidad de evaluaci√≥n de estos m√©todos en contextos reales.

Como conclusi√≥n podriamos decir que MICE demostr√≥ ser el m√©todo de imputaci√≥n m√°s robusto para variables num√©ricas, con mejores resultados en p-valores, preservaci√≥n de la desviaci√≥n est√°ndar, y un ajuste global m√°s estable frente a las otras t√©cnicas. Aunque KNN puede ofrecer menor desviaci√≥n en la media respecto a la media original, su tendencia a subestimar la dispersi√≥n y su bajo rendimiento en RMSE lo hacen menos confiable en general.

#### 3.2.2. Variables categ√≥ricas

Una vez completado el an√°lisis de las variables num√©ricas, el siguiente paso consiste en abordar el estudio de las variables categ√≥ricas. Para mantener la coherencia metodol√≥gica, se seguir√° una estructura similar a la empleada previamente. Como punto de partida, antes de aplicar cualquier t√©cnica de imputaci√≥n, se llevar√° a cabo un an√°lisis exploratorio preliminar que permita identificar qu√© columnas categ√≥ricas presentan datos ausentes.

Con este objetivo, se implementa el siguiente fragmento de c√≥digo, el cual genera un resumen detallado sobre la cantidad absoluta y el porcentaje relativo de valores faltantes en cada una de las columnas categ√≥ricas del conjunto de datos.
"""

# Obtener las columnas categ√≥ricas con valores NA
categorical_cols_with_na = cleaned_kidney_df.select_dtypes(include=['object']).columns[
    cleaned_kidney_df.select_dtypes(include=['object']).isna().any()
].tolist()

# Obtener el total de registros (filas) en el DataFrame
total_registros = len(cleaned_kidney_df)

# Calcular el n√∫mero de valores NA por columna num√©rica
na_counts_cat = cleaned_kidney_df[categorical_cols_with_na].isna().sum()
na_percentages_cat = (na_counts_cat / total_registros) * 100

# Crear DataFrame de resumen
resumen_faltantes = pd.DataFrame({
    'Variable': categorical_cols_with_na,
    'Valores totales': total_registros,
    'Valores faltantes': [f"{val} ({pct:.2f}%)" for val, pct in zip(na_counts_cat, na_percentages_cat)],
    'Valores v√°lidos': [f"{total_registros - val} ({(100 - pct):.2f}%)" for val, pct in zip(na_counts_cat, na_percentages_cat)]
})

print("üîπ Resumen completo de las variables categ√≥ricas:\n")

styled_table4 = resumen_faltantes.style.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})

styled_table4 = styled_table4.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

styled_table4

"""El primer m√©todo de imputaci√≥n que implementaremos para este tipo de variables es la imputaci√≥n por moda. Este enfoque se basa en reemplazar los valores ausentes por la categor√≠a m√°s frecuente (la moda) dentro de cada variable. La simplicidad de este m√©todo lo convierte en una opci√≥n com√∫n en etapas iniciales del an√°lisis, especialmente cuando las distribuciones no presentan una gran dispersi√≥n o cuando la proporci√≥n de valores faltantes es baja.

Para operacionalizar esta t√©cnica de manera sistem√°tica, hemos desarrollado la clase `CategoryModeImputer`. Esta implementaci√≥n busca no solo completar los datos faltantes, sino tambi√©n ofrecer una evaluaci√≥n rigurosa del impacto que tiene esta t√©cnica y su fiabilidad. Esta clase incorpora funcionalidades avanzadas, tales como:
*   Identificaci√≥n autom√°tica de columnas categ√≥ricas con valores faltantes.
*   Aplicaci√≥n del imputador `SimpleImputer` de `scikit-learn` con estrategia `most_frequent`.
*   Validaci√≥n cruzada para estimar la precisi√≥n del m√©todo. El proceso divide los datos no faltantes en `n_folds` (pliegues) y, en cada iteraci√≥n, uno de los pliegues se usa como conjunto de validaci√≥n, mientras que los otros se emplean para entrenar el modelo de imputaci√≥n. Luego, se comparan los valores imputados con los valores originales en el conjunto de validaci√≥n para calcular un puntaje de precisi√≥n.
*   Generaci√≥n de un informe detallado por variable, que incluye:
    *   Proporci√≥n de datos faltantes.
    *   Valor de la moda utilizada para la imputaci√≥n.
    *   Cambios en la distribuci√≥n de frecuencias antes y despu√©s del tratamiento.
    *   Puntaje de validaci√≥n cruzada y su interpretaci√≥n cualitativa (excelente, buena, moderada o baja).
*   Visualizaci√≥n gr√°fica de la distribuci√≥n original e imputada mediante gr√°ficos circulares para facilitar la comparaci√≥n.

La ejecuci√≥n del pipeline completo se realiza a trav√©s de la funci√≥n auxiliar `run_mode_imputation_pipeline`, que aplica la clase previamente definida sobre el conjunto de datos de entrada y devuelve el nuevo DataFrame con los valores imputados.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from typing import List, Optional, Dict, Any
from sklearn.metrics import accuracy_score

class CategoryModeImputer:
    """
    Imputaci√≥n categ√≥rica utilizando SimpleImputer con estrategia de moda.
    Incluye validaci√≥n cruzada, visualizaci√≥n de resultados y reportes de imputaci√≥n.
    """

    def __init__(
        self,
        verbose: bool = True,
        random_state: int = 42
    ):
        """
        Inicializa el imputador con los par√°metros especificados.

        -----------
        Par√°metros:
        -----------
        verbose : indica si se deben imprimir reportes detallados de imputaci√≥n
        random_state : semilla aleatoria para reproducibilidad
        """

        self.verbose = verbose
        self.random_state = random_state
        self.imputation_report = {}
        self.cv_scores = {}

    def impute(
        self,
        df: pd.DataFrame,
        columns: Optional[List[str]] = None,
        plot: bool = True,
        cv_folds: int = 5
    ) -> pd.DataFrame:
        """
        Imputa valores faltantes en columnas categ√≥ricas usando la moda.

        Par√°metros:
        -----------
        df : dataframe de entrada
        columns : columnas categ√≥ricas espec√≠ficas a imputar (si es None, se usan todas las categ√≥ricas)
        plot : indica si se deben mostrar gr√°ficos de los resultados
        cv_folds : n√∫mero de folds para validaci√≥n cruzada

        --------
        Devuelve:
        --------
        pd.DataFrame: Dataframe con valores imputados
        """

        # Identificar columnas categ√≥ricas
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        if columns is not None:
            cat_cols = [col for col in columns if col in cat_cols]

        # Encontrar columnas con valores faltantes
        missing_cols = [col for col in cat_cols if df[col].isna().any()]

        if not missing_cols:
            if self.verbose:
                print("No hay valores faltantes en columnas categ√≥ricas")
            return df.copy()

        # Crear una copia del dataframe original
        result_df = df.copy()

        # Almacenar distribuciones originales
        original_dists = {col: df[col].value_counts(dropna=False) for col in missing_cols}

        # Crear el imputador SimpleImputer con estrategia 'most_frequent' (moda)
        imputer = SimpleImputer(strategy='most_frequent')

        # Imputar cada columna por separado
        for col in missing_cols:
            # Guardar valores originales para an√°lisis posterior
            original_series = df[col].copy()

            # Ejecutar validaci√≥n cruzada utilizando los valores faltantes reales del dataset
            cv_score = self._cross_validate_imputation(df, col, cv_folds)
            self.cv_scores[col] = cv_score

            # Aplicar imputaci√≥n - FIX: Reshape the results to 1D array
            col_values = df[[col]].values
            imputed_values = imputer.fit_transform(col_values)

            # Actualizar dataframe con valores imputados
            # FIX: Convert 2D array to 1D array before assignment
            result_df[col] = imputed_values.flatten()

            # Generar reporte para esta columna
            self.imputation_report[col] = self._generate_column_report(
                original_series,
                result_df[col],
                original_dists[col],
                cv_score
            )

        # Mostrar reporte si verbose es True
        if self.verbose:
            self._print_comprehensive_report()

        # Mostrar gr√°ficos si plot es True
        if plot and missing_cols:
            self._plot_distributions(df, result_df, missing_cols)

        return result_df

    def _cross_validate_imputation(self, df: pd.DataFrame, col: str, n_folds: int) -> float:
        """
        Realiza validaci√≥n cruzada para medir la calidad de la imputaci√≥n.
        Implementa una verdadera validaci√≥n cruzada de n_folds sobre los datos no faltantes
        para evaluar la precisi√≥n de la imputaci√≥n.

        --------
        Devuelve:
        --------
        float: Puntuaci√≥n media de precisi√≥n en la validaci√≥n cruzada
        """
        # Identificar √≠ndices sin valores faltantes en esta columna
        non_missing_idx = df[~df[col].isna()].index.tolist()

        if len(non_missing_idx) < n_folds:  # Necesitamos suficientes datos para n_folds
            if self.verbose:
                print(f"Advertencia: No hay suficientes datos no faltantes para {n_folds} folds en la columna {col}")
            if len(non_missing_idx) == 0:
                return 0.0  # No hay datos para validar
            n_folds = max(2, len(non_missing_idx) // 2)  # Ajustar n_folds si hay pocos datos

        # Aplicar la semilla aleatoria para reproducibilidad
        np.random.seed(self.random_state)
        np.random.shuffle(non_missing_idx)

        # Dividir en n_folds aproximadamente iguales
        fold_size = len(non_missing_idx) // n_folds
        folds = [non_missing_idx[i*fold_size:(i+1)*fold_size] for i in range(n_folds)]
        # Asegurarse de que el √∫ltimo fold contiene todos los elementos restantes
        folds[-1].extend(non_missing_idx[n_folds*fold_size:])

        scores = []

        # Para cada fold
        for i in range(n_folds):
            # Usar el fold actual como conjunto de validaci√≥n
            validation_idx = folds[i]

            # Los √≠ndices restantes son para entrenamiento
            train_idx = []
            for j in range(n_folds):
                if j != i:
                    train_idx.extend(folds[j])

            # Guardar valores reales para comparar despu√©s
            validation_values = df.loc[validation_idx, col].values

            # Crear un dataframe temporal con los valores de validaci√≥n marcados como faltantes
            temp_df = df.copy()
            temp_df.loc[validation_idx, col] = np.nan

            # Ajustar el imputador con los datos de entrenamiento
            imputer = SimpleImputer(strategy='most_frequent')
            imputer.fit(temp_df.loc[train_idx, [col]])

            # Imputar los valores "faltantes" (realmente valores de validaci√≥n)
            imputed_values = imputer.transform(temp_df.loc[validation_idx, [col]]).flatten()

            # Calcular precisi√≥n para este fold
            fold_score = accuracy_score(validation_values, imputed_values)
            scores.append(fold_score)

        # Calcular y devolver la media de las puntuaciones
        if scores:
            return np.mean(scores)
        else:
            # Si no pudimos calcular scores, estimamos la calidad bas√°ndonos en
            # qu√© tan dominante es la moda
            value_counts = df[col].value_counts(normalize=True)
            if not value_counts.empty:
                mode_probability = value_counts.iloc[0]  # Probabilidad de la moda
                return mode_probability  # Mayor probabilidad = mayor confianza
            else:
                return 0.0

    def _interpret_cv_score(self, score: float) -> str:
        """Interpretar la puntuaci√≥n de validaci√≥n cruzada."""

        if score >= 0.8:
            return "Excelente"
        elif score >= 0.6:
            return "Buena"
        elif score >= 0.4:
            return "Moderada"
        else:
            return "Baja"

    def _generate_column_report(
        self,
        original_series: pd.Series,
        imputed_series: pd.Series,
        original_dist: pd.Series,
        cv_score: float
    ) -> Dict[str, Any]:
        """
        Genera un reporte detallado para una columna.
        """

        # Calcular valores faltantes
        original_missing = original_series.isna().sum()
        missing_percentage = original_missing / len(original_series) * 100

        # Distribuci√≥n original vs imputada
        orig_dist = original_series.value_counts(dropna=True, normalize=True)
        imputed_dist = imputed_series.value_counts(normalize=True)

        # Cambio en la distribuci√≥n
        dist_change = {}
        for category in set(orig_dist.index) | set(imputed_dist.index):
            orig_pct = orig_dist.get(category, 0) * 100
            imputed_pct = imputed_dist.get(category, 0) * 100
            dist_change[category] = {
                'original_pct': orig_pct,
                'imputed_pct': imputed_pct,
                'change': imputed_pct - orig_pct
            }

        # Valor de la moda usada para imputaci√≥n
        mode_value = original_series.mode()[0] if not original_series.dropna().empty else None

        # Interpretar la puntuaci√≥n de validaci√≥n cruzada
        cv_quality = self._interpret_cv_score(cv_score)

        return {
            'missing_values': original_missing,
            'missing_percentage': missing_percentage,
            'distribution_change': dist_change,
            'mode_value': mode_value,
            'cv_score': cv_score,
            'cv_quality': cv_quality
        }

    def _plot_distributions(
        self,
        original_df: pd.DataFrame,
        imputed_df: pd.DataFrame,
        columns: List[str]
    ):
        """Grafica las distribuciones original e imputada en gr√°ficas separadas para cada variable."""

        n_cols = len(columns)
        if n_cols == 0:
            return

        # Crear una figura separada para cada columna
        print("\n")
        for col in columns:
            plt.figure(figsize=(12, 5))

            # Crear dos subplots en la misma figura
            plt.subplot(1, 2, 1)

            # Distribuci√≥n original
            orig_series = original_df[col].dropna()
            if not orig_series.empty:
                orig_counts = orig_series.value_counts()
                orig_labels = orig_counts.index
                orig_percentages = orig_counts / len(orig_series) * 100

                plt.pie(
                    orig_percentages,
                    labels=orig_labels,
                    autopct='%1.1f%%',
                    colors=["#006d77", "#83c5be"]
                )
                plt.title(f'Distribuci√≥n original', fontweight='bold', y=0.95)
            else:
                plt.text(0.5, 0.5, "No hay datos no faltantes",
                         ha='center', va='center', fontsize=12)
                plt.title(f'Original: {col} (Sin datos)', fontweight='bold', y=0.95)

            # Distribuci√≥n imputada
            plt.subplot(1, 2, 2)
            imputed_counts = imputed_df[col].value_counts()
            imputed_labels = imputed_counts.index
            imputed_percentages = imputed_counts / len(imputed_df) * 100

            plt.pie(
                imputed_percentages,
                labels=imputed_labels,
                autopct='%1.1f%%',
                colors=["#9d4edd", "#c77dff"]
            )

            mode_value = self.imputation_report[col]['mode_value']
            plt.title(f'Imputaci√≥n (moda utilizada: {mode_value})', fontweight='bold', y=0.95)

            # T√≠tulo principal para esta variable
            cv_quality = self.imputation_report[col]['cv_quality']
            cv_score = self.imputation_report[col]['cv_score']
            plt.suptitle(
                f'Comparaci√≥n de imputaci√≥n por moda para "{col}"',
                fontsize=16, fontweight='bold', y=1
            )

            plt.tight_layout()
            plt.show()
            print("\n")

    def _print_comprehensive_report(self):
        """Imprime un reporte completo de imputaci√≥n."""
        print("\n" + "="*50)
        print("üìä INFORME DE IMPUTACI√ìN POR MODA")
        print("="*50)

        for col, report in self.imputation_report.items():
            print(f'\nüîπ COLUMNA "{col}":')
            print(f"  Valores faltantes: {report['missing_values']} ({report['missing_percentage']:.2f}%)")
            print(f"  Valor utilizado para imputaci√≥n (moda): {report['mode_value']}")
            print(f"  Calidad de imputaci√≥n: {report['cv_quality']} (CV score: {report['cv_score']:.2f})")

            print("  Cambios en la distribuci√≥n:")
            for category, change in report['distribution_change'].items():
                print(f"    {category}:")
                print(f"      Original: {change['original_pct']:.2f}%, Imputada: {change['imputed_pct']:.2f}%")
                print(f"      Cambio: {change['change']:+.2f}%")

def run_mode_imputation_pipeline(df, categorical_columns=None, cv_folds=5):
    """
    Ejecuta el pipeline completo de imputaci√≥n por moda para columnas categ√≥ricas

    -----------
    Par√°metros:
    -----------
    df : Dataframe de entrada con valores faltantes
    categorical_columns : columnas categ√≥ricas espec√≠ficas a imputar (si es None, se usan todas)
    cv_folds : n√∫mero de folds para validaci√≥n cruzada

    --------
    Devuelve:
    --------
    pd.DataFrame: Dataframe con valores imputados
    """

    # Crear y ejecutar imputador
    mode_imputer = CategoryModeImputer(verbose=True)
    imputed_df = mode_imputer.impute(df, columns=categorical_columns, cv_folds=cv_folds)

    return imputed_df

df_mode_cat = run_mode_imputation_pipeline(cleaned_kidney_df, cv_folds=5)

"""Siguiendo el mismo enfoque adoptado para las variables num√©ricas, al concluir la aplicaci√≥n de cada m√©todo de imputaci√≥n, llevaremos a cabo una verificaci√≥n del proceso para evaluar la efectividad de cada estrategia en las variables categ√≥ricas.

El objetivo principal de esta verificaci√≥n es garantizar que los valores faltantes (NaN) hayan sido completamente eliminados de las columnas categ√≥ricas. Esta evaluaci√≥n permitir√° confirmar si las t√©cnicas implementadas han logrado su prop√≥sito de manera adecuada, asegurando la integridad y coherencia del conjunto de datos imputado.
"""

categoric_columns_mode = df_mode_cat.select_dtypes(include=['object'])
categoric_columns_mode.head(10)

categoric_columns_mode.isna().sum()

"""Adem√°s utilizaremos unos m√©todos con un enfoque m√°s sofisticado que tengan en cuenta tanto la estructura de los datos como la posible correlaci√≥n entre variables. Por eso, los dos siguientes m√©todos que utilizaremos para imputar valores faltantes en variables categ√≥ricas se basan en:
*   Imputaci√≥n por K vecinos m√°s cercanos (KNN): este m√©todo se apoya en la idea de que las observaciones similares probablemente tengan categor√≠as similares.
*   Imputaci√≥n Iterativa con clasificadores: inspirada en la t√©cnica de MICE, esta estrategia utiliza un modelo predictivo para estimar los valores faltantes bas√°ndose en las dem√°s variables.

Ambos m√©todos los hemos integrado en una clase personalizada denominada `CategoricalImputer`, la cual no s√≥lo permite imputar los datos, sino tambi√©n:
*   Seleccionar el mejor valor de k de manera autom√°tica (si se usa KNN).
*   Evaluar la calidad de la imputaci√≥n mediante validaci√≥n cruzada.
*   Generar un informe detallado del impacto de la imputaci√≥n realizada en la distribuci√≥n de categor√≠as. Para cada columna imputada muestra:
    *   El n√∫mero y porcentaje de valores faltantes.
    *   La calidad de imputaci√≥n medida por validaci√≥n cruzada.
    *   C√≥mo cambi√≥ la distribuci√≥n de categor√≠as antes y despu√©s de la imputaci√≥n.
    *   Adem√°s, para la imputaci√≥n por KNN, se muestra el valor adaptativo de k utilizado.
*   Imprimir los resultados generados por `compare_imputation_results` en un formato sencillo de comprender y explicativo:
    *   Incluye interpretaciones autom√°ticas seg√∫n los valores de Chi-cuadrado y su valor-p, informaci√≥n mutua (MI) y entrop√≠a.
    *   Muestra si hay evidencia estad√≠stica de diferencias entre m√©todos o si son similares en sus resultados de imputaci√≥n.
*   Generar dos tipos de visualizaciones para comparar visualmente los m√©todos:
    *   Gr√°ficos de torta (pie charts) para observar la distribuci√≥n original (sin NaNs), la distribuci√≥n tras imputaci√≥n por KNN y la distribuci√≥n tras imputaci√≥n iterativa.
    *   Gr√°fico de barras comparativas (si los valores imputados difieren), para ver qu√© categor√≠as fueron imputadas por cada m√©todo.

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Union, Optional, Dict, Any, Tuple
from sklearn.preprocessing import OrdinalEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.neighbors import NearestNeighbors
from scipy import stats
from sklearn.metrics import mutual_info_score, accuracy_score

class CategoricalImputer:
    """
    Imputaci√≥n categ√≥rica avanzada con selecci√≥n k adaptativa, validaci√≥n cruzada
    y visualizaci√≥n integral.
    """

    def __init__(
        self,
        method: str = 'iterative',
        random_state: int = 42,
        verbose: bool = True,
        max_k: int = 10,
        n_estimators: int = 50,
        cv_folds: int = 5
    ):
        """
        Inicializar el imputador con los par√°metros especificados.

        -----------
        Par√°metros:
        -----------
        method : m√©todo de imputaci√≥n. Opciones:
            - 'knn': imputaci√≥n K-Vecinos M√°s Cercanos (K-Nearest Neighbors).
            - 'iterative': imputaci√≥n m√∫ltiple mediante ecuaciones encadenadas.
        random_state : semilla aleatoria para reproducibilidad.
        verbose : si se imprimen informes detallados de imputaci√≥n.
        max_k : n√∫mero m√°ximo de vecinos a considerar para la selecci√≥n adaptativa de k.
        n_estimators : n√∫mero de estimadores para RandomForestClassifier.
        cv_folds : n√∫mero de pliegues para la validaci√≥n cruzada.
        """

        self.method = method
        self.random_state = random_state
        self.verbose = verbose
        self.max_k = max_k
        self.n_estimators = n_estimators
        self.cv_folds = cv_folds
        self.adaptive_k = None
        self._validate_method()

        # Almacenar los resultados de imputaci√≥n para su posterior an√°lisis
        self.imputation_report = {}
        self.cv_results = {}
        self.encoder = None

    def _validate_method(self):
        """Validar el m√©todo de imputaci√≥n."""

        valid_methods = ['knn', 'iterative']
        if self.method not in valid_methods:
            raise ValueError(
                f"El m√©todo debe ser uno de {valid_methods}. "
                f"Recibido: {self.method}"
            )

    def _find_optimal_k(self, X: np.ndarray, y: np.ndarray) -> int:
        """
        Encuentra de forma adaptativa el n√∫mero √≥ptimo de vecinos utilizando la validaci√≥n cruzada.

        -----------
        Par√°metros:
        -----------
        X : datos de entrada para la selecci√≥n k.
        y : variable objetivo para la selecci√≥n supervisada.

        --------
        Devuelve:
        --------
        int: n√∫mero √≥ptimo de vecinos.
        """

        if X.shape[0] <= self.max_k:
            return max(3, X.shape[0] // 2)

        best_k = 1
        best_score = float('-inf')
        kf = KFold(n_splits=min(self.cv_folds, X.shape[0]), shuffle=True, random_state=self.random_state)

        # Si tenemos una variable objetivo, utilizamos la selecci√≥n k supervisada
        if y is not None and len(np.unique(y)) > 1:
            for k in range(1, min(self.max_k + 1, X.shape[0])):
                scores = []
                for train_idx, test_idx in kf.split(X):
                    X_train, X_test = X[train_idx], X[test_idx]
                    y_train, y_test = y[train_idx], y[test_idx]

                    # Omitir si no hay suficientes muestras en una clase
                    if len(np.unique(y_train)) < 2:
                        continue

                    try:
                        # Utiliza RandomForestClassifier con k como max_features
                        clf = RandomForestClassifier(
                            n_estimators=self.n_estimators,
                            max_features=min(k, X.shape[1]),
                            random_state=self.random_state
                        )
                        clf.fit(X_train, y_train)
                        score = clf.score(X_test, y_test)
                        scores.append(score)
                    except Exception as e:
                        if self.verbose:
                            print(f"Error en CV para k={k}: {str(e)}")
                        continue

                if scores:
                    avg_score = np.mean(scores)
                    if avg_score > best_score:
                        best_score = avg_score
                        best_k = k
        else:
            # Selecci√≥n k no supervisada basada en distancias vecinas
            for k in range(1, min(self.max_k + 1, X.shape[0])):
                try:
                    # Crear un modelo de NearestNeighbors
                    nn = NearestNeighbors(n_neighbors=k)
                    nn.fit(X)

                    # Calcula la distancia promedio a los vecinos k m√°s cercanos
                    distances, _ = nn.kneighbors(X)
                    avg_distance = np.mean(distances[:, -1])

                    # Utiliza la distancia promedio negativa como indicador de la calidad de imputaci√≥n
                    score = -avg_distance

                    if score > best_score:
                        best_score = score
                        best_k = k
                except Exception as e:
                    if self.verbose:
                        print(f"Error en selecci√≥n de k={k}: {str(e)}")
                    continue

        return best_k

    def _perform_cross_validation(self, df: pd.DataFrame, target_cols: List[str]) -> Dict[str, Any]:
        """
        Realizar validaci√≥n cruzada para la evaluaci√≥n de la calidad de la imputaci√≥n.

        -----------
        Par√°metros:
        -----------
        df : Dataframe de entrada.
        target_cols : columnas que se utilizar√°n como objetivos en la validaci√≥n cruzada.

        --------
        Devuelve:
        --------
        Dict[str, Any]: resultados de la validaci√≥n cruzada.
        """

        results = {}

        # Obtener columnas con valores faltantes
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        missing_cols = [col for col in cat_cols if df[col].isna().any()]

        if not missing_cols:
            return results

        # Para cada columna con valores faltantes, realiza una CV utilizando otras
        # columnas como caracter√≠sticas
        for col in missing_cols:
            # Omitir si no hay suficientes valores no faltantes
            if df[col].notna().sum() < self.cv_folds:
                results[col] = {"cv_score": np.nan, "imputation_quality": "Datos insuficientes"}
                continue

            # Seleccionar filas donde no falte esta columna
            train_mask = df[col].notna()
            train_df = df[train_mask].copy()

            # Seleccionar caracter√≠sticas (otras columnas categ√≥ricas + columnas objetivo)
            features = [c for c in cat_cols if c != col and c in df.columns]
            features.extend([c for c in target_cols if c != col and c in df.columns])

            # Omitir si no hay caracteristicas disponibles
            if not features:
                results[col] = {"cv_score": np.nan, "imputation_quality": "No hay caracteristicas disponibles"}
                continue

            try:
                # Codificar caracter√≠sticas y objetivos
                X_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                y_encoder = OrdinalEncoder()

                X = X_encoder.fit_transform(train_df[features])
                y = y_encoder.fit_transform(train_df[[col]])

                # Realizar validaci√≥n cruzada
                if self.method == 'knn':
                    if self.adaptive_k is None:
                        self.adaptive_k = self._find_optimal_k(X, y.ravel())
                    imputer = KNNImputer(n_neighbors=self.adaptive_k)
                else:
                    estimator = RandomForestClassifier(
                        n_estimators=self.n_estimators,
                        random_state=self.random_state
                    )
                    imputer = IterativeImputer(
                        estimator=estimator,
                        max_iter=20,
                        random_state=self.random_state
                    )

                # Definir la estrategia de validaci√≥n cruzada
                kf = KFold(n_splits=min(self.cv_folds, sum(train_mask)),
                          shuffle=True, random_state=self.random_state)

                # Realizar manualmente CV para evaluar la calidad de la imputaci√≥n
                cv_scores = []
                for train_idx, test_idx in kf.split(X):
                    X_train, X_test = X[train_idx], X[test_idx]
                    y_train, y_test = y[train_idx], y[test_idx]

                    # Crear una m√°scara de valores originales en el conjunto de pruebas
                    mask = np.random.rand(len(y_test)) < 0.3

                    if sum(mask) > 0:  # S√≥lo si tenemos valores que imputar
                        # Crear una copia con valores enmascarados
                        X_test_masked = X_test.copy()

                        # Ajuste del imputador a los datos de entrenamiento
                        if self.method == 'knn':
                            combined = np.hstack((X_train, y_train))
                            imputer.fit(combined)

                            # Preparar datos de prueba para la imputaci√≥n
                            X_test_with_target = np.hstack((X_test, y_test))
                            X_test_masked_with_target = X_test_with_target.copy()
                            X_test_masked_with_target[mask, -1] = np.nan

                            # Imputar valores faltantes
                            imputed = imputer.transform(X_test_masked_with_target)
                            imputed_y = imputed[mask, -1]

                            # Comparar con los valores originales
                            original_y = y_test[mask].ravel()
                        else:
                            # Para imputador iterativo, entrene un clasificador
                            clf = RandomForestClassifier(
                                n_estimators=self.n_estimators,
                                random_state=self.random_state
                            )
                            clf.fit(X_train, y_train.ravel())

                            # Predecir valores faltantes
                            imputed_y = clf.predict(X_test[mask])
                            original_y = y_test[mask].ravel()

                        # Calcular la precisi√≥n
                        original_y_rounded = np.round(original_y).astype(int)
                        imputed_y_rounded = np.round(imputed_y).astype(int)
                        score = accuracy_score(original_y_rounded, imputed_y_rounded)
                        cv_scores.append(score)

                # Calcular la puntuaci√≥n media del CV
                if cv_scores:
                    avg_score = np.mean(cv_scores)
                    results[col] = {
                        "cv_score": avg_score,
                        "imputation_quality": self._interpret_cv_score(avg_score)
                    }
                else:
                    results[col] = {"cv_score": np.nan, "imputation_quality": "CV failed"}

            except Exception as e:
                results[col] = {"cv_score": np.nan, "imputation_quality": f"Error: {str(e)}"}

        return results

    def _interpret_cv_score(self, score: float) -> str:
        """Interpretar la puntuaci√≥n de validaci√≥n cruzada."""

        if score >= 0.8:
            return "Excelente"
        elif score >= 0.6:
            return "Buena"
        elif score >= 0.4:
            return "Moderada"
        else:
            return "Baja"

    def impute(
        self,
        df: pd.DataFrame,
        columns: Optional[List[str]] = None,
        target_cols: Optional[List[str]] = None,
        plot: bool = True
    ) -> pd.DataFrame:
        """
        Imputar valores faltantes en columnas categ√≥ricas.

        -----------
        Par√°metros:
        -----------
        df : Dataframe de entrada.
        columns : columnas categ√≥ricas espec√≠ficas para imputar (si None, se utilizan todas las columnas categ√≥ricas)
        target_cols : columnas adicionales para usar como objetivos en la validaci√≥n cruzada.
        plot : si se deben graficar los resultados de imputaci√≥n.

        --------
        Devuelve:
        --------
        pd.DataFrame: Dataframe con valores imputados.
        """

        # Identificar columnas categ√≥ricas
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        if columns is not None:
            cat_cols = [col for col in columns if col in cat_cols]

        # Buscar columnas con valores faltantes
        missing_cols = [col for col in cat_cols if df[col].isna().any()]

        if not missing_cols:
            if self.verbose:
                print("No faltan valores en las columnas categ√≥ricas")
            return df

        df_cat = df[cat_cols].copy()

        # Almacenar distribuciones originales
        original_dists = {col: df_cat[col].value_counts(dropna=False) for col in missing_cols}

        # Establecer columnas de destino predeterminadas si no se proporcionan
        if target_cols is None:
            target_cols = list(df.select_dtypes(include=['number']).columns)

        # Realizar validaci√≥n cruzada para la evaluaci√≥n de la calidad
        self.cv_results = self._perform_cross_validation(df, target_cols)

        # Preparar diccionario de resultados de imputaci√≥n
        self.imputation_report = {}

        # Crea un codificador una vez y lo reutiliza
        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)

        # Manejo separado para cada columna con valores faltantes
        imputed_cols = {}
        for col in missing_cols:
            # Omitir si no faltan valores
            if not df[col].isna().any():
                imputed_cols[col] = df[col]
                continue

            # Seleccionar caracter√≠sticas (otras columnas)
            features = [c for c in cat_cols if c != col]
            # Agrega columnas de objetivos num√©ricos si est√°n disponibles
            features.extend([c for c in target_cols if c != col and c in df.columns])

            # Omitir si no hay funciones disponibles
            if not features:
                imputed_cols[col] = df[col]
                continue

            try:
                # Separar los datos completos de los incompletos
                complete_mask = df[col].notna()
                incomplete_mask = df[col].isna()

                # Si no hay suficientes datos completos, utilice la imputaci√≥n modal
                if sum(complete_mask) < 5:
                    mode_value = df[col].mode()[0] if not df[col].dropna().empty else None
                    imputed_col = df[col].copy()
                    imputed_col.fillna(mode_value, inplace=True)
                    imputed_cols[col] = imputed_col
                    continue

                # Funciones de codificaci√≥n
                X_all = self.encoder.fit_transform(df[features])

                if self.method == 'knn':
                    # Para KNN, primero necesitamos encontrar el k √≥ptimo
                    if self.adaptive_k is None:
                        y_complete = self.encoder.fit_transform(df.loc[complete_mask, [col]])
                        X_complete = X_all[complete_mask]
                        self.adaptive_k = self._find_optimal_k(X_complete, y_complete.ravel())

                    # Utiliza la imputaci√≥n KNN
                    imputer = KNNImputer(n_neighbors=self.adaptive_k)

                    # Codifica la columna de destino cuando est√© disponible
                    y_encoder = OrdinalEncoder()
                    y_values = df[col].copy().astype('object')  # Convertir a objeto para una codificaci√≥n consistente
                    encoded_y = np.full(len(df), np.nan)
                    encoded_y[complete_mask] = y_encoder.fit_transform(df.loc[complete_mask, [col]]).ravel()

                    # Combinar caracter√≠sticas y objetivos para la imputaci√≥n KNN
                    combined = np.column_stack((X_all, encoded_y))

                    # Imputar los datos combinados
                    imputed_combined = imputer.fit_transform(combined)

                    # Extraer valores objetivo imputados
                    imputed_y = imputed_combined[:, -1]

                    # Volver a las categor√≠as originales
                    imputed_categories = y_encoder.inverse_transform(imputed_y.reshape(-1, 1)).ravel()

                    # Crear la columna imputada
                    imputed_col = pd.Series(imputed_categories, index=df.index)

                else:  # m√©todo iterativo
                    # Para la imputaci√≥n iterativa, entrena un clasificador
                    clf = RandomForestClassifier(
                        n_estimators=self.n_estimators,
                        random_state=self.random_state
                    )

                    # Codifica la columna de destino para el entrenamiento
                    y_encoder = OrdinalEncoder()
                    y_train = y_encoder.fit_transform(df.loc[complete_mask, [col]]).ravel()

                    # Entrena con datos completos
                    clf.fit(X_all[complete_mask], y_train)

                    # Predecir valores faltantes
                    predicted_encoded = clf.predict(X_all[incomplete_mask])

                    # Convertir las predicciones de nuevo a las categor√≠as originales
                    predicted_categories = y_encoder.inverse_transform(
                        predicted_encoded.reshape(-1, 1)).ravel()

                    # Crear la columna imputada
                    imputed_col = df[col].copy()
                    imputed_col.iloc[incomplete_mask] = predicted_categories

                imputed_cols[col] = imputed_col

                # Generar informe de columnas
                self.imputation_report[col] = self._generate_column_report(
                    df[col],
                    imputed_col,
                    original_dists[col]
                )

            except Exception as e:
                if self.verbose:
                    print(f"Error al imputar la columna {col}: {str(e)}")

        # Crear una copia del marco de datos y actualizar las columnas imputadas
        result_df = df.copy()
        for col, imputed_col in imputed_cols.items():
            result_df[col] = imputed_col

        # Primero imprime el informe completo antes de trazar
        if self.verbose:
            self._print_comprehensive_report()

        # Luego grafica si se te solicita
        if plot and missing_cols:
            self._plot_distributions(df, result_df, missing_cols)

        # Devolver s√≥lo las columnas imputadas
        return result_df[missing_cols]

    def _generate_column_report(
        self,
        original_series: pd.Series,
        imputed_series: pd.Series,
        original_dist: pd.Series
    ) -> Dict[str, Any]:
        """
        Generar informe detallado para una sola columna.
        """

        # Calcular valores faltantes
        original_missing = original_series.isna().sum()
        missing_percentage = original_missing / len(original_series) * 100

        # Distribuci√≥n original vs. distribuci√≥n imputada
        orig_dist = original_series.value_counts(dropna=True, normalize=True)
        imputed_dist = imputed_series.value_counts(normalize=True)

        # Cambio de distribuci√≥n
        dist_change = {}
        for category in set(orig_dist.index) | set(imputed_dist.index):
            orig_pct = orig_dist.get(category, 0) * 100
            imputed_pct = imputed_dist.get(category, 0) * 100
            dist_change[category] = {
                'original_pct': orig_pct,
                'imputed_pct': imputed_pct,
                'change': imputed_pct - orig_pct
            }

        # Obtener calidad de validaci√≥n cruzada si est√° disponible
        cv_quality = "No evaluada"
        cv_score = np.nan
        if self.cv_results and original_series.name in self.cv_results:
            cv_result = self.cv_results[original_series.name]
            cv_quality = cv_result.get("imputation_quality", "No evaluada")
            cv_score = cv_result.get("cv_score", np.nan)

        return {
            'missing_values': original_missing,
            'missing_percentage': missing_percentage,
            'distribution_change': dist_change,
            'cv_quality': cv_quality,
            'cv_score': cv_score
        }

    def _plot_distributions(
        self,
        original_df: pd.DataFrame,
        imputed_df: pd.DataFrame,
        columns: List[str]
    ):
        """Grafica las distribuciones original e imputada."""

        n_cols = len(columns)
        if n_cols == 0:
            return

        fig, axes = plt.subplots(n_cols, 2, figsize=(12, 4*n_cols))

        # Asegurar de que los ejes sean siempre una matriz 2D
        if n_cols == 1:
            axes = np.array([axes]).reshape(1, 2)

        fig.suptitle(
            f'Comparaci√≥n de imputaci√≥n categ√≥rica por el m√©todo ({self.method})',
            fontsize=16, fontweight='bold', y=1.02)

        for idx, col in enumerate(columns):
            # Distribuci√≥n original
            orig_series = original_df[col].dropna()  # Solo valores no faltantes
            if not orig_series.empty:
                orig_counts = orig_series.value_counts()
                orig_labels = orig_counts.index
                orig_percentages = orig_counts / len(orig_series) * 100

                axes[idx, 0].pie(
                    orig_percentages,
                    labels=orig_labels,
                    autopct='%1.1f%%',
                    colors=plt.cm.Pastel1.colors
                )
                axes[idx, 0].set_title(f'Original: {col}')
            else:
                axes[idx, 0].text(0.5, 0.5, "No hay datos no faltantes",
                                ha='center', va='center', fontsize=12)
                axes[idx, 0].set_title(f'Original: {col} (Sin datos)')

            # Distribuci√≥n imputada
            imputed_counts = imputed_df[col].value_counts()
            imputed_labels = imputed_counts.index
            imputed_percentages = imputed_counts / len(imputed_df) * 100

            axes[idx, 1].pie(
                imputed_percentages,
                labels=imputed_labels,
                autopct='%1.1f%%',
                colors=plt.cm.Pastel2.colors
            )
            cv_quality = "No evaluada"
            if col in self.cv_results:
                cv_quality = self.cv_results[col].get("imputation_quality", "No evaluada")
            axes[idx, 1].set_title(f'Imputado: {col} (Calidad: {cv_quality})')

        plt.tight_layout()
        plt.subplots_adjust(top=0.95, hspace=0.4)
        plt.show()

    def _print_comprehensive_report(self):
        """Imprime un informe de imputaci√≥n completo."""

        print("\n" + "="*50)
        print(f"üìö INFORME DE IMPUTACI√ìN POR EL M√âTODO {self.method.upper()}")
        print("="*50)

        if self.method == 'knn':
            print(f"Valor k adaptativo: {self.adaptive_k}")

        for col, report in self.imputation_report.items():
            print(f'\nüîπ COLUMNA "{col}":')
            print(f"  Valores faltantes: {report['missing_values']} ({report['missing_percentage']:.2f}%)")
            print(f"  Calidad de imputaci√≥n: {report['cv_quality']} (Score CV: {report['cv_score']:.4f})")

            print("  Cambios en la distribuci√≥n:")
            for category, change in report['distribution_change'].items():
                print(f"    {category}:")
                print(f"      Original: {change['original_pct']:.2f}%, Imputada: {change['imputed_pct']:.2f}%")
                print(f"      Cambio: {change['change']:+.2f}%")

    @staticmethod
    def compare_imputation_results(
        original_df: pd.DataFrame,
        knn_imputed_df: pd.DataFrame,
        iterative_imputed_df: pd.DataFrame
    ) -> Dict[str, Any]:
        """
        Compara resultados de KNN y m√©todos de imputaci√≥n iterativos.
        """

        categorical_columns = original_df.select_dtypes(include=['object', 'category']).columns
        comparison_results = {}

        for col in categorical_columns:
            # Omitir si la columna no tiene valores faltantes
            if not original_df[col].isna().any():
                continue

            missing_mask = original_df[col].isna()
            knn_imputed_values = knn_imputed_df.loc[missing_mask, col]
            iter_imputed_values = iterative_imputed_df.loc[missing_mask, col]

            # Calcular m√©tricas de acuerdo
            total_imputed = len(knn_imputed_values)
            if total_imputed > 0:
                agreement_count = sum(knn_imputed_values == iter_imputed_values)
                agreement_percentage = agreement_count / total_imputed * 100
            else:
                agreement_count = 0
                agreement_percentage = np.nan

            # C√°lculo de entrop√≠a en conjuntos de datos completos
            def calculate_entropy(series):
                if len(series) == 0:
                    return np.nan
                freq = series.value_counts(normalize=True)
                if len(freq) == 0:
                    return np.nan
                return -np.sum(freq * np.log2(freq + 1e-10))

            # Calcular la entrop√≠a en distribuciones completas
            knn_entropy = calculate_entropy(knn_imputed_df[col])
            iter_entropy = calculate_entropy(iterative_imputed_df[col])

            try:
                # Crear tabla de contingencia de distribuciones completas
                categories = list(set(knn_imputed_df[col].unique()) | set(iterative_imputed_df[col].unique()))
                knn_dist = knn_imputed_df[col].value_counts().reindex(categories, fill_value=0)
                iter_dist = iterative_imputed_df[col].value_counts().reindex(categories, fill_value=0)

                # Prueba de chi-cuadrado en distribuciones completas
                contingency = np.array([knn_dist.values, iter_dist.values])
                chi2_stat, chi2_p = stats.chi2_contingency(contingency)[:2]
            except (ValueError, np.linalg.LinAlgError):
                chi2_stat, chi2_p = np.nan, np.nan

            try:
                # Convertir a c√≥digos categ√≥ricos
                knn_encoded = pd.Categorical(knn_imputed_df[col]).codes
                iter_encoded = pd.Categorical(iterative_imputed_df[col]).codes

                # Calcular informaci√≥n mutua sobre distribuciones completas
                mi_score = mutual_info_score(knn_encoded, iter_encoded)

                # Normalizar la informaci√≥n mutua
                max_entropy = max(calculate_entropy(knn_imputed_df[col]),
                                  calculate_entropy(iterative_imputed_df[col]))

                if max_entropy > 0:
                    mi_score_normalized = mi_score / max_entropy
                else:
                    mi_score_normalized = mi_score
            except Exception:
                mi_score = np.nan
                mi_score_normalized = np.nan

            comparison_results[col] = {
                'chi2_statistic': chi2_stat,
                'chi2_p_value': chi2_p,
                'mutual_information': mi_score,
                'mutual_information_normalized': mi_score_normalized,
                'agreement_percentage': agreement_percentage,
                'knn_entropy': knn_entropy,
                'iterative_entropy': iter_entropy,
                'imputed_count': total_imputed
            }

        return comparison_results

    @staticmethod
    def print_imputation_comparison(comparison_results: Dict[str, Any]):
        """
        Imprime un informe comparativo completo.
        """

        print("\n" + "="*50)
        print("üìã COMPARACI√ìN DE M√âTODOS DE IMPUTACI√ìN")
        print("="*50)

        if not comparison_results:
            print("No hay columnas con valores faltantes para comparar.")
            return

        # Resultados detallados
        for col, results in comparison_results.items():
            print(f'\nüîπ COLUMNA "{col}":')
            print(f"  Total de valores imputados: {results['imputed_count']}")

            if not np.isnan(results['agreement_percentage']):
                print(f"  Porcentaje de concordancia: {results['agreement_percentage']:.2f}%")
            else:
                print("  Porcentaje de concordancia: no disponible")

            if not np.isnan(results['chi2_statistic']):
                print(f"  Estad√≠stico Chi-cuadrado: {results['chi2_statistic']:.4f}")
                print(f"  Valor-p Chi-cuadrado: {results['chi2_p_value']:.4f}")

                if results['chi2_p_value'] < 0.05:
                    print("      üîç Interpretaci√≥n: las distribuciones son significativamente diferentes")
                else:
                    print("      üîç Interpretaci√≥n: no hay evidencia de diferencias significativas entre m√©todos")
            else:
                print("  Estad√≠stico Chi-cuadrado: no disponible")

            if not np.isnan(results['mutual_information']):
                print(f"  Informaci√≥n mutua: {results['mutual_information']:.4f}")
                if 'mutual_information_normalized' in results and not np.isnan(results['mutual_information_normalized']):
                    print(f"  Informaci√≥n mutua normalizada: {results['mutual_information_normalized']:.4f}")

                    if results['mutual_information_normalized'] > 0.7:
                        print("      üîç Interpretaci√≥n: alta similitud entre m√©todos")
                    elif results['mutual_information_normalized'] > 0.3:
                        print("      üîç Interpretaci√≥n: similitud moderada entre m√©todos")
                    else:
                        print("      üîç Interpretaci√≥n: baja similitud entre m√©todos")
            else:
                print("  Informaci√≥n Mutua: No disponible")

            if not np.isnan(results['knn_entropy']) and not np.isnan(results['iterative_entropy']):
                print(f"  Entrop√≠a KNN (dataset completo): {results['knn_entropy']:.4f}")
                print(f"  Entrop√≠a Iterativa (dataset completo): {results['iterative_entropy']:.4f}")

                entropy_diff = abs(results['knn_entropy'] - results['iterative_entropy'])
                if entropy_diff < 0.1:
                    print("      üîç Interpretaci√≥n: distribuciones muy similares")
                elif entropy_diff < 0.5:
                    print("      üîç Interpretaci√≥n: distribuciones moderadamente similares")
                else:
                    print("      üîç Interpretaci√≥n: distribuciones con diferencias notables")
            else:
                print("  An√°lisis de entrop√≠a: no disponible")

    def visualize_comparison(
        self,
        original_df: pd.DataFrame,
        knn_imputed_df: pd.DataFrame,
        iterative_imputed_df: pd.DataFrame
    ):
        """
        Visualiza la comparaci√≥n entre los m√©todos de imputaci√≥n KNN e iterativa.

        -----------
        Par√°metros:
        -----------
        original_df : Dataframe original con valores faltantes.
        knn_imputed_df : Dataframe con imputaci√≥n KNN.
        iterative_imputed_df : Dataframe con imputaci√≥n iterativa.
        """

        print("\n" + "="*68)
        print("üìä GR√ÅFICOS COMPARATIVOS DE LOS DIFERENTES M√âTODOS DE IMPUTACI√ìN")
        print("="*68)

        categorical_columns = original_df.select_dtypes(include=['object', 'category']).columns
        missing_cols = [col for col in categorical_columns if original_df[col].isna().any()]

        if not missing_cols:
            print("No hay columnas categ√≥ricas con valores faltantes para visualizar.")
            return

        # Comparaci√≥n de gr√°ficos para cada columna con valores faltantes
        print("\n")
        for col in missing_cols:
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            fig.suptitle(f'Comparaci√≥n de m√©todos de imputaci√≥n para "{col}"', fontsize=16, fontweight='bold', y=1)

            # Distribuci√≥n original (sin valores faltantes)
            orig_series = original_df[col].dropna()
            if not orig_series.empty:
                orig_counts = orig_series.value_counts()
                axes[0].pie(
                    orig_counts.values,
                    labels=orig_counts.index,
                    autopct='%1.1f%%',
                    colors=["#006d77", "#83c5be"]
                )
                axes[0].set_title('Distribuci√≥n original', fontweight='bold', y=0.95)
            else:
                axes[0].text(0.5, 0.5, "No hay datos no faltantes",
                           ha='center', va='center', fontsize=12)
                axes[0].set_title('Distribuci√≥n original (sin datos)', fontweight='bold', y=0.95)

            # Distribuci√≥n imputada de KNN
            knn_counts = knn_imputed_df[col].value_counts()
            axes[1].pie(
                knn_counts.values,
                labels=knn_counts.index,
                autopct='%1.1f%%',
                colors=["#9d4edd", "#c77dff"]
            )
            axes[1].set_title('Imputaci√≥n KNN', fontweight='bold', y=0.95)

            # Distribuci√≥n imputada iterativa
            iter_counts = iterative_imputed_df[col].value_counts()
            axes[2].pie(
                iter_counts.values,
                labels=iter_counts.index,
                autopct='%1.1f%%',
                colors=["#9d4edd", "#c77dff"]
            )
            axes[2].set_title('Imputaci√≥n Iterativa', fontweight='bold', y=0.95)

            plt.tight_layout()
            plt.show()

            # Gr√°fico de barras para comparar √∫nicamente valores imputados
            missing_mask = original_df[col].isna()
            print("\n")

            if sum(missing_mask) > 0:
                knn_imputed_values = knn_imputed_df.loc[missing_mask, col]
                iter_imputed_values = iterative_imputed_df.loc[missing_mask, col]

                # Comprobar si los valores son diferentes
                if not (knn_imputed_values == iter_imputed_values).all():
                    # Obtener categor√≠as combinadas
                    all_categories = list(set(knn_imputed_values.unique()) | set(iter_imputed_values.unique()))

                    # Contar ocurrencias
                    knn_counts = knn_imputed_values.value_counts().reindex(all_categories, fill_value=0)
                    iter_counts = iter_imputed_values.value_counts().reindex(all_categories, fill_value=0)

                    # Trazar un gr√°fico de barras en paralelo
                    fig, ax = plt.subplots(figsize=(10, 6))
                    x = np.arange(len(all_categories))
                    width = 0.35

                    ax.bar(x - width/2, knn_counts, width, label='KNN', color='#ff99ac')
                    ax.bar(x + width/2, iter_counts, width, label='Iterativo', color='#880d1e')

                    ax.set_xticks(x)
                    ax.set_xticklabels([str(c) for c in all_categories], ha='right')
                    ax.legend()

                    ax.set_title(f'Comparaci√≥n de valores imputados para "{col}"', fontsize=16, fontweight='bold', y=1.02)
                    ax.set_ylabel('Frecuencia', labelpad=12)

                    plt.tight_layout()
                    plt.show()
                    print("\n")

                # An√°lisis de acuerdos
                agreement_count = sum(knn_imputed_values == iter_imputed_values)
                total_imputed = len(knn_imputed_values)
                agreement_pct = agreement_count / total_imputed * 100

    def run_full_pipeline(
        self,
        df: pd.DataFrame,
        categorical_columns: Optional[List[str]] = None,
        target_columns: Optional[List[str]] = None,
        compare_methods: bool = True
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Ejecuta el proceso de imputaci√≥n completo con ambos m√©todos y comparaci√≥n.

        -----------
        Par√°metros:
        -----------
        df : Dataframe de entrada con valores faltantes.
        categorical_columns : columnas categ√≥ricas espec√≠ficas para imputar (si None, se utilizan todas las columnas categ√≥ricas).
        target_columns : columnas para usar como caracter√≠sticas adicionales y para la validaci√≥n cruzada.
        compare_methods : si se ejecutan y comparan m√©todos KNN e iterativos.

        --------
        Devuelve:
        --------
        Tuple[pd.DataFrame, pd.DataFrame]: Dataframes con KNN e imputaci√≥n iterativa.
        """

        # Establecer columnas de destino predeterminadas si no se proporcionan
        if target_columns is None:
            target_columns = list(df.select_dtypes(include=['number']).columns)

        # 1. Ejecutar imputaci√≥n KNN
        knn_imputer = CategoricalImputer(method='knn', verbose=True)
        knn_imputed = df.copy()
        knn_cols = knn_imputer.impute(
            knn_imputed,
            columns=categorical_columns,
            target_cols=target_columns,
            plot=False
        )
        for col in knn_cols.columns:
            knn_imputed[col] = knn_cols[col]

        # 2. Ejecute la imputaci√≥n iterativa si se solicita
        if compare_methods:
            iter_imputer = CategoricalImputer(method='iterative', verbose=True)
            iter_imputed = df.copy()
            iter_cols = iter_imputer.impute(
                iter_imputed,
                columns=categorical_columns,
                target_cols=target_columns,
                plot=False
            )
            for col in iter_cols.columns:
                iter_imputed[col] = iter_cols[col]

            # 3. Comparar resultados
            comparison_results = self.compare_imputation_results(
                df, knn_imputed, iter_imputed
            )
            self.print_imputation_comparison(comparison_results)

            # 4. Visualizar la comparaci√≥n
            self.visualize_comparison(df, knn_imputed, iter_imputed)

            return knn_imputed, iter_imputed
        else:
            print("\n Comparaci√≥n de m√©todos no solicitada, omitiendo...")
            return knn_imputed, None


imputer = CategoricalImputer()
df_knn_cat, df_iter_cat = imputer.run_full_pipeline(cleaned_kidney_df)

categoric_columns_KNN = df_knn_cat.select_dtypes(include=['object'])
categoric_columns_KNN.head(10)

categoric_columns_KNN.isna().sum()

categoric_columns_iter = df_iter_cat.select_dtypes(include=['object'])
categoric_columns_iter.head(10)

categoric_columns_iter.isna().sum()

"""Nuestro objetivo es elegir un buen m√©todo de imputaci√≥n para as√≠ evitar sesgar los an√°lisis posteriores. Es por eso, que en el siguiente c√≥digo hemos desarrollamos un an√°lisis comparativo entre las tres t√©cnicas de imputaci√≥n utilizadas para los valores categ√≥ricos.

Para evaluar cu√°l de estas t√©cnicas conserva mejor las propiedades estad√≠sticas de las variables originales, hemos empleado m√©tricas como:
*   Proporciones de la categor√≠a modal.
*   Diferencia absoluta en proporciones.
*   Valor p del test Chi-cuadrado.
*   Entrop√≠a y diferencia de entrop√≠a.

Todos estos resultados los hemos presentado de forma clara en una tabla comparativa estilizada, que facilita la interpretaci√≥n visual y permite identificar r√°pidamente el m√©todo que m√°s se asemeja a la distribuci√≥n original de los datos. Adem√°s, para facilitar la tarea hemos utilizado la siguiente l√≥gica visual:
*   Se resaltan en azul las menores diferencias en proporciones modales.
*   Se resaltan en amarillo los mayores p valores de chi-cuadrado, lo cual indica que las distribuciones original e imputada son similares
*   Se resaltan en verde las menores diferencias de entrop√≠a (mejor ajuste en diversidad).
*   Se resaltan en rojo los valores p significativos (< 0.05), indicando diferencias estad√≠sticamente significativas entre imputados y originales.
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import entropy
from IPython.display import display

def compare_imputation_with_stats(mode_imputed, knn_imputed, iter_imputed, original_df):
    """
    Comparar m√©todos de imputaci√≥n con m√©tricas estad√≠sticas y visualizaciones

    -----------
    Par√°metros:
    -----------
    mode_imputed: Dataframe con valores imputados por moda.
    knn_imputed: Dataframe con valores imputados por KNN.
    iter_imputed: Dataframe con valores imputados por iterativa.
    original_df: Dataframe original antes de la imputaci√≥n.

    --------
    Devuelve:
    --------
    dict: contiene la comparaci√≥n estad√≠stica y visualizaciones.
    """

    categorical_columns = original_df.select_dtypes(include=['object']).columns
    cat_cols_with_missing = [col for col in categorical_columns if original_df[col].isna().sum() > 0]

    if not cat_cols_with_missing:
        print("No se encontraron columnas con valores faltantes.")
        return {'stats_comparison': pd.DataFrame(), 'plots': {}}

    results = {
        'stats_comparison': pd.DataFrame(),
        'plots': {}
    }

    stats_data = []

    # Funci√≥n para calcular entrop√≠a de manera segura
    def safe_entropy(dist):
        """Calcular la entrop√≠a de forma segura, gestionando casos extremos."""

        # Asegurar de que la distribuci√≥n sume 1 y eliminar los ceros
        if not isinstance(dist, np.ndarray):
            dist = np.array(dist)
        # Filtrar ceros para evitar problemas de registro (0)
        dist = dist[dist > 0]
        # Normalizar si no est√° ya normalizado
        if np.sum(dist) != 1.0:
            dist = dist / np.sum(dist)
        # Calcular la entrop√≠a
        return -np.sum(dist * np.log2(dist))

    # Calcular estad√≠sticas para cada columna categ√≥rica
    for column in cat_cols_with_missing:
        original_complete = original_df[column].dropna()
        missing_mask = original_df[column].isna()

        # Extraer valores imputados (solo los que reemplazaron valores nulos)
        mode_values = mode_imputed.loc[missing_mask, column]
        knn_values = knn_imputed.loc[missing_mask, column]
        iter_values = iter_imputed.loc[missing_mask, column]

        # Calcular moda y su proporci√≥n en los datos originales completos
        if len(original_complete) > 0:
            original_value_counts = original_complete.value_counts(normalize=True)
            original_mode = original_complete.mode()[0]
            original_mode_prop = original_value_counts.get(original_mode, 0)
        else:
            original_mode = None
            original_mode_prop = 0

        # Calcular las proporciones de la categor√≠a modal original en los valores imputados
        # Para imputaci√≥n por moda
        if not mode_values.empty and original_mode is not None:
            # Contar cu√°ntos valores imputados son iguales a la moda original
            matches = (mode_values == original_mode).sum()
            # Calcular la proporci√≥n
            mode_mode_prop = matches / len(mode_values) if len(mode_values) > 0 else 0
        else:
            mode_mode_prop = 0

        # Para imputaci√≥n por KNN
        if not knn_values.empty and original_mode is not None:
            # Contar cu√°ntos valores imputados son iguales a la moda original
            matches = (knn_values == original_mode).sum()
            # Calcular la proporci√≥n
            knn_mode_prop = matches / len(knn_values) if len(knn_values) > 0 else 0
        else:
            knn_mode_prop = 0

        # Para imputaci√≥n iterativa
        if not iter_values.empty and original_mode is not None:
            # Contar cu√°ntos valores imputados son iguales a la moda original
            matches = (iter_values == original_mode).sum()
            # Calcular la proporci√≥n
            iter_mode_prop = matches / len(iter_values) if len(iter_values) > 0 else 0
        else:
            iter_mode_prop = 0

        # Calculamos la diferencia absoluta entre las proporciones de la moda
        # entre los datos originales y los imputados
        mode_diff = abs(mode_mode_prop - original_mode_prop)
        knn_diff = abs(knn_mode_prop - original_mode_prop)
        iter_diff = abs(iter_mode_prop - original_mode_prop)

        # Obtener distribuciones completas para test chi-cuadrado y c√°lculo de entrop√≠a
        # Primero obtenemos todos los valores √∫nicos en todas las versiones del dataset
        all_categories = sorted(list(set().union(
            set(original_df[column].dropna().unique()),
            set(mode_imputed[column].unique()),
            set(knn_imputed[column].unique()),
            set(iter_imputed[column].unique())
        )))

        # Calculamos las distribuciones de frecuencias normalizadas para los datos originales sin NA
        if len(original_complete) > 0:
            original_counts = original_complete.value_counts(normalize=True)
            original_counts_reindexed = original_counts.reindex(all_categories, fill_value=0)
        else:
            original_counts_reindexed = pd.Series(0, index=all_categories)

        # Distribuciones completas de todos los datasets (incluyendo valores imputados)
        mode_full_counts = mode_imputed[column].value_counts(normalize=True).reindex(all_categories, fill_value=0)
        knn_full_counts = knn_imputed[column].value_counts(normalize=True).reindex(all_categories, fill_value=0)
        iter_full_counts = iter_imputed[column].value_counts(normalize=True).reindex(all_categories, fill_value=0)

        # Distribuciones solo de los valores imputados (para el test chi-cuadrado)
        if not mode_values.empty:
            mode_counts = mode_values.value_counts(normalize=True).reindex(all_categories, fill_value=0)
        else:
            mode_counts = pd.Series(0, index=all_categories)

        if not knn_values.empty:
            knn_counts = knn_values.value_counts(normalize=True).reindex(all_categories, fill_value=0)
        else:
            knn_counts = pd.Series(0, index=all_categories)

        if not iter_values.empty:
            iter_counts = iter_values.value_counts(normalize=True).reindex(all_categories, fill_value=0)
        else:
            iter_counts = pd.Series(0, index=all_categories)

        # Calcular chi-cuadrado solo si hay suficientes datos (comparando distribuciones de valores imputados)
        chi_pval_mode = chi_pval_knn = chi_pval_iter = np.nan

        if len(mode_values) > 0 and len(original_complete) > 0:
            try:
                # Verificar que las distribuciones no est√©n completamente vac√≠as
                if sum(mode_counts.values) > 0 and sum(original_counts_reindexed.values) > 0:
                    _, chi_pval_mode = stats.chisquare(
                        f_obs=mode_counts.values,
                        f_exp=original_counts_reindexed.values
                    )

                if sum(knn_counts.values) > 0 and sum(original_counts_reindexed.values) > 0:
                    _, chi_pval_knn = stats.chisquare(
                        f_obs=knn_counts.values,
                        f_exp=original_counts_reindexed.values
                    )

                if sum(iter_counts.values) > 0 and sum(original_counts_reindexed.values) > 0:
                    _, chi_pval_iter = stats.chisquare(
                        f_obs=iter_counts.values,
                        f_exp=original_counts_reindexed.values
                    )
            except Exception as e:
                print(f"Error al calcular chi-cuadrado para {column}: {e}")

        # Calcular entrop√≠a usando la funci√≥n segura para el dataset completo en cada caso
        original_entropy = np.nan
        mode_entropy = np.nan
        knn_entropy = np.nan
        iter_entropy = np.nan

        # Entrop√≠a de los datos originales (sin NaN)
        if len(original_complete) > 0 and sum(original_counts_reindexed.values) > 0:
            original_entropy = safe_entropy(original_counts_reindexed.values)

        # Entrop√≠a de los datasets completos con imputaci√≥n
        if sum(mode_full_counts.values) > 0:
            mode_entropy = safe_entropy(mode_full_counts.values)

        if sum(knn_full_counts.values) > 0:
            knn_entropy = safe_entropy(knn_full_counts.values)

        if sum(iter_full_counts.values) > 0:
            iter_entropy = safe_entropy(iter_full_counts.values)

        # Calcular diferencia de entrop√≠a respecto a los datos originales
        mode_entropy_diff = abs(mode_entropy - original_entropy) if not np.isnan(mode_entropy) and not np.isnan(original_entropy) else np.nan
        knn_entropy_diff = abs(knn_entropy - original_entropy) if not np.isnan(knn_entropy) and not np.isnan(original_entropy) else np.nan
        iter_entropy_diff = abs(iter_entropy - original_entropy) if not np.isnan(iter_entropy) and not np.isnan(original_entropy) else np.nan

        # Guardar resultados para esta columna
        stats_data.append({
            'Variable': column,
            'N_imputed': missing_mask.sum(),
            'Original_mode': original_mode,
            'Original_mode_prop': round(original_mode_prop, 4),
            'Mode_mode_prop': round(mode_mode_prop, 4),
            'KNN_mode_prop': round(knn_mode_prop, 4),
            'Iter_mode_prop': round(iter_mode_prop, 4),
            'Mode_diff': round(mode_diff, 4),
            'KNN_diff': round(knn_diff, 4),
            'Iter_diff': round(iter_diff, 4),
            'Chi_pvalue_Mode': chi_pval_mode,
            'Chi_pvalue_KNN': chi_pval_knn,
            'Chi_pvalue_Iter': chi_pval_iter,
            'Original_entropy': round(original_entropy, 4) if not np.isnan(original_entropy) else np.nan,
            'Mode_entropy': round(mode_entropy, 4) if not np.isnan(mode_entropy) else np.nan,
            'KNN_entropy': round(knn_entropy, 4) if not np.isnan(knn_entropy) else np.nan,
            'Iter_entropy': round(iter_entropy, 4) if not np.isnan(iter_entropy) else np.nan,
            'Mode_entropy_diff': round(mode_entropy_diff, 4) if not np.isnan(mode_entropy_diff) else np.nan,
            'KNN_entropy_diff': round(knn_entropy_diff, 4) if not np.isnan(knn_entropy_diff) else np.nan,
            'Iter_entropy_diff': round(iter_entropy_diff, 4) if not np.isnan(iter_entropy_diff) else np.nan
        })

    # Crear DataFrame con resultados estad√≠sticos
    stats_df = pd.DataFrame(stats_data)
    stats_df.set_index('Variable', inplace=True)

    # Funci√≥n para aplicar estilos a la tabla
    def apply_styles(stats_df):
        # Definir los grupos de columnas
        column_groups = {
            'intro': ['N_imputed'],
            'moda': ['Original_mode'],
            'mode_prop': ['Original_mode_prop', 'Mode_mode_prop', 'KNN_mode_prop', 'Iter_mode_prop'],
            'diff': ['Mode_diff', 'KNN_diff', 'Iter_diff'],
            'chi_pvalue': ['Chi_pvalue_Mode', 'Chi_pvalue_KNN', 'Chi_pvalue_Iter'],
            'entropy': ['Original_entropy', 'Mode_entropy', 'KNN_entropy', 'Iter_entropy'],
            'entropy_diff': ['Mode_entropy_diff', 'KNN_entropy_diff', 'Iter_entropy_diff']
        }

        # 1. Resaltar las celdas con valores de diferencia m√°s bajos (mejor ajuste)
        def highlight_min_diff(row):
            diff_cols = ['Mode_diff', 'KNN_diff', 'Iter_diff']
            entropy_diff_cols = ['Mode_entropy_diff', 'KNN_entropy_diff', 'Iter_entropy_diff']

            styles = [''] * len(row)

            # Para las diferencias de proporciones de moda
            valid_diff_values = row[diff_cols].dropna()
            if len(valid_diff_values) > 0:
                min_diff_val = valid_diff_values.min()
                for i, (col, val) in enumerate(row.items()):
                    if col in diff_cols and val == min_diff_val:
                        styles[i] = 'background-color: #468faf'

            # Para las diferencias de entrop√≠a
            valid_entropy_diff_values = row[entropy_diff_cols].dropna()
            if len(valid_entropy_diff_values) > 0:
                min_entropy_diff_val = valid_entropy_diff_values.min()
                for i, (col, val) in enumerate(row.items()):
                    if col in entropy_diff_cols and val == min_entropy_diff_val:
                        styles[i] = 'background-color: #90a955'

            return styles

        # 2. Resaltar valores p bajos (< 0.05) para las pruebas estad√≠sticas
        def highlight_significant_pvalues(row):
            styles = [''] * len(row)
            pvalue_cols = ['Chi_pvalue_Mode', 'Chi_pvalue_KNN', 'Chi_pvalue_Iter']

            for i, (col, val) in enumerate(row.items()):
                if col in pvalue_cols and not pd.isna(val) and val < 0.05:
                    styles[i] = 'background-color: #bc4749'

            return styles

        # 3. Resaltar las celdas con valor m√°s alto para las pruebas estad√≠sticas
        def highlight_max_pvalues(row):
            styles = [''] * len(row)
            pvalue_cols = ['Chi_pvalue_Mode', 'Chi_pvalue_KNN', 'Chi_pvalue_Iter']

            valid_pvalue_values = row[pvalue_cols].dropna()
            if len(valid_pvalue_values) > 0:
                max_pvalue = valid_pvalue_values.max()
                for i, (col, val) in enumerate(row.items()):
                    if col in pvalue_cols and val == max_pvalue:
                        styles[i] = 'background-color: #f9a620'

            return styles

        # Crear estilos de borde para separar grupos de columnas
        border_styles = []

        # Determinar las primeras columnas de cada grupo para aplicar bordes
        for group_name, group_cols in column_groups.items():
            matched_cols = [col for col in group_cols if col in stats_df.columns]
            if matched_cols:
                first_col = matched_cols[0]
                col_idx = list(stats_df.columns).index(first_col)
                # +2 porque nth-child es 1-indexed y necesitamos considerar el √≠ndice
                border_styles.append({
                    'selector': f'td:nth-child({col_idx + 2})',
                    'props': [('border-left', '2px solid #000')]
                })

        # Aplicar todos los estilos
        styled = stats_df.style

        # Aplicar formato a valores num√©ricos
        format_dict = {
            'Original_mode_prop': '{:.2%}',
            'Mode_mode_prop': '{:.2%}',
            'KNN_mode_prop': '{:.2%}',
            'Iter_mode_prop': '{:.2%}',
            'Mode_diff': '{:.2%}',
            'KNN_diff': '{:.2%}',
            'Iter_diff': '{:.2%}',
            'Chi_pvalue_Mode': '{:.4f}',
            'Chi_pvalue_KNN': '{:.4f}',
            'Chi_pvalue_Iter': '{:.4f}',
            'Original_entropy': '{:.4f}',
            'Mode_entropy': '{:.4f}',
            'KNN_entropy': '{:.4f}',
            'Iter_entropy': '{:.4f}',
            'Mode_entropy_diff': '{:.4f}',
            'KNN_entropy_diff': '{:.4f}',
            'Iter_entropy_diff': '{:.4f}'
        }
        styled = styled.format(format_dict, na_rep="N/A")

        # Aplicar resaltado
        styled = styled.apply(highlight_min_diff, axis=1)
        styled = styled.apply(highlight_significant_pvalues, axis=1)
        styled = styled.apply(highlight_max_pvalues, axis=1)

        # Aplicar bordes de separaci√≥n
        styled = styled.set_table_styles(border_styles)

        return styled

    styled_df = apply_styles(stats_df)

    results['stats_comparison'] = stats_df
    results['styled_comparison'] = styled_df

    print("\nüîπ Comparaci√≥n estad√≠stica de m√©todos de imputaci√≥n:\n")
    display(styled_df)

    return results


def run_comparison_analysis(original_df, mode_imputed, knn_imputed, iter_imputed):
    """
    Ejecuta el an√°lisis comparativo entre varios m√©todos de imputaci√≥n

    -----------
    Par√°metros:
    -----------
    original_df : DataFrame original con valores faltantes.
    mode_imputed : DataFrame con valores imputados por moda.
    knn_imputed : DataFrame con valores imputados por KNN.
    iter_imputed : DataFrame con valores imputados por iterativa.

    --------
    Devuelve:
    --------
    dict: resultados del an√°lisis comparativo
    """

    # Ejecutar la comparaci√≥n
    comparison_results = compare_imputation_with_stats(
        mode_imputed=mode_imputed,
        knn_imputed=knn_imputed,
        iter_imputed=iter_imputed,
        original_df=original_df
    )

    return comparison_results

comparison_results = run_comparison_analysis(
     original_df=cleaned_kidney_df,
     mode_imputed=categoric_columns_mode,
     knn_imputed=categoric_columns_KNN,
     iter_imputed=categoric_columns_iter
)

"""Para la imputaci√≥n de variables categ√≥ricas se evaluaron tres enfoques: imputaci√≥n por moda (`Mode`), imputaci√≥n mediante vecinos m√°s cercanos (K-Nearest Neighbors, `KNN`) e imputaci√≥n iterativa (`IterativeImputer`).

Como hemos mencionado anteriormente, la evaluaci√≥n comparativa se bas√≥ en varias m√©tricas estad√≠sticas, entre ellas:
*   La proporci√≥n de la categor√≠a m√°s frecuente antes y despu√©s de la imputaci√≥n.
*   La entrop√≠a, como medida de diversidad de la variable.
*   El valor p de la prueba de chi-cuadrado, que compara la distribuci√≥n original de categor√≠as con la distribuci√≥n obtenida tras la imputaci√≥n.

En general, el m√©todo `IterativeImputer` ofreci√≥ un mejor equilibrio entre conservar la distribuci√≥n original de las categor√≠as y preservar la diversidad interna de cada variable. A diferencia de la imputaci√≥n por moda, que tiende a sobrerrepresentar la categor√≠a dominante y reduce significativamente la variabilidad, o de KNN, que en algunas variables, alter√≥ notablemente la distribuci√≥n original. Adem√°s, la imputaci√≥n iterativa mostr√≥ un desempe√±o m√°s consistente, con diferencias m√≠nimas tanto en la proporci√≥n de la moda como en la entrop√≠a, y p valores m√°s altos en chi-cuadrado.

Por ejemplo, en la variable "rbc", el m√©todo iterativo mantuvo la proporci√≥n original de la categor√≠a m√°s frecuente con una diferencia m√≠nima de apenas 1.85%, mientras que KNN redujo dr√°sticamente dicha frecuencia de 81.05% a 27.63%. Asimismo, la entrop√≠a resultante con `Iterative` fue casi id√©ntica a la original (0.6855 vs. 0.7005), lo que sugiere que no se perdi√≥ diversidad categ√≥rica durante la imputaci√≥n.

Debido a estos resultados, se decidi√≥ utilizar imputaci√≥n iterativa como m√©todo general para las variables categ√≥ricas, ya que permite una mayor fidelidad estad√≠stica a las distribuciones originales y minimiza el sesgo hacia cualquier categor√≠a espec√≠fica.

### 3.3. Resultado de la limpieza

Una vez analizados los diferentes m√©todos de imputaci√≥n aplicados a las variables categ√≥ricas y num√©ricas, y tras comparar sus resultados mediante diversas m√©tricas estad√≠sticas, integramos los valores imputados en una nueva versi√≥n del conjunto de datos, que ser√° utilizada para los an√°lisis posteriores.

Tal como se mencion√≥ anteriormente, optamos por utilizar la imputaci√≥n iterativa en ambos tipos de variables. Para las variables num√©ricas, se emple√≥ el m√©todo MICE y para las variables categ√≥ricas, se utiliz√≥ tambi√©n una imputaci√≥n iterativa (`IterativeImputer`).
"""

imputed_kidney_df = cleaned_kidney_df.copy()

for col in df_iter_cat.columns:
  if col in imputed_kidney_df.columns:
    imputed_kidney_df[col] = df_iter_cat[col]


for col in numeric_columns_MICE.columns:
    if col in imputed_kidney_df.columns:
        imputed_kidney_df[col] =numeric_columns_MICE[col]

"""A continuaci√≥n, verificamos visualmente que todas las columnas del conjunto de datos final se encuentren correctamente imputadas. Para ello, generamos un mapa de calor que muestra los valores faltantes antes y despu√©s del proceso de imputaci√≥n. En este gr√°fico, los valores presentes se muestran en azul, mientras que los valores ausentes aparecen en amarillo."""

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Patch

sns.set(style="whitegrid")

paleta_colores = ['#0077b6', '#ffc43d']

fig, axes = plt.subplots(1, 2, figsize=(18, 8))
fig.suptitle('Mapa de calor de valores faltantes', fontsize=20, fontweight='bold', y=1)
legend_labels = [Patch(color='#0077b6', label='Valores presentes'), Patch(color='#ffc43d', label='Valores faltantes')]

axes[0] = sns.heatmap(cleaned_kidney_df.isnull(), cmap=paleta_colores, cbar=False, xticklabels=True, yticklabels=False, ax=axes[0])
axes[0].set_title('Dataset original', fontsize=15, pad=15, fontweight='bold')
axes[0].set_xlabel('Columnas', fontsize=14)
axes[0].set_ylabel('Filas', fontsize=14, labelpad=15)
plt.setp(axes[0].get_xticklabels(), fontsize=12, rotation=45)

axes[1] = sns.heatmap(imputed_kidney_df.isnull(), cmap=paleta_colores, cbar=False, xticklabels=True, yticklabels=False, ax=axes[1])
axes[1].set_title('Dataset imputado', fontsize=15, pad=15, fontweight='bold')
axes[1].set_xlabel('Columnas', fontsize=14)
axes[1].set_ylabel('Filas', fontsize=14, labelpad=15)
plt.setp(axes[1].get_xticklabels(), fontsize=12, rotation=45)

fig.legend(handles=legend_labels, bbox_to_anchor=(0.98, 1), loc='upper right', fontsize=12)

plt.tight_layout()
plt.subplots_adjust(top=0.88)
plt.show()

"""### 3.4. An√°lisis de la variable objetivo en el dataset imputado

Antes de aplicar cualquier modelo de regresi√≥n, es fundamental analizar la variable objetivo en el dataset imputado, para comprender su distribuci√≥n, posibles sesgos introducidos por la imputaci√≥n y su relevancia cl√≠nica. En nuestro caso, la variable objetivo asignada es "bu", que corresponde a los niveles de urea en sangre.

La urea es un producto de desecho generado a partir del metabolismo de las prote√≠nas, y se elimina normalmente por los ri√±ones. Por ello, niveles elevados de urea en sangre son un indicador clave de disfunci√≥n renal, particularmente en enfermedades renales cr√≥nicas.
*   En personas sanas, los valores normales de urea suelen situarse entre 1.8 y 7.1 mmol/L (~5 a 20 mg/dL). <a href="https://www.ncbi.nlm.nih.gov/books/NBK305/" target="_blank"> Referencia: NCBI - Clinical Methods: The History, Physical, and Laboratory Examinations</a></p>
*   Valores por encima de 50.0 mmol/L (~140 mg/dL) suelen observarse en pacientes con insuficiencia renal terminal, lo que indica un deterioro grave de la funci√≥n renal. <a href="https://acutecaretesting.org/en/articles/urea-and-the-clinical-value-of-measuring-blood-urea-concentration" target="_blank"> Referencia: acutecaretesting.org</a></p>

* En pacientes con enfermedad renal cr√≥nica, los niveles s√©ricos de urea aumentan progresivamente seg√∫n el deterioro de la funci√≥n renal, con valores normales entre 10 y 40 mg/dl, elevaciones leves por encima de 40 mg/dl en estadios iniciales, y cifras superiores a 100 mg/dl en estadios avanzados <a href="http://www.scielo.org.pe/scielo.php?script=sci_arttext&pid=S1018-130X2021000400216" target="_blank"> Referencia: Ram√≠rez-Soto MC, et al. Relaci√≥n entre el √≠ndice ur√©mico y la funci√≥n renal en pacientes con enfermedad renal cr√≥nica.</a></p>

Por tanto, "bu" no solo es un excelente biomarcador de severidad cl√≠nica, sino que su predicci√≥n precisa podr√≠a ser clave para la detecci√≥n temprana, monitorizaci√≥n de progresi√≥n y estratificaci√≥n de riesgo en pacientes con compromiso renal

Nuestro an√°lisis comenzar√° estudiando las estad√≠sticas b√°sicas de esta variable en el dataset imputado. Para ello utilizaremos el siguiente c√≥digo, que nos permitir√°:
*   Identificar valores extremos o outliers.
*   Evaluar la media y la mediana como posibles indicadores del sesgo.
*   Verificar la amplitud y dispersi√≥n de los valores (m√°ximo, m√≠nimo, desviaci√≥n est√°ndar).
*   Observar si tras la imputaci√≥n persisten patrones consistentes.
"""

print('üîπ Estad√≠sticas descriptivas de "bu":')
print(imputed_kidney_df['bu'].describe())

"""A continuaci√≥n, se lleva a cabo un an√°lisis univariable exhaustivo, similar al desarrollado previamente en el apartado 2.5.4, con los siguientes objetivos espec√≠ficos:
*   Cuantificar las principales medidas de tendencia central y dispersi√≥n: media, mediana, moda, varianza, desviaci√≥n est√°ndar, rango e IQR.
*   Evaluar la forma de la distribuci√≥n a trav√©s de estad√≠sticas de asimetr√≠a (skewness) y curtosis (kurtosis).
*   Identificar posibles valores at√≠picos mediante el m√©todo del rango intercuart√≠lico (IQR).
*   Visualizar gr√°ficamente la distribuci√≥n con histogramas, gr√°ficos de densidad y boxplots para facilitar la interpretaci√≥n visual de patrones.
"""

import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

media_new = imputed_kidney_df["bu"].mean()
mediana_new = imputed_kidney_df["bu"].median()
moda_new = imputed_kidney_df["bu"].mode()[0]
desviacion_tipica_new = imputed_kidney_df["bu"].std()
var_new = imputed_kidney_df["bu"].var()
min_val_new = imputed_kidney_df["bu"].min()
max_val_new = imputed_kidney_df["bu"].max()
rango_new = max_val_new - min_val_new
iqr_new = imputed_kidney_df["bu"].quantile(0.75) - imputed_kidney_df["bu"].quantile(0.25)
varianza_new = imputed_kidney_df["bu"].var()
asimetria_new = stats.skew(imputed_kidney_df["bu"].dropna())
curtosis_new = stats.kurtosis(imputed_kidney_df["bu"].dropna())
q1_new = imputed_kidney_df["bu"].quantile(0.25)
q3_new = imputed_kidney_df["bu"].quantile(0.75)

# Mostrar las medidas
print(f'''
üìä An√°lisis estad√≠stico para "bu" en el dataset imputado:\n
üîπ Medidas de tendencia central:
   ‚Ä¢ Media: {media_new:.3f}
   ‚Ä¢ Mediana: {mediana_new:.3f}
   ‚Ä¢ Moda: {moda_new:.3f}
üîπ Medidas de dispersi√≥n:
   ‚Ä¢ Desviaci√≥n est√°ndar: {desviacion_tipica_new:.3f}
   ‚Ä¢ Varianza: {var_new:.3f}
   ‚Ä¢ Rango: {rango_new:.3f}
   ‚Ä¢ Rango intercuart√≠lico (IQR): {iqr_new:.3f}
üîπ Valores extremos:
   ‚Ä¢ M√≠nimo: {min_val_new:.3f}
   ‚Ä¢ M√°ximo: {max_val_new:.3f}
   ‚Ä¢ Primer cuartil (Q1): {q1_new:.3f}
   ‚Ä¢ Tercer cuartil (Q3): {q3_new:.3f}
üîπ Forma de la distribuci√≥n:
   ‚Ä¢ Asimetr√≠a: {asimetria_new:.3f} ({'positiva' if asimetria_new > 0 else 'negativa' if asimetria_new < 0 else 'sim√©trica'})
   ‚Ä¢ Curtosis: {curtosis_new:.3f} ({'leptoc√∫rtica' if curtosis_new > 0 else 'platic√∫rtica' if curtosis_new < 0 else 'mesoc√∫rtica'})
''')

lower_bound_new = q1_new - 1.5 * iqr_new
upper_bound_new = q3_new + 1.5 * iqr_new
outliers_new = imputed_kidney_df[(imputed_kidney_df["bu"] < lower_bound_new) | (imputed_kidney_df["bu"] > upper_bound_new)]["bu"]

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.tight_layout(pad=6.0)
plt.subplots_adjust(hspace=0.3)

# Histograma de la variable 'bu'
sns.histplot(imputed_kidney_df['bu'], kde=True, color='#457b9d', bins=30, edgecolor='black', ax=axes[0, 0])
axes[0, 0].axvline(media_new, color='#fb8b24', linestyle='--', linewidth=1.5, label=f"Media: {media_new:.2f}")
axes[0, 0].axvline(mediana_new, color='#9d0208', linestyle='--', linewidth=1.5, label=f"Mediana: {mediana_new:.2f}")
axes[0, 0].legend(loc='upper right')
axes[0, 0].set_title('Distribuci√≥n de la variable (regresi√≥n)', fontsize=14, pad=11, fontweight='bold')
axes[0, 0].set_xlabel('Valor de "bu"', fontsize=12, labelpad=11)
axes[0, 0].set_ylabel('Frecuencia', fontsize=12, labelpad=11)
axes[0, 0].grid(True, linestyle='--', alpha=0.6)

# Gr√°fico de Densidad (KDE)
sns.kdeplot(imputed_kidney_df['bu'], fill=True, color='#457b9d', linewidth=2, ax=axes[0, 1])
axes[0, 1].set_title('Distribuci√≥n de densidad (regresi√≥n)', fontsize=14, pad=11, fontweight='bold')
axes[0, 1].set_xlabel('Valor de "bu"', fontsize=12, labelpad=11)
axes[0, 1].set_ylabel('Densidad', fontsize=12, labelpad=11)
axes[0, 1].grid(True, linestyle='--', alpha=0.6)

# Boxplot para identificar valores at√≠picos
sns.boxplot(x=imputed_kidney_df['bu'], color='#457b9d', fliersize=8, linewidth=1.5, ax=axes[1, 0])
axes[1, 0].set_title('Boxplot', fontsize=14, pad=11, fontweight='bold')
axes[1, 0].set_xlabel('Valor de "bu"', fontsize=12, labelpad=11)
axes[1, 0].grid(True, linestyle='--', alpha=0.6)

# Histograma con valores descriptivos
axes[1, 1].text(0.5, 0.5, f"""
Estad√≠sticas de "bu":\n
Media: {media_new:.3f}
Mediana: {mediana_new:.3f}
Moda: {moda_new:.3f}
Desviaci√≥n t√≠pica: {desviacion_tipica_new:.3f}
Rango: [{min_val_new:.1f}, {max_val_new:.1f}]
IQR: {iqr_new:.3f}
Varianza: {varianza_new:.3f}
Outliers: {len(outliers_new)} ({(len(outliers_new) / len(cleaned_kidney_df)) * 100:.1f}%)
""", horizontalalignment='center', verticalalignment='center', fontsize=12)
axes[1, 1].axis('off')  # Apagar los ejes ya que es solo texto

plt.suptitle(f'An√°lisis completo de la variable objetivo "bu" en el dataset imputado', fontsize=16, fontweight='bold', y=1)
plt.show()

"""Al examinar los resultados del an√°lisis estad√≠stico de la variable "bu" en el dataset imputado, se observa que el rango total de los valores (409.197) es considerablemente amplio en comparaci√≥n con el rango intercuart√≠lico (IQR), que asciende a solo 39.250. Esta diferencia sugiere la presencia de valores extremos o at√≠picos (ouliers) que se encuentran muy alejados del n√∫cleo central de la distribuci√≥n y que, por tanto, pueden estar influyendo de manera significativa en las medidas de tendencia central y dispersi√≥n, como la media y la varianza.

En cuanto a la asimetr√≠a (skewness), se ha obtenido un valor de 2.556, lo que indica una asimetr√≠a positiva pronunciada. Este tipo de distribuci√≥n est√° sesgada hacia la derecha, lo que implica que existe una cola alargada en el extremo superior de los valores de "bu". En t√©rminos pr√°cticos, esto significa que, aunque la mayor√≠a de los pacientes presentan valores moderados de urea, existe un subconjunto con concentraciones anormalmente elevadas, probablemente reflejo de condiciones cl√≠nicas graves como insuficiencia renal avanzada.

Asimismo, el valor de curtosis es de 8.936, lo que clasifica la distribuci√≥n como leptoc√∫rtica. Esta caracter√≠stica describe una distribuci√≥n m√°s picuda y con colas m√°s largas que una distribuci√≥n normal. Es decir, hay una mayor concentraci√≥n de valores cerca de la mediana, pero tambi√©n una mayor probabilidad de observar valores extremos en ambos extremos de la distribuci√≥n.

En conjunto, estos indicadores estad√≠sticos describen una distribuci√≥n con fuerte sesgo a la derecha y con propensi√≥n a valores extremos, lo cual es coherente con la naturaleza cl√≠nica de la variable "bu", ya que en poblaciones cl√≠nicas, es esperable encontrar casos con niveles elevados de urea debido a patolog√≠as renales severas.

### 3.5.  An√°lisis multivariable de valores at√≠picos en variables num√©ricas

Tras haber identificado la presencia de posibles valores at√≠picos en la variable objetivo "bu" y su considerable impacto en las estad√≠sticas descriptivas, hemos considerado esencial extender este an√°lisis al conjunto completo de variables num√©ricas del dataset imputado. Este enfoque nos permitir√° comprender la distribuci√≥n general de los datos, detectar variables potencialmente afectadas por outliers y, en consecuencia, facilitar decisiones m√°s fundamentadas en la etapa de modelado.

En el c√≥digo, se ha llevado a cabo un an√°lisis exploratorio univariable de cada columna num√©rica, haciendo especial √©nfasis en la detecci√≥n y cuantificaci√≥n de valores at√≠picos, distinguiendo entre:
*   Outliers leves: aquellos que se encuentran fuera del rango intercuart√≠lico (IQR) ampliado en 1.5 veces.
*   Outliers extremos: aquellos que exceden 3 veces el IQR respecto a los cuartiles primero (Q1) y tercero (Q3).

Adem√°s, tambi√©n hemos calculado para cada variable los siguientes indicadores:
*   Los l√≠mites inferior y superior para la detecci√≥n de outliers leves y extremos.
*   El n√∫mero y porcentaje de observaciones at√≠picas (leves y extremas).

Finalmente, hemos generado visualizaciones detalladas utilizando boxplots personalizados, sobre los cuales se han superpuesto los puntos individuales de cada observaci√≥n, codificados por color seg√∫n su categor√≠a:
*   Normal: azul.
*   Outlier leve: naranja.
*   Outlier extremo: rojo.

Esta representaci√≥n gr√°fica facilita una interpretaci√≥n inmediata del grado de afectaci√≥n de cada variable.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate
import matplotlib.gridspec as gridspec
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.patches as mpatches

# Configurar el estilo general para todas las visualizaciones
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("talk")
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.titleweight'] = 'bold'
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10

# Definir una paleta de colores personalizada para los gr√°ficos
colors = {
    'normal': '#3498db',          # Azul para datos normales
    'outlier_mild': '#f39c12',    # Naranja para outliers leves
    'outlier_extreme': '#e74c3c', # Rojo para outliers extremos
    'background': '#f8f9fa',      # Fondo claro
    'grid': '#dee2e6',            # L√≠neas de cuadr√≠cula suaves
    'text': '#2c3e50',            # Texto oscuro
    'highlight': '#2ecc71'        # Verde para destacados
}

# Seleccionar columnas num√©ricas
numeric_columns = imputed_kidney_df.select_dtypes(include=['number']).columns
outlier_summary = []

# Verificar si hay valores NA y asegurarnos de que los datos est√©n limpios
imputed_kidney_df_clean = imputed_kidney_df.copy()

# Calcular estad√≠sticas de outliers sin usar m√°scaras
for column in numeric_columns:
    # Convertir a array NumPy est√°ndar para evitar problemas con m√°scaras
    data = imputed_kidney_df_clean[column].to_numpy()

    # Usar numpy directamente para calcular cuartiles
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1

    # L√≠mites para outliers leves (1.5*IQR)
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # L√≠mites para outliers extremos (3*IQR)
    lower_extreme = Q1 - 3 * IQR
    upper_extreme = Q3 + 3 * IQR

    # Conteos de outliers
    outliers_mild = imputed_kidney_df_clean[(imputed_kidney_df_clean[column] < lower_bound) &
                                    (imputed_kidney_df_clean[column] >= lower_extreme) |
                                    (imputed_kidney_df_clean[column] > upper_bound) &
                                    (imputed_kidney_df_clean[column] <= upper_extreme)]

    outliers_extreme = imputed_kidney_df_clean[(imputed_kidney_df_clean[column] < lower_extreme) |
                                        (imputed_kidney_df_clean[column] > upper_extreme)]

    total_outliers = len(outliers_mild) + len(outliers_extreme)

    # Calcular porcentajes
    outlier_percent = (total_outliers / len(imputed_kidney_df_clean)) * 100
    outlier_mild_percent = (len(outliers_mild) / len(imputed_kidney_df_clean)) * 100
    outlier_extreme_percent = (len(outliers_extreme) / len(imputed_kidney_df_clean)) * 100

    # Guardamos todos los valores para el resumen
    outlier_summary.append({
        "Variable": column,
        "Q1": Q1,
        "Q3": Q3,
        "IQR": IQR,
        "L√≠mite inferior (outliers leves)": lower_bound,
        "L√≠mite superior (outliers leves)": upper_bound,
        "Outliers leves": len(outliers_mild),
        "Outliers extremos": len(outliers_extreme),
        "Total outliers": total_outliers,
        "% Outliers leves": round(outlier_mild_percent, 2),
        "% Outliers extremos": round(outlier_extreme_percent, 2),
        "% Total outliers": round(outlier_percent, 2)
    })

# Crear DataFrame y ordenar por porcentaje total de outliers
outlier_df = pd.DataFrame(outlier_summary)

# Visualizaci√≥n por variable: crear boxplots individuales con puntos de datos coloreados seg√∫n si son outliers
num_plots = len(numeric_columns)
num_cols = 3
num_rows = (num_plots + num_cols - 1) // num_cols  # Redondeo hacia arriba

plt.figure(figsize=(18, num_rows * 5))
plt.subplots_adjust(hspace=0.8, wspace=0.5)

for i, column in enumerate(outlier_df['Variable']):
    plt.subplot(num_rows, num_cols, i+1)

    # Obtener datos y calcular l√≠mites
    data = imputed_kidney_df_clean[column].to_numpy()  # Convertir a numpy array regular
    Q1 = outlier_df.loc[outlier_df['Variable'] == column, 'Q1'].values[0]
    Q3 = outlier_df.loc[outlier_df['Variable'] == column, 'Q3'].values[0]
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    lower_extreme = Q1 - 3 * IQR
    upper_extreme = Q3 + 3 * IQR

    # Crear boxplot con seaborn (que maneja mejor los outliers)
    sns.boxplot(x=data, color=colors['normal'], width=0.3)

    # Superponer stripplot coloreando los puntos seg√∫n tipo de outlier
    normal_idx = np.where((data >= lower_bound) & (data <= upper_bound))[0]
    mild_idx = np.where(((data < lower_bound) & (data >= lower_extreme)) |
                         ((data > upper_bound) & (data <= upper_extreme)))[0]
    extreme_idx = np.where((data < lower_extreme) | (data > upper_extreme))[0]

    # Jitter para mejor visualizaci√≥n
    jitter = 0.02
    if len(normal_idx) > 0:
        plt.scatter(data[normal_idx], np.random.normal(0, jitter, size=len(normal_idx)),
                   color=colors['normal'], alpha=0.4, s=20)
    if len(mild_idx) > 0:
        plt.scatter(data[mild_idx], np.random.normal(0, jitter, size=len(mild_idx)),
                   color=colors['outlier_mild'], alpha=0.7, s=30)
    if len(extreme_idx) > 0:
        plt.scatter(data[extreme_idx], np.random.normal(0, jitter, size=len(extreme_idx)),
                   color=colors['outlier_extreme'], alpha=0.7, s=40)

    # A√±adir l√≠neas para los l√≠mites
    plt.axvline(x=lower_bound, color=colors['outlier_mild'], linestyle='--', alpha=0.7)
    plt.axvline(x=upper_bound, color=colors['outlier_mild'], linestyle='--', alpha=0.7)
    plt.axvline(x=lower_extreme, color=colors['outlier_extreme'], linestyle='--', alpha=0.7)
    plt.axvline(x=upper_extreme, color=colors['outlier_extreme'], linestyle='--', alpha=0.7)

    # A√±adir t√≠tulo y etiquetas
    plt.title(f'Distribuci√≥n y outliers de la variable "{column}"', pad=15, fontweight='bold')
    plt.xlabel(f'Valor de "{column}"', labelpad=15)
    plt.ylabel('')

    # A√±adir informaci√≥n sobre outliers
    outlier_info = outlier_df[outlier_df['Variable'] == column]
    total_pct = outlier_info['% Total outliers'].values[0]
    plt.annotate(f'Outliers: {total_pct:.1f}%', xy=(0.95, 0.95), xycoords='axes fraction',
                ha='right', va='top', bbox=dict(boxstyle="round,pad=0.3",
                fc=colors['background'], ec="gray", alpha=0.8))

# A√±adir leyenda general
normal_patch = mpatches.Patch(color=colors['normal'], label='Datos normales')
mild_patch = mpatches.Patch(color=colors['outlier_mild'], label='Outliers leves')
extreme_patch = mpatches.Patch(color=colors['outlier_extreme'], label='Outliers extremos')
plt.figlegend(handles=[normal_patch, mild_patch, extreme_patch],
              loc='center', ncol=3, bbox_to_anchor=(0.5, 0.05))

plt.tight_layout(pad=3.0, h_pad=2.0, w_pad=2.0, rect=[0, 0.05, 1, 0.95])
plt.show()

"""Con el fin de facilitar la interpretaci√≥n de los resultados obtenidos en el an√°lisis de outliers, se ha optado por presentar un resumen tabular con las m√©tricas m√°s relevantes para cada variable num√©rica. La tabla que que genera el c√≥digo incluye:
*   El n√∫mero total de outliers por variable.
*   El porcentaje que representan estos valores respecto al total de observaciones.
*   El porcentaje de outliers leves y extremos por separado.
*   Los l√≠mites superior e inferior considerados para identificar los valores at√≠picos.

Esta presentaci√≥n estructurada permite identificar de forma r√°pida las variables m√°s afectadas por valores at√≠picos, lo que es crucial para la toma de decisiones en etapas posteriores.

Adem√°s, hemos decidido proporcionar una interpretaci√≥n visual y conceptual que resume c√≥mo se clasifican los outliers, y qu√© criterios se deben considerar para identificar variables que requieran un tratamiento especial.
"""

from IPython.display import HTML, display

# Seleccionar solo las columnas m√°s relevantes para la tabla
table_columns = ['Variable', 'Total outliers', '% Total outliers', '% Outliers leves', '% Outliers extremos',
                'L√≠mite inferior (outliers leves)', 'L√≠mite superior (outliers leves)']
table_data = outlier_df[table_columns]

print("üîπ Resumen de valores at√≠picos (outliers) por variable:\n")

# Aplicar estilos a la tabla
styled_table5 = table_data.style.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})

styled_table5 = styled_table5.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

# Mostrar la tabla estilizada
display(styled_table5)

interpretation_html = """
<div style="background-color: #f8f9fa; border-left: 6px solid #457b9d; margin: 15px 0; padding: 15px;">
    <h3 style="color: #457b9d; margin-top: 0;">üìä Interpretaci√≥n de outliers</h3>
    <ul style="list-style-type: none; padding-left: 10px;">
        <li style="margin-bottom: 8px;">
            <span style="color: #f39c12; font-weight: bold;">‚ñ™Ô∏è Outliers leves:</span>
            <span style="color: #000000;">valores fuera del rango [Q1-1.5*IQR, Q3+1.5*IQR].</span>
        </li>
        <li style="margin-bottom: 8px;">
            <span style="color: #e74c3c; font-weight: bold;">‚ñ™Ô∏è Outliers extremos:</span>
            <span style="color: #000000;">valores fuera del rango [Q1-3*IQR, Q3+3*IQR].</span>
        </li>
        <li style="margin-bottom: 8px;">
            <span style="color: #1d3557; font-weight: bold;">‚ñ™Ô∏è Criterio de atenci√≥n:</span>
            <span style="color: #000000;">variables con m√°s del 10% de outliers pueden requerir tratamiento especial antes del modelado.</span>
        </li>
    </ul>
</div>
"""

display(HTML(interpretation_html))

"""#### 3.5.1. An√°lisis cl√≠nico de outliers

La detecci√≥n de valores at√≠picos (outliers) no solo debe abordarse desde un punto de vista estad√≠stico, sino tambi√©n desde una perspectiva cl√≠nica. Es fundamental evaluar si estos valores extremos son errores, condiciones fisiol√≥gicas raras, o reflejan casos cl√≠nicos v√°lidos pero poco frecuentes. A continuaci√≥n se analiza la relevancia cl√≠nica de los principales outliers detectados en el conjunto de datos sobre enfermedad renal cr√≥nica (ERC):

**Variables con la mayoria de outliers leves:**

1. Edad ("age"):
    *   Observaci√≥n cl√≠nica: aunque la ERC es m√°s com√∫n en adultos mayores, tambi√©n puede presentarse en ni√±os debido a enfermedades cong√©nitas, s√≠ndromes hereditarios o malformaciones del tracto urinario. Estos casos pedi√°tricos son cl√≠nicamente relevantes pero poco frecuentes. <a href="https://www.niddk.nih.gov/health-information/kidney-disease/children" target="_blank">Referencia: NIH - Pediatric Chronic Kidney Disease </a></p>
    *   Efecto sobre el modelo: la inclusi√≥n de pacientes pedi√°tricos puede introducir patrones diferentes en los datos, alterando las predicciones y reduciendo la generalizaci√≥n del modelo entrenado para adultos.
    *   Posible soluci√≥n: realizar un an√°lisis comparativo del modelo con y sin los datos de pacientes menores de edad, para determinar si deben excluirse o tratarse de forma diferenciada.

2. Presi√≥n arterial ("bp"):
    *   Observaci√≥n cl√≠nica: se identificaron valores at√≠picos por encima de 160 mmHg, correspondientes a hipertensi√≥n severa. <a href="https://www.kidney.org/kidney-topics/what-high-blood-pressure" target="_blank">Referencia: National Kidney Foundation ‚Äì Hypertension and Kidneys</a></p>
    *   Efecto sobre el modelo: estos casos pueden causar que el modelo sobreestime la hipertensi√≥n como predictor, perdiendo sensibilidad para otras variables relevantes.
    *   Posible soluci√≥n: aplicar Winsorization para limitar la influencia de casos extremos sin eliminar informaci√≥n cr√≠tica.

3. Glucosa en sangre aleatoria ("brg"):
    *   Observaci√≥n cl√≠nica: se han identificado valores superiores a 300‚Äì400 mg/dL, lo que sugiere casos severos de hiperglucemia o diabetes no controlada. <a href="https://medlineplus.gov/lab-tests/blood-glucose-test/" target="_blank">Referencia: MedlinePlus - Blood Glucose Test</a></p>
    *   Efecto sobre el modelo: estos valores pueden sesgar el modelo hacia pacientes con diabetes tipo 2 avanzada, una comorbilidad com√∫n en la ERC, pero no necesariamente representativa de todos los casos.
    *   Posible soluci√≥n: aplicar una transformaci√≥n logar√≠tmica puede reducir el impacto de estos valores extremos sin eliminarlos, preservando as√≠ su valor cl√≠nico.

4. Niveles de sodio ("sod"):
    *   Observaci√≥n cl√≠nica: se han identificado valores superiores a 150 mEq/L, lo que sugiere hipernatremia, potencialmente vinculada a deshidrataci√≥n, diur√©ticos o disfunci√≥n renal. <a href="https://www.akdh.org/article/S1548-5595(15)00165-2/fulltext" target="_blank">Referencia: Advances in Chronic Kidney Disease - Sodium and Volume Disorders in Advanced Chronic Kidney Disease</a></p>
    *   Efecto sobre el modelo: estos valores pueden hacer que el modelo se vuelva excesivamente sensible a alteraciones electrol√≠ticas no frecuentes.
    *   Posible soluci√≥n: aplicar t√©cnicas de Winsorization o analizar la distribuci√≥n por percentiles para limitar el impacto de valores extremos sin suprimir datos cl√≠nicamente v√°lidos.

5. Niveles de potasio ("pot"):
    *   Observaci√≥n cl√≠nica: se han registrado valores superiores a 10 mEq/L, lo cual se considera un estado de hiperpotasemia grave, com√∫n en etapas avanzadas de insuficiencia renal. <a href="https://www.kidneyfund.org/living-kidney-disease/health-problems-caused-kidney-disease/high-potassium-hyperkalemia-causes-prevention-and-treatment" target="_blank">Referencia: National Kidney Foundation ‚Äì High Potassium (Hyperkalemia)</a></p>
    *   Efecto sobre el modelo: estos valores extremos pueden introducir ruido si el modelo los interpreta como parte del rango normal, aunque en realidad indican alto riesgo de complicaciones cardiovasculares.
    *   Posible soluci√≥n: aplicar t√©cnicas de Winsorization para limitar el impacto de los valores extremos manteniendo su presencia en el conjunto de datos.

6. Niveles de hemoglobina ("hemo"):
    *   Observaci√≥n cl√≠nica: se reportaron valores por debajo de 5 g/dL, indicativos de anemia severa, condici√≥n frecuente en pacientes con ERC avanzada. <a href="https://emedicine.medscape.com/article/1389854-overview" target="_blank">Referencia: Medscape ‚Äì Anemia of Chronic Disease and Kidney Failure</a></p>
    *   Efecto sobre el modelo: estos valores bajos pueden sesgar el modelo si no se representan adecuadamente como un estado cl√≠nico.
    *   Posible soluci√≥n: en lugar de transformaci√≥n estad√≠stica, reclasificar los valores en categor√≠as como "bajo", "normal" y "alto", de acuerdo con rangos cl√≠nicos estandarizados, para facilitar la interpretaci√≥n cl√≠nica y mejorar la robustez del modelo.

7. Volumen celular empaquetado ("pvc"):
    *   Observaci√≥n cl√≠nica: se han detectado valores por debajo de 20%, lo cual indica un volumen celular empaquetado (hematocrito) reducido. Valores bajos en el porcentaje de gl√≥bulos rojos en la sangre pueden ser indicativos de anemia, que es com√∫n en pacientes con enfermedad renal cr√≥nica debido a la disminuci√≥n en la producci√≥n de eritropoyetina. <a href="https://www.labtestsonline.es/tests/hematocrito" target="_blank">Referencia: SeMedLab - Hematocrito.</a></p>
    *   Efecto sobre el modelo: si estos valores extremos son poco frecuentes (como es nuestro caso), pueden introducir desequilibrios en modelos sensibles a los rangos num√©ricos.
    *   Posible soluci√≥n: aplicar transformaciones o normalizaci√≥n robusta para suavizar el efecto de estos extremos.

8. Recuento de gl√≥bulos blancos ("wc"):
    *   Observaci√≥n cl√≠nica: elevaciones significativas del conteo leucocitario pueden estar asociadas con procesos inflamatorios o infecciosos frecuentes en pacientes con ERC. <a href="https://www.researchgate.net/publication/318354798_Low_white_blood_cell_count_is_independently_associated_with_chronic_kidney_disease_progression_in_the_elderly_The_CKD-ROUTE_study" target="_blank">Referencia: Low white blood cell count is independently associated with chronic kidney disease progression in the elderly: The CKD-ROUTE study.</a></p>
    *   Efecto sobre el modelo: outliers muy elevados pueden sesgar el modelo hacia la identificaci√≥n de infecciones como factor predominante, afectando la precisi√≥n general.
    *   Posible soluci√≥n: utilizar una transformaci√≥n logar√≠tmica para suavizar la influencia de estos valores sin eliminarlos, preservando su importancia cl√≠nica.

9. Recuento de gl√≥bulos rojos ("rc"):
    *   Observaci√≥n cl√≠nica: valores superiores a 7‚Äì8 millones/mm¬≥ pueden ser indicativos de policitemia secundaria o errores de laboratorio. <a href="https://www.kidney.org/kidney-topics/anemia-and-chronic-kidney-disease" target="_blank">Referencia: National Kidney Foundation ‚Äì Anemia and CKD</a></p>
    *   Efecto sobre el modelo: dado que la anemia es una complicaci√≥n frecuente en ERC, estos valores podr√≠an interferir en la detecci√≥n correcta de su presencia.
    *   Posible soluci√≥n: aplicar Winsorization o considerar umbrales cl√≠nicos para categorizar los datos y mitigar la distorsi√≥n.

10. Urea en sangre ("bu"):
    *   Observaci√≥n cl√≠nica:  Se han detectado valores extremos de urea significativamente superiores al rango habitual y como se mencion√≥ en el punto 4.1, la urea en sangre es una variable objetivo cr√≠tica en el diagn√≥stico y seguimiento de la enfermedad renal cr√≥nica (ERC).
    *   Efecto sobre el modelo: estos valores extremos pueden distorsionar los modelos predictivos, especialmente si no est√°n adecuadamente tratados. Esto podr√≠a llevar a sobreajustar el modelo hacia casos muy severos que no representan la mayor√≠a de la poblaci√≥n.
    *   Posible soluci√≥n: aplicar t√©cnicas como Winsorization o cut-off cl√≠nico.

**Variables con la mayoria de outliers extremos:**

1. Niveles de az√∫car en orina ("su"):
    *   Observaci√≥n cl√≠nica: en el conjunto de datos, se identifican valores distintos de 0. En condiciones normales, la orina no contiene glucosa detectable. La presencia de m√°s de 0.25 mg/L de glucosa en la orina se considera glucosuria. Una de las causas es una posible disfunci√≥n en la reabsorci√≥n renal de glucosa, lo que podr√≠a estar relacionado con alteraciones en la funci√≥n renal. <a href="https://my.clevelandclinic.org/health/diseases/glycosuria" target="_blank">Referencia: Cleveland clinic - Glycosuria</a></p>
    *   Efecto en el modelo: valores mayores pueden desbalancear la distribuci√≥n e inducir al modelo a sobreestimar la importancia de la glucosuria.
    *   Posible soluci√≥n: en lugar de transformaci√≥n estad√≠stica, convertir la variable a categ√≥tica o binaria, para facilitar la interpretaci√≥n cl√≠nica y mejorar la robustez del modelo.

2. Creatinina s√©rica ("sc"):
    *   Observaci√≥n cl√≠nica: valores elevados de creatinina s√©rica (>10 mg/dL) pueden indicar insuficiencia renal aguda o ERC terminal. Son esperables en contextos cl√≠nicos severos. <a href="https://www.kidneyfund.org/all-about-kidneys/tests/serum-creatinine-test" target="_blank">Referencia: National Kidney Foundation ‚Äì Serum Creatinine Test</a></p>
    *   Efecto sobre el modelo: dado que la creatinina es un biomarcador clave para el diagn√≥stico de ERC, estos valores pueden dominar el comportamiento del modelo si no se gestionan adecuadamente.
    *   Posible soluci√≥n: considerar el uso de t√©cnicas como Winsorization o recorte de percentiles superiores, en lugar de eliminar los datos. Esto preserva la informaci√≥n mientras se reduce la distorsi√≥n estad√≠stica.

### 3.6. Tratamiento de outliers

En esta primera etapa, realizaremos el tratamiento de outliers detectados previamente mediante la t√©cnica de recorte por l√≠mites cl√≠nicamente informados. Adem√°s, corregiremos valores inv√°lidos como aquellos menores a 0, que no son fisiol√≥gicamente posibles para la mayor√≠a de las variables.

El enfoque seguido no es meramente estad√≠stico, sino basado en la evidencia cl√≠nica descrita en el anterior apartado, con el objetivo de preservar la validez m√©dica de los datos sin introducir sesgos innecesarios en el modelo.
"""

import numpy as np
import pandas as pd
from scipy.stats.mstats import winsorize

# Funci√≥n para censurar variables con l√≠mites
def cap_var(df, variables_limits):
    """
    Censura los valores de columnas espec√≠ficas en un DataFrame, de acuerdo con l√≠mites
    definidos por el usuario (m√≠nimos y/o m√°ximos).

    -----------
    Par√°metros:
    -----------
    - df: DataFrame original.
    - variables_limits: diccionario con pares 'variable': (min, max).

    ---------
    Devuelve:
    ---------
    - df_capped: nuevo DataFrame con valores censurados.
    """

    df_capped = df.copy()  # Crear una copia para no modificar el original
    for var, limits in variables_limits.items():
        lower, upper = limits
        capped_col = df[var].copy()
        if lower is not None:
            capped_col = capped_col.clip(lower=lower)
        if upper is not None:
            capped_col = capped_col.clip(upper=upper)
        df_capped[var] = capped_col
    return df_capped

variables_a_censurar = {
    'bu': (0, 250),
    'sod': (100, None),
    'sc': (0, 20),
    'pot': (0, 15),
    'wc': (0, 20000),
    'bgr': (0, 400),
    'al': (0, None),
    'pcv': (0, None),
    'su': (0, None)
}

# Asignar el resultado de cap_var al dataframe
imputed_kidney_df = cap_var(imputed_kidney_df, variables_a_censurar)

# Mostrar los primeros 10 registros
imputed_kidney_df.head(10)

"""#### 3.6.1 Evaluaci√≥n asimetr√≠a, distribuci√≥n, etc.

Despu√©s de aplicar los l√≠mites para censurar valores at√≠picos extremos, es esencial examinar c√≥mo se distribuyen ahora la variables num√©ricas con mayor cantidad de outliers. Esto nos permitir√° validar si el recorte ha logrado reducir sesgos severos y, en particular, si persiste asimetr√≠a, que podr√≠a requerir una transformaci√≥n mate√°tica para mejorar la normalidad de las variables.

El c√≥digo que tenemos a continuaci√≥n, genera gr√°ficos combinados para cada variable, en los que aparecen: histograma con densidad, boxplot, gr√°fico Q-Q, diagrama de viol√≠n y un cuadro con estad√≠sticas descriptivas completas.

Este an√°lisis nos ayuda a cuantificar varios par√°metros, entre los cuales se incluyen:
*   Medidas de tendencia central y dispersi√≥n.
*   Coeficiente de variaci√≥n (CV) para entender la variabilidad relativa.
*   Grado de asimetr√≠a e interpretaci√≥n cualitativa.
*   Resultado del test de Shapiro-Wilk para evaluar la normalidad de la distribuci√≥n.

Finalmente, generamos una tabla resumen con los indicadores clave para todas las variables de inter√©s, lo que facilita su comparaci√≥n y posterior toma de decisiones.
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import skew, shapiro, probplot
import matplotlib.gridspec as gridspec
import time
import matplotlib.font_manager as fm

# Medir tiempo de ejecuci√≥n
start_time = time.time()

# Lista de variables para analizar
variables = ['bu', 'sod', 'sc', 'pot', 'wc', 'bgr', 'bp', 'su']

# Diccionario para nombres completos de variables
variable_names = {
    'bu': 'Urea en sangre',
    'sod': 'Niveles de sodio',
    'sc': 'Creatinina s√©rica',
    'pot': 'Niveles de potasio',
    'wc': 'Recuento de gl√≥bulos blancos',
    'bgr': 'Glucosa en sangre aleatoria',
    'bp': 'Tensi√≥n arterial',
    'su': 'Niveles de azucar en sangre'
}

# Paleta de colores para los gr√°ficos
palette = sns.color_palette("husl", len(variables))
colors = dict(zip(variables, palette))

# Funci√≥n para interpretar la asimetr√≠a
def interpret_skewness(skew_value):
    if skew_value < -1:
        return "asimetr√≠a negativa fuerte"
    elif -1 <= skew_value < -0.5:
        return "asimetr√≠a negativa moderada"
    elif -0.5 <= skew_value < 0:
        return "asimetr√≠a negativa leve"
    elif 0 <= skew_value < 0.5:
        return "asimetr√≠a positiva leve"
    elif 0.5 <= skew_value < 1:
        return "asimetr√≠a positiva moderada"
    else:
        return "asimetr√≠a positiva fuerte"

# Calcular todas las estad√≠sticas
def calculate_statistics(df, variables):
    all_stats = {}

    # Calcular estad√≠sticas para todas las variables de una vez
    describe_df = df[variables].describe()
    quantiles = df[variables].quantile([0.25, 0.75])
    modes = df[variables].mode().iloc[0]
    variances = df[variables].var()
    skewness = df[variables].apply(skew)

    for variable in variables:
        var_stats = {}

        # Estad√≠sticas b√°sicas (ya calculadas en describe)
        var_stats['media'] = describe_df.loc['mean', variable]
        var_stats['desv_tipica'] = describe_df.loc['std', variable]
        var_stats['min'] = describe_df.loc['min', variable]
        var_stats['max'] = describe_df.loc['max', variable]
        var_stats['mediana'] = describe_df.loc['50%', variable]

        # Otras estad√≠sticas
        var_stats['moda'] = modes[variable]
        var_stats['q1'] = quantiles.loc[0.25, variable]
        var_stats['q3'] = quantiles.loc[0.75, variable]
        var_stats['rango'] = var_stats['max'] - var_stats['min']
        var_stats['iqr'] = var_stats['q3'] - var_stats['q1']
        var_stats['varianza'] = variances[variable]
        var_stats['cv'] = (var_stats['desv_tipica'] / var_stats['media']) * 100 if var_stats['media'] != 0 else float('inf')
        var_stats['asimetria'] = skewness[variable]

        # Test de normalidad (esto no se puede vectorizar f√°cilmente)
        var_stats['shapiro_stat'], var_stats['shapiro_p'] = shapiro(df[variable])
        var_stats['normal'] = var_stats['shapiro_p'] > 0.05

        all_stats[variable] = var_stats

    return all_stats

# Calcular todas las estad√≠sticas
all_stats = calculate_statistics(imputed_kidney_df, variables)

# Configurar estilo general
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("notebook", font_scale=1.1)

# Crear un informe para cada variable
for i, variable in enumerate(variables):
    var_stats = all_stats[variable]
    var_name = variable_names.get(variable, variable)

    # Crear figura con layout optimizado
    fig = plt.figure(figsize=(20, 12))
    gs = gridspec.GridSpec(2, 3, width_ratios=[1.2, 1, 1], height_ratios=[1, 1.2], wspace=0.3, hspace=0.3,
                           left=0.05, right=0.95, bottom=0.05, top=0.92)

    # T√≠tulo principal
    plt.suptitle(f'An√°lisis estad√≠stico para: {var_name}', fontsize=18, fontweight='bold', y=1)

    # 1. Histograma con KDE (m√°s eficiente con bins='auto')
    ax1 = plt.subplot(gs[0, 0])
    sns.histplot(imputed_kidney_df[variable], kde=True, color=colors[variable],
                 bins='auto', edgecolor='black', alpha=0.7, ax=ax1)
    ax1.set_title('Histograma con densidad', fontsize=16, fontweight='bold', pad=15)
    ax1.set_xlabel(var_name, fontsize=10, labelpad=15)
    ax1.set_ylabel('Frecuencia', fontsize=10, labelpad=15)

    # 2. Boxplot
    ax2 = plt.subplot(gs[0, 1])
    sns.boxplot(y=imputed_kidney_df[variable], color=colors[variable], ax=ax2)
    ax2.set_title('Diagrama de caja', fontsize=16, fontweight='bold', pad=15)
    ax2.set_ylabel(var_name, fontsize=10, labelpad=15)

    # 3. Q-Q Plot
    ax3 = plt.subplot(gs[1, 0])
    probplot(imputed_kidney_df[variable], dist="norm", plot=ax3)
    ax3.set_title('Gr√°fico Q-Q de normalidad', fontsize=16, fontweight='bold', pad=15)
    ax3.set_xlabel('Cuantiles te√≥ricos', fontsize=10, labelpad=15)
    ax3.set_ylabel('Valores ordenados', fontsize=10, labelpad=15)

    # 4. Violin plot
    ax4 = plt.subplot(gs[1, 1])
    sns.violinplot(y=imputed_kidney_df[variable], color=colors[variable], ax=ax4)
    ax4.set_title('Diagrama de viol√≠n', fontsize=16, fontweight='bold', pad=15)
    ax4.set_ylabel(var_name, fontsize=10, labelpad=15)

    # 5. Estad√≠sticas descriptivas
    ax5 = plt.subplot(gs[:, 2])
    ax5.axis('off')

    # Preparar texto con estad√≠sticas e interpretaciones
    skew_interpretation = interpret_skewness(var_stats['asimetria'])
    normal_text = "sigue" if var_stats['normal'] else "no sigue"

    # Cuadro de texto con estad√≠sticas
    stats_text = f"""

    ‚Ä¢ Medidas de tendencia central:
    Media: {var_stats['media']:.3f}
    Mediana: {var_stats['mediana']:.3f}
    Moda: {var_stats['moda']:.3f}

    ‚Ä¢ Medidas de dispersi√≥n:
    Desviaci√≥n t√≠pica: {var_stats['desv_tipica']:.3f}
    Varianza: {var_stats['varianza']:.3f}
    Coeficiente de variaci√≥n: {var_stats['cv']:.2f}%
    Rango: {var_stats['rango']:.3f} (Min: {var_stats['min']:.3f}, Max: {var_stats['max']:.3f})
    Rango intercuart√≠lico (IQR): {var_stats['iqr']:.3f} (Q1: {var_stats['q1']:.3f}, Q3: {var_stats['q3']:.3f})

    ‚Ä¢ Forma de la distribuci√≥n:
    Coeficiente de asimetr√≠a: {var_stats['asimetria']:.3f} ({skew_interpretation})
    Test de Shapiro-Wilk: estad√≠stico={var_stats['shapiro_stat']:.3f}, p-valor={var_stats['shapiro_p']:.6f}
    La variable {normal_text} una distribuci√≥n normal (Œ±=0.05)

    ‚Ä¢ Interpretaci√≥n:
    {var_stats['media']:.2f} ¬± {var_stats['desv_tipica']:.2f} (media ¬± desviaci√≥n t√≠pica)
    El {var_stats['cv']:.1f}% de variabilidad relativa indica una dispersi√≥n {"alta" if var_stats['cv'] > 30 else "moderada" if var_stats['cv'] > 15 else "baja"}
    La distribuci√≥n est√° {"sesgada a la derecha (asimetr√≠a positiva)" if var_stats['asimetria'] > 0 else "sesgada a la izquierda (asimetr√≠a negativa)" if var_stats['asimetria'] < 0 else "aproximadamente sim√©trica"}
    """

    # Colocar el texto en el centro del √°rea disponible
    ax5.text(0.5, 0.5, stats_text, transform=ax5.transAxes,
             fontsize=13, verticalalignment='center', horizontalalignment='center',
             bbox=dict(boxstyle='round,pad=1', facecolor=colors[variable], alpha=0.3))

    ax5.text(0.5, 0.71, 'Estad√≠sticas descriptivas', fontsize=16, fontweight='bold',
             horizontalalignment='center', transform=ax5.transAxes)

    fig.subplots_adjust()
    plt.show()
    print("\n")


# Crear una tabla resumen con todas las estad√≠sticas
summary_data = {
    'Variable': [variable_names.get(var, var) for var in variables],
    'Media': [all_stats[var]['media'] for var in variables],
    'Mediana': [all_stats[var]['mediana'] for var in variables],
    'Desv. T√≠pica': [all_stats[var]['desv_tipica'] for var in variables],
    'CV (%)': [all_stats[var]['cv'] for var in variables],
    'Asimetr√≠a': [all_stats[var]['asimetria'] for var in variables],
    'P-valor Shapiro': [all_stats[var]['shapiro_p'] for var in variables],
    'Normal': [all_stats[var]['normal'] for var in variables]
}

summary_df = pd.DataFrame(summary_data)
print("üîπ Resumen de estad√≠sticas para todas las variables:\n")
summary_df_styled = summary_df.style.format({
    'Media': '{:.3f}',
    'Mediana': '{:.3f}',
    'Desv. T√≠pica': '{:.3f}',
    'CV (%)': '{:.3f}',
    'Asimetr√≠a': '{:.3f}',
    'P-valor Shapiro': '{:.3f}'
})

# Aplicar estilos a la tabla
styled_table6 = summary_df_styled.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})

styled_table6 = styled_table6.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

# Mostrar la tabla estilizada
display(styled_table6)

# Medir y mostrar tiempo de ejecuci√≥n
execution_time = time.time() - start_time
print(f"\nTiempo de ejecuci√≥n: {execution_time:.2f} segundos")

"""#### 3.6.2 Valoraci√≥n de aplicaci√≥n de transformaciones

Despu√©s de analizar el comportamiento de las variables tras aplicar l√≠mites cl√≠nicamente para censurar valores extremos, el siguiente paso consiste en evaluar y corregir la asimetr√≠a (skewness) de aquellas distribuciones que aun presentan sesgo, en nuestro caso, principalmente positivo.

Este sesgo puede afectar negativamente a modelos estad√≠sticos, especialmente aquellos que asumen normalidad o simetr√≠a en las variables. Por este motivo, el siguiente paso en nuestro an√°lisis es aplicar distintas transformaciones matem√°ticas con el objetivo de reducir dicha asimetr√≠a y acercar las variables a una distribuci√≥n m√°s sim√©trica o normal para as√≠ mejorar tanto la validez estad√≠stica como el rendimiento predictivo de los modelos.

En total, hemos aplicado cuatro tipos de transformaciones distintas a cada variable num√©rica del dataset. Cada una est√° pensada para aplicarse en situaciones espec√≠ficas seg√∫n la forma y caracter√≠sticas de la variable.:

1. Transformaci√≥n logar√≠tmica: aplica la funci√≥n `log(1 + x)` sobre cada valor de la variable. Esta forma espec√≠fica permite incluir tambi√©n valores iguales a cero, evitando errores matem√°ticos. Al comprimir valores grandes m√°s que los peque√±os, el resultado tiende a redistribuir los datos de manera m√°s sim√©trica y reducir el impacto de valores at√≠picos altos. T√≠picamente se utiliza en:
    *   Variables con gran sesgo hacia la derecha (asimetr√≠a positiva).
    *   Datos con una gran dispersi√≥n y valores extremos.
    *   Cuando los valores son estrictamente positivos o pueden ser desplazados artificialmente para serlo.

2. Transformaci√≥n por ra√≠z cuadrada: como su nombre indica, aplicar una ra√≠z cuadrada a una variable (`sqrt(x)`). Esta es una forma menos agresiva que la logar√≠tmica de comprimir los valores altos. A diferencia del logaritmo, la ra√≠z cuadrada es menos sensible a valores extremos, por lo que es adecuada en situaciones donde se desea mejorar la simetr√≠a sin modificar demasiado la estructura original de los datos. Com√∫nmente se emplea en:
    *   Variables de conteo (frecuencias, recuentos de c√©lulas, etc.).
    *   Datos positivos con sesgo moderado (asimetr√≠a no excesiva).
    *   Situaciones donde se busca un compromiso entre estabilizar la varianza y preservar la interpretabilidad.

3. Winsorizaci√≥n: es una t√©cnica de recorte o censura suave que reemplaza los valores m√°s extremos de una distribuci√≥n por un l√≠mite definido. En este caso, se reemplazan los valores por debajo del percentil 1 y por encima del percentil 99 por dichos percentiles respectivos. A diferencia de las transformaciones anteriores, no cambia la escala ni la forma general de los datos, sino que mitiga el impacto de los outliers extremos sin eliminarlos. Es √∫til cuando:
    *   Se tiene alta sensibilidad a outliers.
    *   No se quiere transformar la escala original.
    *   El sesgo es causado por valores at√≠picos y no por la forma global de la distribuci√≥n.

4. Transformaci√≥n Yeo-Johnson: internamente, aplica una transformaci√≥n param√©trica que busca encontrar una lambda √≥ptima (par√°metro de forma) que maximice la normalidad de la variable transformada. La transformaci√≥n Yeo-Johnson se define por tramos, dependiendo del signo del valor de entrada x:

 Para cualquier valor real x‚ààR, y un par√°metro de transformaci√≥n Œª‚ààR:

$$T(x,\lambda )=\left\{\begin{matrix}
\frac{[(x+1)^{\lambda}-1]}{\lambda},  &  x\geq 0,\lambda\neq 0\\
\log(x+1),  &  x\geq 0,\lambda= 0\\
-\frac{[(-x+1)^{(2-\lambda)}-1]}{2-\lambda}, &  x<0,\lambda\neq 2\\
-\log(-x+1), &  x<0,\lambda= 2\\
\end{matrix}\right.$$

Para tener una mejor idea de su funcionamiento, a continuaci√≥n tenermos una interpretaci√≥n de lo que sucede con distintos valores de lambda:
*   Œª > 1: expande valores grandes, aplana los peque√±os.
*   Œª < 1: comprime valores grandes, ampl√≠a los peque√±os.
*   Œª = 1: transformaci√≥n lineal (sin cambios).
*   Œª = 0: se convierte en una transformaci√≥n logar√≠tmica.
*   Œª = 2: aplica logaritmo sobre valores negativos (invirtiendo el eje).

Esta t√©cnica es muy poderosa cuando se trabaja con m√∫ltiples tipos de sesgo, ya que puede corregir tanto asimetr√≠as positivas como negativas. Normalmente, se recomendada para:
*   Variables con mezcla de valores negativos, ceros y positivos.
*   Distribuciones muy alejadas de la normalidad.

Finalmente, para poder conseguir esto, hemos creado la funci√≥n `compare_transformations(DataFrame)`, cuyo objetivo es automatizar el proceso de transformaci√≥n de variables num√©ricas, calcular la asimetr√≠a antes y despu√©s de aplicar cada transformaci√≥n, y ofrecer un resumen visual para comparar cu√°l t√©cnica fue m√°s eficaz para cada variable.

Adem√°s, hemos decidido aplicar un estilo visual que resalta la transformaci√≥n m√°s efectiva para cada variable (la que logra menor asimetr√≠a en valor absoluto) en color azul.
"""

df = imputed_kidney_df

import pandas as pd
import numpy as np
from scipy.stats import mstats, skew
from sklearn.preprocessing import PowerTransformer
import matplotlib.pyplot as plt

def compare_transformations(DataFrame):
    # Crear una copia del DataFrame para no modificar el original
    df_copy = DataFrame.copy()

    # Seleccionar columnas num√©ricas
    numeric_columns = df_copy.select_dtypes(include=['number']).columns
    skew_data = []

    for var in numeric_columns:
        # Guardamos la asimetr√≠a antes de la transformaci√≥n
        skewness_before = skew(df_copy[var].dropna())

        # 1. Transformaci√≥n logar√≠tmica (manejo de skewness positivo)
        # Creamos una copia para la transformaci√≥n logar√≠tmica
        log_data = df_copy[var].copy()
        # Reemplazamos valores negativos con la mediana
        median_value = log_data[log_data > 0].median() if any(log_data > 0) else 1
        log_data = log_data.map(lambda x: median_value if x <= 0 else x)
        # Aplicamos la transformaci√≥n
        log_transformed = np.log1p(log_data)
        # Guardamos la asimetr√≠a despu√©s de la transformaci√≥n
        skewness_after_log = skew(log_transformed.dropna())

        # 2. Transformaci√≥n ra√≠z cuadrada (manejo de variables sesgadas)
        # Creamos una copia para la transformaci√≥n sqrt
        sqrt_data = df_copy[var].copy()
        # Reemplazamos valores negativos con la mediana
        sqrt_data = sqrt_data.map(lambda x: median_value if x < 0 else x)
        # Aplicamos la transformaci√≥n
        sqrt_transformed = np.sqrt(sqrt_data)
        # Guardamos la asimetr√≠a despu√©s de la transformaci√≥n
        skewness_after_sqrt = skew(sqrt_transformed.dropna())

        # 3. Winsorizaci√≥n (1% en ambos extremos)
        # Creamos una copia para la transformaci√≥n de winsorizaci√≥n
        win_data = df_copy[var].dropna().values  # Necesitamos un array para winsorize
        if len(win_data) > 0:  # Verificamos que no est√© vac√≠o
            # Aplicamos la transformaci√≥n
            win_transformed = mstats.winsorize(win_data, limits=[0.01, 0.01])
            # Guardamos la asimetr√≠a despu√©s de la transformaci√≥n
            skewness_after_win = skew(win_transformed)
        else:
            skewness_after_win = np.nan

        # 4. Yeo-Johnson (manejo de skewness negativos)
        # Creamos una copia para la transformaci√≥n Yeo-Johnson
        yeo_data = df_copy[var].copy().to_frame()
        # Rellenamos NaNs con la mediana
        yeo_data = yeo_data.fillna(yeo_data.median())

        # Verificamos que tengamos datos suficientes
        if yeo_data.shape[0] > 0 and not yeo_data.isna().all().all():
            # Aplicamos la transformaci√≥n
            pt = PowerTransformer(method='yeo-johnson')
            try:
                yeo_transformed = pt.fit_transform(yeo_data).flatten()
                # Guardamos la asimetr√≠a despu√©s de la transformaci√≥n
                skewness_after_yeo = skew(yeo_transformed)
            except:
                skewness_after_yeo = np.nan
        else:
            skewness_after_yeo = np.nan

        # Guardar resultados para cada columna
        skew_data.append({
                'Variable': var,
                'Asimetr√≠a antes': skewness_before,
                'Asimetr√≠a log': skewness_after_log,
                'Asimetr√≠a sqrt': skewness_after_sqrt,
                'Asimetr√≠a Winsorizaci√≥n': skewness_after_win,
                'Asimetr√≠a Yeo-Johnson': skewness_after_yeo
        })

        # Guardamos las transformaciones en la copia del DataFrame
        # Solo si deseamos mantener las transformaciones
        df_copy[f'log_{var}'] = log_transformed
        df_copy[f'sqrt_{var}'] = sqrt_transformed
        if len(win_data) > 0:
            # Necesitamos convertir win_transformed de nuevo a Series con los mismos √≠ndices
            win_series = pd.Series(win_transformed, index=df_copy[var].dropna().index)
            df_copy.loc[win_series.index, f'win_{var}'] = win_series
        if 'yeo_transformed' in locals() and not pd.isna(skewness_after_yeo):
            df_copy.loc[yeo_data.index, f'yeo_{var}'] = yeo_transformed

    # Crear DataFrame con resultados de la asimetria
    skew_df = pd.DataFrame(skew_data)
    skew_df.set_index('Variable', inplace=True)

    # Funci√≥n para aplicar estilos a la tabla
    def apply_styles(skew_df):
        # Resaltar las celdas con valores m√°s cercanos a cero (menor asimetr√≠a en valor absoluto)
        def highlight_min_abs(row):
            min_cols = ['Asimetr√≠a log', 'Asimetr√≠a sqrt', 'Asimetr√≠a Winsorizaci√≥n', 'Asimetr√≠a Yeo-Johnson']
            valid_values = row[min_cols].dropna()

            if len(valid_values) > 0:
                # Calculamos el valor absoluto y encontramos el m√≠nimo
                abs_values = valid_values.abs()
                min_abs_val = abs_values.min()
                # Identificamos qu√© columna tiene el valor m√≠nimo absoluto
                min_abs_col = abs_values[abs_values == min_abs_val].index[0]

                return ['background-color: #468faf' if (col == min_abs_col) else '' for col, v in row.items()]
            else:
                return ['' for _ in row.items()]

        # Aplicar formato a valores num√©ricos
        styled = skew_df.style.format({
            'Asimetr√≠a antes': '{:.4f}',
            'Asimetr√≠a log': '{:.4f}',
            'Asimetr√≠a sqrt': '{:.4f}',
            'Asimetr√≠a Winsorizaci√≥n': '{:.4f}',
            'Asimetr√≠a Yeo-Johnson': '{:.4f}'
        })

        # Aplicar resaltado de m√≠nimos en valor absoluto
        styled = styled.apply(highlight_min_abs, axis=1)

        return styled

    styled_df_skew = apply_styles(skew_df)

    print("\nüîπ Resumen de las asimetr√≠as para todas las variables:\n")
    from IPython.display import display
    display(styled_df_skew)

    # Verificaci√≥n de NaNs en el DataFrame transformado
    print("\nüîπ Total NaNs en el DataFrame:", df_copy.isna().sum().sum())
    print("\nüîπ NaNs por columna:\n", df_copy.isna().sum())

    return df_copy, styled_df_skew

df_transformado, tabla_asimetria = compare_transformations(df)

"""Despu√©s de observar los resultados de la tabla de asimetr√≠a, podemos apreciar que la gran mayor√≠a de las variables presentan una reducci√≥n significativa en su asimetr√≠a tras aplicar la transformaci√≥n de Yeo-Johnson. Esto se debe a que esta transformaci√≥n ajusta de forma flexible tanto datos positivos como negativos, adapt√°ndose a la estructura de cada variable.

Sin embargo, no siempre es la mejor opci√≥n cuando la variable ya est√° cerca de una distribuci√≥n normal o cuando el sesgo no es muy fuerte ya que esta transformaci√≥n est√°n dise√±adas para corregir asimetr√≠as notables y esto puede tener efectos contraproducentes:
*   Distorsi√≥n innecesaria de la forma original.
*   P√©rdida de informaci√≥n relativa.
*   Reducci√≥n del poder explicativo.
*   Complejidad adicional.

Por eso, es importante evaluar la necesidad de la transformaci√≥n caso por caso. Si la asimetr√≠a original es baja, muchas veces no es necesario aplicar ninguna transformaci√≥n, ya que esa variable puede considerarse suficientemente normal para prop√≥sitos pr√°cticos.

A continuaci√≥n, analizamos las nuevas distribuciones de las variables transformadas, tanto de forma visual como estad√≠stica, con el objetivo de comprobar si las transformaciones lograron acercar las distribuciones a una forma m√°s sim√©trica y parecida a la normal. Este paso es crucial para validar la efectividad de la transformaci√≥n y guiar el preprocesamiento de datos en futuros modelos.


"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import skew, shapiro, probplot
import matplotlib.gridspec as gridspec
import time

# Medir tiempo de ejecuci√≥n
start_time = time.time()

# Lista de variables para analizar
variables = ['yeo_bu', 'sqrt_bu', 'log_bu', 'win_bu', 'win_sod', 'yeo_sod', 'log_sc', 'yeo_sc', 'win_pot', 'yeo_pot', 'sqrt_wc', 'yeo_wc', 'log_bgr', 'yeo_bgr', 'log_bp', 'yeo_bp', 'log_su', 'yeo_su']

# Diccionario para nombres completos de variables
variable_names = {
    'bu': 'Urea en sangre',
    'sod': 'Niveles de sodio',
    'sc': 'Creatinina s√©rica',
    'pot': 'Niveles de potasio',
    'wc': 'Recuento de gl√≥bulos blancos',
    'bgr': 'Glucosa en sangre aleatoria',
    'bp': 'Tensi√≥n arterial',
    'su': 'Niveles de azucar en sangre'
}

# Paleta de colores para los gr√°ficos
palette = sns.color_palette("husl", len(variables))
colors = dict(zip(variables, palette))

# Calcular todas las estad√≠sticas
stats = calculate_statistics(df_transformado, variables)

# Configurar estilo general
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("notebook", font_scale=1.1)

# Crear un informe para cada variable
for i, variable in enumerate(variables):
    var_stats = stats[variable]
    var_name = variable_names.get(variable, variable)

    # Crear figura con layout optimizado
    fig = plt.figure(figsize=(20, 12))
    gs = gridspec.GridSpec(2, 3, width_ratios=[1.2, 1, 1], height_ratios=[1, 1.2], wspace=0.3, hspace=0.3,
                           left=0.05, right=0.95, bottom=0.05, top=0.92)

    # T√≠tulo principal
    plt.suptitle(f'An√°lisis estad√≠stico para: {var_name}', fontsize=18, fontweight='bold', y=1)

    # 1. Histograma con KDE
    ax1 = plt.subplot(gs[0, 0])
    sns.histplot(df_transformado[variable], kde=True, color=colors[variable],
                 bins='auto', edgecolor='black', alpha=0.7, ax=ax1)
    ax1.set_title('Histograma con densidad', fontsize=16, fontweight='bold', pad=15)
    ax1.set_xlabel(var_name, fontsize=10, labelpad=15)
    ax1.set_ylabel('Frecuencia', fontsize=10, labelpad=15)

    # 2. Boxplot
    ax2 = plt.subplot(gs[0, 1])
    sns.boxplot(y=df_transformado[variable], color=colors[variable], ax=ax2)
    ax2.set_title('Diagrama de caja', fontsize=16, fontweight='bold', pad=15)
    ax2.set_ylabel(var_name, fontsize=10, labelpad=15)

    # 3. Q-Q Plot
    ax3 = plt.subplot(gs[1, 0])
    probplot(df_transformado[variable], dist="norm", plot=ax3)
    ax3.set_title('Gr√°fico Q-Q de normalidad', fontsize=16, fontweight='bold', pad=15)
    ax3.set_xlabel('Cuantiles te√≥ricos', fontsize=10, labelpad=15)
    ax3.set_ylabel('Valores ordenados', fontsize=10, labelpad=15)

    # 4. Violin plot
    ax4 = plt.subplot(gs[1, 1])
    sns.violinplot(y=df_transformado[variable], color=colors[variable], ax=ax4)
    ax4.set_title('Diagrama de viol√≠n', fontsize=16, fontweight='bold', pad=15)
    ax4.set_ylabel(var_name, fontsize=10, labelpad=15)

    # 5. Estad√≠sticas descriptivas
    ax5 = plt.subplot(gs[:, 2])
    ax5.axis('off')

    # Preparar texto con estad√≠sticas e interpretaciones
    skew_interp = interpret_skewness(var_stats['asimetria'])
    normal_text = "sigue" if var_stats['normal'] else "no sigue"

    # Cuadro de texto con estad√≠sticas
    stats_text = f"""

    ‚Ä¢ Medidas de tendencia central:
    Media: {var_stats['media']:.3f}
    Mediana: {var_stats['mediana']:.3f}
    Moda: {var_stats['moda']:.3f}

    ‚Ä¢ Medidas de dispersi√≥n:
    Desviaci√≥n t√≠pica: {var_stats['desv_tipica']:.3f}
    Varianza: {var_stats['varianza']:.3f}
    Coeficiente de variaci√≥n: {var_stats['cv']:.2f}%
    Rango: {var_stats['rango']:.3f} (Min: {var_stats['min']:.3f}, Max: {var_stats['max']:.3f})
    Rango intercuart√≠lico (IQR): {var_stats['iqr']:.3f} (Q1: {var_stats['q1']:.3f}, Q3: {var_stats['q3']:.3f})

    ‚Ä¢ Forma de la distribuci√≥n:
    Coeficiente de asimetr√≠a: {var_stats['asimetria']:.3f} ({skew_interp})
    Test de Shapiro-Wilk: estad√≠stico={var_stats['shapiro_stat']:.3f}, p-valor={var_stats['shapiro_p']:.6f}
    La variable {normal_text} una distribuci√≥n normal (Œ±=0.05)

    ‚Ä¢ Interpretaci√≥n:
    {var_stats['media']:.2f} ¬± {var_stats['desv_tipica']:.2f} (media ¬± desviaci√≥n t√≠pica)
    El {var_stats['cv']:.1f}% de variabilidad relativa indica una dispersi√≥n {"alta" if var_stats['cv'] > 30 else "moderada" if var_stats['cv'] > 15 else "baja"}
    La distribuci√≥n est√° {"sesgada a la derecha (asimetr√≠a positiva)" if var_stats['asimetria'] > 0 else "sesgada a la izquierda (asimetr√≠a negativa)" if var_stats['asimetria'] < 0 else "aproximadamente sim√©trica"}
    """

    # Colocar el texto en el centro del √°rea disponible
    ax5.text(0.5, 0.5, stats_text, transform=ax5.transAxes,
             fontsize=13, verticalalignment='center', horizontalalignment='center',
             bbox=dict(boxstyle='round,pad=1', facecolor=colors[variable], alpha=0.3))

    ax5.text(0.5, 0.71, 'Estad√≠sticas descriptivas', fontsize=16, fontweight='bold',
             horizontalalignment='center', transform=ax5.transAxes)

    fig.subplots_adjust()
    plt.show()
    print("\n")


# Crear una tabla resumen con todas las estad√≠sticas
summary_skew_data = {
    'Variable': [variable_names.get(var, var) for var in variables],
    'Media': [stats[var]['media'] for var in variables],
    'Mediana': [stats[var]['mediana'] for var in variables],
    'Desv. T√≠pica': [stats[var]['desv_tipica'] for var in variables],
    'CV (%)': [stats[var]['cv'] for var in variables],
    'Asimetr√≠a': [stats[var]['asimetria'] for var in variables],
    'P-valor Shapiro': [stats[var]['shapiro_p'] for var in variables],
    'Normal': [stats[var]['normal'] for var in variables]
}

summary_skew_df = pd.DataFrame(summary_skew_data)
print("üîπ Resumen de estad√≠sticas para todas las variables:\n")
summary_skew_df_styled = summary_skew_df.style.format({
    'Media': '{:.3f}',
    'Mediana': '{:.3f}',
    'Desv. T√≠pica': '{:.3f}',
    'CV (%)': '{:.3f}',
    'Asimetr√≠a': '{:.3f}',
    'P-valor Shapiro': '{:.3f}'
})

# Aplicar estilos a la tabla
styled_table7 = summary_skew_df_styled.set_properties(**{
    'text-align': 'center',
    'border': '1px solid #000000',
    'background-color': '#ffffff',
    'color': '#000000',
})

styled_table7 = styled_table7.set_table_styles([
    {'selector': 'th', 'props': [
      ('background-color', '#457b9d'),
      ('color', 'white'),
      ('font-weight', 'bold'),
      ('text-align', 'center'),
      ('padding', '10px'),
      ('border', '1px solid #000000')
    ]},
])

# Mostrar la tabla estilizada
display(styled_table7)

# Medir y mostrar tiempo de ejecuci√≥n
execution_time_skew = time.time() - start_time
print(f"\nTiempo de ejecuci√≥n: {execution_time_skew:.2f} segundos")

"""Gracias al an√°lisis visual y num√©rico de las variables transformadas podemos observar que, aunque una transformaci√≥n pueda parecer la m√°s adecuada en funci√≥n de la mejora en la asimetr√≠a (skewness), no necesariamente es la mejor opci√≥n desde una perspectiva estad√≠stica o de interpretaci√≥n del modelo.

Para ejemplificarlo, vamos a realizar una evaluaci√≥n exhaustiva de las diferentes transformaciones aplicadas a nuestra variable objetivo "bu":

1. Distribuci√≥n original (`bu`): presenta una distribuci√≥n positivamente sesgada (asimetr√≠a hacia la derecha), lo que es com√∫n en variables biom√©dicas como niveles de urea, donde existen valores m√°ximos at√≠picamente altos debido a condiciones cl√≠nicas severas, como la insuficiencia renal.

2. Transformaci√≥n logar√≠tmica (`log_bu`): reduce la amplitud de los valores altos y comprime la escala hacia los valores inferiores, lo cual disminuye notablemente el sesgo positivo.
    *   Visualmente, la distribuci√≥n aparece m√°s sim√©trica, pero ha generado una asimetr√≠a inversa (ligeramente negativa).
    *   Este efecto inverso afect√≥ negativamente la distribuci√≥n de los residuos, generando heterocedasticidad en los modelos.

3. Ra√≠z cuadrada (`sqrt_bu`): suaviza los valores extremos (reduce el sesgo) sin comprimir excesivamente la distribuci√≥n, ofreciendo un equilibrio √≥ptimo.
    *   Reduce la influencia de valores at√≠picos extremos sin alterar dr√°sticamente los valores originales.
    *   La distribuci√≥n resultante es m√°s sim√©trica, con colas menos pesadas y valores extremos suavizados, pero no eliminados.
    *   El gr√°fico Q-Q muestra una alineaci√≥n razonablemente buena, y el boxplot refleja una reducci√≥n efectiva de los valores at√≠picos sin necesidad de truncamiento.
    *   No se introducen valores negativos ni se altera la interpretaci√≥n cl√≠nica de los datos.

4. Winsorizaci√≥n (`win_bu`): esta transformaci√≥n no logra mejorar la simetr√≠a de manera sino que incluso la aumenta.
    *   Se observa que los valores extremos han sido recortados artificialmente, lo que resulta en una distribuci√≥n m√°s compacta pero menos representativa de la variabilidad real.
    *   El boxplot muestra una reducci√≥n de los valores at√≠picos, pero esta simplificaci√≥n conlleva la p√©rdida de informaci√≥n cl√≠nica potencialmente relevante.

5. Transformaci√≥n Yeo-Johnson (`yeo_bu`): fue la m√°s eficaz reduciendo la asimetr√≠a, logrando una curva visualmente m√°s sim√©trica y con mejor alineaci√≥n en el gr√°fico Q-Q.
    *   Sin embargo, introduce valores negativos en el resultado transformado, lo que puede ser problem√°tico desde el punto de vista cl√≠nico, ya que la urea en sangre no puede ser negativa ni tener un significado en ese rango.
    *   Adem√°s, mostr√≥ una dispersi√≥n anormalmente alta, lo cual puede amplificar la varianza de los errores y afectar la estabilidad del modelo.
    *   Si bien la normalidad se mejora estad√≠sticamente, pierde sentido pr√°ctico e interpretativo, comprometiendo la utilidad de esta transformaci√≥n.

En resumen, aunque los indicadores de asimetr√≠a son un buen punto de partida, la elecci√≥n de la mejor transformaci√≥n debe basarse en un enfoque integral, considerando tanto el ajuste estad√≠stico como la interpretaci√≥n cl√≠nica y el impacto en el modelo. En este caso, la transformaci√≥n por ra√≠z cuadrada fue la que ofreci√≥ el mejor compromiso entre simplicidad, interpretabilidad y mejora estad√≠stica.

Observando los diferentes resultados del resto de variables, podemos concluir que aunque la transformaci√≥n Yeo-Johnson ha demostrado ser una de las m√°s eficaces a la hora de corregir la asimetr√≠a de las variables, genera valores negativos en los registros.

Este comportamiento es problem√°tico en nuestro caso, ya que:
*   Desde el punto de vista biol√≥gico, no es posible tener una concentraci√≥n negativa/volumen/etc., ya que se tratan de medidas que por definici√≥n deben ser igual o mayor a cero, y su aparici√≥n en valores negativos carece de sentido cl√≠nico.
*   La introducci√≥n de estos valores compromete la interpretaci√≥n de los resultados y puede inducir errores si se utilizan en an√°lisis posteriores o si se comunican sin un contexto estad√≠stico claro.
*   Puede afectar el rendimiento del modelo predictivo, ya que algunos algoritmos pueden verse influenciados por la aparici√≥n de valores fuera del rango esperado.

# 4. Modelos de regresi√≥n

### 4.1. Codificaci√≥n de variables categ√≥ricas

Despu√©s de haber trabajado en profundidad con las variables num√©ricas, ahora nos enfocamos en las variables categ√≥ricas, que tambi√©n deben estar adecuadamente representadas para ser utilizadas en nuestro modelo de regresi√≥n.

Los modelos estad√≠sticos tradicionales, como la regresi√≥n lineal, no pueden trabajar directamente con variables categ√≥ricas expresadas como texto o etiquetas. Por ello, es necesario codificarlas num√©ricamente. Una de las t√©cnicas m√°s utilizadas es la codificaci√≥n mediante variables ficticias o dummies, que convierte cada categor√≠a en una nueva columna binaria (0 o 1), indicando la presencia o ausencia de esa categor√≠a en cada observaci√≥n.

En este paso, realizaremos dos versiones de codificaci√≥n one-hot para nuestras variables categ√≥ricas:
*   Una versi√≥n que mantiene todas las categor√≠as.
*   Otra versi√≥n que elimina una categor√≠a por variable para evitar colinealidad perfecta (trampa de las variables ficticias). Esto ocurre porque, cuando una variable categ√≥rica tiene k categor√≠as y generamos k columnas dummy, se introduce una relaci√≥n lineal perfecta entre ellas: una columna se puede reconstruir a partir de las dem√°s, lo que viola los supuestos de independencia lineal del modelo.

El segundo enfoque comienza generando dummies para todas las categor√≠as (usando `drop_first=False`), y luego elimina manualmente la √∫ltima categor√≠a alfab√©ticamente de cada variable categ√≥rica.

Aunque podr√≠amos haber utilizado directamente `pd.get_dummies(..., drop_first=True)` para evitar esta colinealidad, hemos optado por hacerlo manualmente por las siguientes razones:
*   Nos da m√°s control sobre qu√© categor√≠a eliminar como referencia. En este caso quer√≠amos preservar la variable `classification_ckd`
*   Evitamos posibles errores en el orden interno de las categor√≠as cuando no est√°n correctamente organizadas.
*   Nos permite inspeccionar los nombres completos de las dummies antes de eliminarlas, lo que puede ser √∫til para tener documentaci√≥n.

Adem√°s, hemos decidido eliminar las variables transformadas mediante la t√©cnica de Yeo-Johnson, ya que introduc√≠an valores negativos en variables cuyo dominio natural es estrictamente positivo.

Esto resulta problem√°tico ya que modelar con transformaciones que introducen valores fuera del rango esperado puede distorsionar la distribuci√≥n real de los datos, dificultar la interpretaci√≥n cl√≠nica y afectar la calidad de los modelos predictivos.

As√≠ que hemos priorizado conservar las otras transformaciones ya que respetan la naturaleza positiva de las variables y, al mismo tiempo, contribuyen a mejorar la simetr√≠a y a reducir la influencia de valores at√≠picos.
"""

import pandas as pd
import numpy as np

# Separar columnas num√©ricas y categ√≥ricas
numeric_cols_re = df_transformado.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols_re.remove('bu')

# Eliminar columnas transformadas con Yeo-Johnson (ya que contienen valores negativos y no nos interesan)
yeo_cols = [col for col in numeric_cols_re if col.startswith('yeo_')]
for col in yeo_cols:
    if col in numeric_cols_re:
        numeric_cols_re.remove(col)

categorical_cols_re = df_transformado.select_dtypes(include=['object', 'category']).columns.tolist()

# 1. Versi√≥n con todo dummies
dummies_all = pd.get_dummies(df_transformado[categorical_cols_re], drop_first=False)
combined_kidney_df_all = pd.concat([
    df_transformado[numeric_cols_re],
    dummies_all,
    df_transformado[['bu']]
], axis=1)

# 2. Versi√≥n que elimina la √∫ltima categor√≠a alfab√©ticamente
dummies_drop_last = pd.get_dummies(df_transformado[categorical_cols_re], drop_first=False)

# Eliminar la √∫ltima categor√≠a alfab√©ticamente para cada columna categ√≥rica
dummies_drop_last.drop(
    columns=[f"{col}_{sorted(df_transformado[col].dropna().unique())[-1]}" for col in categorical_cols_re],
    inplace=True
)

combined_kidney_df = pd.concat([
    df_transformado[numeric_cols_re],
    dummies_drop_last,
    df_transformado[['bu']]
], axis=1)

"""### 4.2. An√°lisis de correlaci√≥n con la variable objetivo

Tras completar la transformaci√≥n de las variables num√©ricas y la codificaci√≥n de las variables categ√≥ricas mediante one-hot encoding, estamos en condiciones de realizar un an√°lisis m√°s exhaustivo de las relaciones lineales entre todas las variables del conjunto de datos.

Para ello, lo primero que hemos decidido es emplear un mapa de calor (`heatmap`) de la matriz de correlaciones, que permite visualizar de forma intuitiva la intensidad y direcci√≥n de las relaciones lineales entre pares de variables. Dicha matriz nos sirve para identificar:
*   Qu√© variables presentan una mayor o menor correlaci√≥n lineal con la variable objetivo "bu" (urea en sangre).
*   Posibles relaciones colineales entre predictores, que podr√≠an influir en el rendimiento de los modelos de regresi√≥n.
*   Variables redundantes o poco informativas desde la perspectiva del modelado.

Asimismo, eliminamos columnas de variables repetidas despu√©s de realizar las transformaciones.
"""

# Eliminar columnas innecesarias
columnas_a_eliminar = ['log_bu', 'bu', 'win_bu', 'sqrt_sc', 'win_sc', 'sc', 'win_pcv', 'pcv', 'sqrt_pcv',
                       'hemo', 'win_hemo','log_hemo', 'log_rc', 'sqrt_rc', 'rc', 'al', 'win_al', 'log_sg', 'sqrt_sg',
                       'sg','log_age', 'win_age', 'sqrt_age', 'sod', 'sqrt_sod', 'su', 'win_su', 'log_sod', 'sqrt_sod',
                       'pot','win_pot','sqrt_pot','sqrt_bp', 'bp', 'log_al','win_bgr', 'wc', 'sqrt_wc', 'log_wc',
                       'bgr','sqrt_bgr', 'log_bp','log_su']

df_filtrado_1 = combined_kidney_df.drop(columns=columnas_a_eliminar, errors='ignore')

# Matriz de correlaci√≥n
corr_matrix = df_filtrado_1.corr()

# Crear m√°scara para mostrar solo una mitad
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.set(style="white")

# Crear gr√°fico
plt.figure(figsize=(16, 14))
sns.heatmap(corr_matrix,
            mask=mask,
            cmap='coolwarm',
            vmax=1.0, vmin=-1.0,
            center=0,
            square=True,
            linewidths=0.8,
            cbar_kws={"shrink": .6},
            annot=True, fmt=".2f", annot_kws={"size": 9})

plt.title('Matriz de Correlaci√≥n', fontsize=20, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()
plt.show()

"""Despu√©s de haber representado visualmente la matriz de correlaciones con un `heatmap`, hemos decidido complementar el an√°lisis con una visi√≥n m√°s enfocada y num√©rica de las relaciones entre la variable objetivo con la transformaci√≥n deseada (`sqrt_bu`) y el resto de caracter√≠sticas del conjunto de datos.

Dicha transformaci√≥n fue seleccionada previamente como la m√°s adecuada para representar los niveles de urea en sangre debido a su comportamiento m√°s estable, normalizado y menos influenciado por valores extremos.

Centrarnos en las correlaciones con la varible:
*   Nos permite detectar relaciones lineales fuertes, tanto positivas como negativas, con otras variables.
*   Sirve como criterio inicial para la selecci√≥n de variables para nuestro modelo de regresi√≥n.
*   Nos ayuda a filtrar variables irrelevantes o redundantes que no aportan valor predictivo.
*   Es √∫til para entender el comportamiento cl√≠nico de la urea en sangre respecto a otras caracter√≠sticas del paciente.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Eliminar variables redundantes y autocorrelaciones
columnas_excluir = ['sqrt_bu', 'log_bu', 'bu', 'win_bu', 'sqrt_sc', 'win_sc', 'sc', 'win_pcv', 'pcv', 'sqrt_pcv',
                    'hemo', 'win_hemo','log_hemo', 'log_rc', 'sqrt_rc', 'rc', 'al', 'win_al', 'log_sg', 'sqrt_sg',
                    'sg','log_age', 'win_age', 'sqrt_age', 'sod', 'sqrt_sod', 'su', 'win_su', 'log_sod', 'sqrt_sod',
                    'pot','win_pot','sqrt_pot','sqrt_bp', 'bp', 'log_al','win_bgr', 'wc', 'sqrt_wc', 'log_wc',
                    'bgr','sqrt_bgr', 'log_bp', 'log_su']

bu_correlations = corr_matrix['sqrt_bu'].drop(labels=columnas_excluir, errors='ignore').sort_values()

# Crear paleta de colores seg√∫n signo
colors = bu_correlations.apply(lambda x: 'steelblue' if x > 0 else 'indianred')

# Plot
plt.figure(figsize=(10, 8))
bars = plt.bar(bu_correlations.index, bu_correlations.values, color=colors)

# A√±adir etiquetas de valor
for bar in bars:
    height = bar.get_height()
    if abs(height) > 0.05:
        plt.text(bar.get_x() + bar.get_width()/2, height + 0.015*np.sign(height),
                 f'{height:.2f}', ha='center', va='bottom' if height > 0 else 'top', fontsize=9)

plt.title('Correlaciones de caracter√≠sticas con "sqrt_bu"', fontsize=18, fontweight='bold', pad=20)
plt.axhline(0, color='gray', linewidth=1.2, linestyle='--')
plt.ylabel('Coeficiente de correlaci√≥n', labelpad=10)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.show()

# Mostrar correlaciones absolutas m√°s fuertes
abs_correlations = bu_correlations.abs().sort_values(ascending=False)
top_10_correlated = abs_correlations.head(15)

print('\nüîπ Top 15 caracter√≠sticas m√°s correlacionadas con "sqrt_bu":')
for feature, corr_value in zip(top_10_correlated.index, bu_correlations[top_10_correlated.index]):
    print(f"{feature}: {corr_value:.4f}")

"""### 4.3. Codificaci√≥n final de variables categ√≥ricas booleanas

Una vez finalizado el proceso de transformaci√≥n de variables num√©ricas y la creaci√≥n de variables categ√≥ricas mediante codificaci√≥n one-hot, es fundamental asegurarnos de que todas las variables del dataset est√©n correctamente representadas en un formato num√©rico compatible.

En esta secci√≥n, nos centraremos en detectar y transformar autom√°ticamente aquellas variables categ√≥ricas o booleanas que, aunque ya est√©n en formato de texto o bool, representan valores l√≥gicos (`'True'/'False'`, `True/False`) y que pueden ser f√°cilmente codificados como valores binarios.

Mapeamos 0 como `False` y 1 como `True`. Esta transformaci√≥n:
*   Reduce el riesgo de errores durante la modelizaci√≥n.
*   Asegura que todas las variables puedan ser interpretadas num√©ricamente.
*   Mantiene la informaci√≥n de las variables l√≥gicas de manera compacta y eficiente.
"""

def encode_categorical_variables(df):
    df_encoded = df.copy()
    # Convierte todas las cadenas booleanas o "Verdadero"/"Falso" a 0/1
    for col in df_encoded.select_dtypes(include=['object', 'bool', 'category']).columns:
        unique_vals = df_encoded[col].dropna().unique()
        if set(unique_vals).issubset({'True', 'False', True, False}):
            df_encoded[col] = df_encoded[col].astype(str).map({'False': 0, 'True': 1})
    return df_encoded

encoded_kidney_df = encode_categorical_variables(combined_kidney_df)

"""Para asegurarnos de que la codificaci√≥n ha sido exitosa y que no se ha perdido informaci√≥n, vamos a realizar una serie de exploraciones b√°sicas.

Lo primero que haremos es hacer una vista preliminar del DataFrame, esto nos permite:
*   Confirmar que las nuevas columnas booleanas ahora contienen exclusivamente valores 0 y 1.
*   Observar si los nombres de las variables se han mantenido correctamente.
*   Detectar r√°pidamente posibles errores de codificaci√≥n.
"""

encoded_kidney_df.head(10)

"""Seguidamente observaremos sus estad√≠sticas generales, que nos aporta informaci√≥n esencial como:
*   N√∫mero de valores no nulos por variable.
*   El rango de valores (min, max) en cada columna.
*   La media, desviaci√≥n est√°ndar y cuartiles, que nos ayudan a identificar columnas con poca variabilidad o valores extremos.
"""

encoded_kidney_df.describe()

"""Finalmente, lo que haremos es trabajar con una de las variables categ√≥ricas m√°s importantes del conjunto de datos, `classification_ckd`. Esta columna representa la presencia o ausencia de enfermedad renal cr√≥nica (Chronic Kidney Disease, CKD).

El siguiente c√≥digo nos permite observar si la variable se encuentra correctamente representada en formato binario (0 y 1), sin errores de codificaci√≥n.
"""

encoded_kidney_df['classification_ckd']

"""#### Relaci√≥n entre variable target `sqrt_bu` y la variable categ√≥rica `classifcation_ckd`

Para profundizar en la relaci√≥n entre los niveles de urea en sangre y la presencia de enfermedad renal cr√≥nica (ERC), hemos decido relaizar un an√°lisis visual utilizando la versi√≥n transformada de la variable objetivo (`sqrt_bu`).

A continuaci√≥n, se representa la distribuci√≥n de `sqrt_bu` para dos grupos de pacientes: aquellos con diagn√≥stico de ERC (`classification_ckd = 1`) y aquellos sin la enfermedad (`classification_ckd = 0`). Esta visualizaci√≥n permite evaluar si existen diferencias significativas en los niveles de urea entre ambos grupos.
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(encoded_kidney_df[encoded_kidney_df['classification_ckd'] == 1]['sqrt_bu'], label='CKD = 1', fill=True,  color='#38b000')
sns.kdeplot(encoded_kidney_df[encoded_kidney_df['classification_ckd'] == 0]['sqrt_bu'], label='CKD = 0', fill=True,  color='#9d4edd')

plt.xlabel('sqrt_bu',labelpad=11)
plt.ylabel('Densidad', labelpad=11)
plt.title('Distribuci√≥n de "sqrt_bu" por estado de ERC', pad=14)
plt.legend()
plt.xlim(0)
plt.show()

"""Alguna observaciones clave del gr√°fico que hemos obtenido son:
*   Desplazamiento de las distribuciones: la distribuci√≥n correspondiente al grupo sin ERC (CKD = 0) se concentra principalmente en valores de `sqrt_bu` entre aproximadamente 4.5 y 8, con un claro pico alrededor de 6.5. En contraste, la distribuci√≥n de pacientes con ERC (CKD = 1) est√° desplazada hacia la derecha, mostrando una mayor dispersi√≥n y una cola alargada que se extiende hasta valores cercanos a 18. Esto indica niveles significativamente m√°s elevados de urea transformada en este grupo.
*   Superposici√≥n parcial: aunque existe una zona de superposici√≥n entre ambas curvas (principalmente entre valores de `sqrt_bu` de 5 a 8), el grupo con ERC tiende a presentar valores m√°s elevados de manera consistente, lo que sugiere una clara diferencia entre los dos grupos.
*   Simetr√≠a y forma: la distribuci√≥n del grupo sin ERC parece relativamente sim√©trica, mientras que la del grupo con ERC presenta una asimetr√≠a positiva marcada (cola derecha), lo que es coherente con los casos cl√≠nicos de insuficiencia renal, donde los niveles de urea pueden aumentar considerablemente.

Este an√°lisis confirma que la variable `sqrt_bu` posee un alto valor discriminativo en relaci√≥n con el diagn√≥stico de ERC. Sin embargo, la presencia de pacientes enfermos en todo el espectro de valores refuerza la necesidad de utilizar este biomarcador en combinaci√≥n con otros indicadores para una evaluaci√≥n m√°s precisa.

## 5. Regresi√≥n linear

### 5.1. An√°lisis de regresi√≥n con variables num√©ricas

Como primer acercamiento al modelado predictivo, hemos optado por construir un modelo de regresi√≥n lineal simple empleando exclusivamente variables num√©ricas transformadas como predictores. Esta elecci√≥n responde a varios objetivos:
*   Evaluar la relaci√≥n lineal directa entre las variables cuantitativas y la variable dependiente (`sqrt_bu`), sin a√±adir la complejidad del tratamiento de variables categ√≥ricas.
*   Establecer una l√≠nea base de rendimiento que permita comparar modelos m√°s complejos en fases posteriores del an√°lisis.
*   Explorar el comportamiento del modelo desde el punto de vista estad√≠stico: coeficientes, residuos, distribuci√≥n de errores, normalidad, entre otros.

Para la preparaci√≥n del modelo hemos seleccionaron un conjunto de predictores num√©ricos transformados, los cuales hab√≠an mostrado correlaci√≥n significativa con la variable objetivo en los an√°lisis anteriores. Hemos elegido `sqrt_bu` como variable objetivo y divido los datos en conjunto de entrenamiento (80%) y prueba (20%).

El modelo fue entrenado usando `LinearRegression` de `Scikit-Learn`, y adem√°s se utiliz√≥ `statsmodels` para evaluar la significaci√≥n estad√≠stica de los coeficientes.

El an√°lisis posterior al entrenamiento incluy√≥ una evaluaci√≥n exhaustiva a trav√©s de diversos gr√°ficos y m√©tricas:
*   Gr√°fico de valores reales vs. predichos, para evaluar visualmente el ajuste del modelo.
*   Distribuci√≥n de los residuales, para analizar su simetr√≠a y cercan√≠a a una distribuci√≥n normal.
*   Gr√°fico de residuales vs. predichos, que permite identificar patrones que podr√≠an indicar relaciones no lineales no capturadas por el modelo.
*   Q-Q Plot, que eval√∫a si los errores se distribuyen de forma aproximadamente normal.
*   Gr√°fico de coeficientes, que muestra la influencia (positiva o negativa) de cada predictor en la variable objetivo.
*   Relaci√≥n entre residuales y las dos variables m√°s influyentes, que ayuda a detectar efectos no lineales o heterocedasticidad.
*   Tabla resumen de m√©tricas, que incluye:
    *   Error cuadr√°tico medio (MSE).
    *   Ra√≠z del error cuadr√°tico medio (RMSE).
    *   Coeficiente de determinaci√≥n R¬≤.
    *   Tama√±o de muestra de prueba.

Los predictores han sido seleccionados en funci√≥n de su nivel de correlaci√≥n con la variable objetivo `sqrt_bu`, as√≠ como por tener un valor de p menor a 0.05
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec
from scipy import stats

numeric_predictors = ['log_sc',  'sqrt_al', 'log_pot', 'sqrt_hemo']


df_model = encoded_kidney_df.copy()
X = df_model[numeric_predictors]
y = df_model["sqrt_bu"]

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresi√≥n lineal
model = LinearRegression()
model.fit(X_train, y_train)

# Hacer predicciones
y_pred = model.predict(X_test)
y_train_pred = model.predict(X_train)

# Evaluar el modelo
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Coeficientes del modelo
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
})
coefficients = coefficients.sort_values('Coefficient', ascending=False)

# Significaci√≥n estad√≠stica con statsmodels
X_train_sm = sm.add_constant(X_train)
sm_model = sm.OLS(y_train, X_train_sm).fit()

# Calcular residuos
residuals = y_test - y_pred
residuals_train = y_train - y_train_pred

# Establecer el estilo para todos los gr√°ficos
plt.style.use('seaborn-v0_8-whitegrid')

# Create figure with GridSpec for control over subplot layout
fig = plt.figure(figsize=(16, 16))
gs = GridSpec(3, 3)

# 1. Plot real vs. predicha
ax1 = plt.subplot(gs[0, :2])
scatter = ax1.scatter(y_test, y_pred, alpha=0.7, c=residuals, cmap='viridis', s=70)
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
ax1.set_xlabel('"sqrt_bu" actual', fontsize=12, labelpad=12)
ax1.set_ylabel('"sqrt_bu" previsto', fontsize=12, labelpad=12)
ax1.set_title('Valores reales vs. valores previstos', fontsize=16, fontweight='bold', pad=15)

# Agregar etiqueta de l√≠nea de predicci√≥n perfecta
ax1.text(min_val + 0.05 * (max_val - min_val),
         min_val + 0.1 * (max_val - min_val),
         'Predicci√≥n perfecta',
         color='red', fontsize=10, rotation=30)

# A√±adir anotaci√≥n R¬≤ con borde negro
ax1.text(min_val + 0.05 * (max_val - min_val),
         min_val + 0.9 * (max_val - min_val),
         f'R¬≤ = {r2:.4f}\nRMSE = {rmse:.4f}',
         bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5', edgecolor='black'),
         fontsize=12)

# Agregar barra de colores para mostrar la magnitud residual
cbar = plt.colorbar(scatter, ax=ax1)
cbar.set_label('Valor residual', rotation=270, labelpad=20)

# 2. Distribuci√≥n de residuales
ax2 = plt.subplot(gs[0, 2])
sns.histplot(residuals, kde=True, color='#34a0a4', ax=ax2, bins=20, label='Distribuci√≥n de residuales')
ax2.axvline(x=0, color='r', linestyle='--', linewidth=2, label='L√≠nea de residual cero')

# A√±adir curva de ajuste normal
x = np.linspace(min(residuals), max(residuals), 100)
mu, std = stats.norm.fit(residuals)
p = stats.norm.pdf(x, mu, std)
ax2.plot(x, p * (len(residuals) * (max(residuals) - min(residuals)) / 20),
         'k--', linewidth=2, label=f'Curva normal: Œº={mu:.2f}, œÉ={std:.2f}')

ax2.set_title('Distribuci√≥n de residuales', fontsize=16, fontweight='bold', pad=15)
ax2.set_xlabel('Valor residual', fontsize=12, labelpad=12)
ax2.legend(loc='upper left', fontsize=11)

# 3. Residuales vs predichos
ax3 = plt.subplot(gs[1, 0])
ax3.scatter(y_pred, residuals, alpha=0.7, color='#9d4edd', s=70)
ax3.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax3.set_xlabel('Valores predichos', fontsize=12, labelpad=12)
ax3.set_ylabel('Residuales', fontsize=12, labelpad=12)
ax3.set_title('Residuales vs. valores predichos', fontsize=16, fontweight='bold', pad=15)

# Agregar una l√≠nea de tendencia suavizada para detectar patrones
sns.regplot(x=y_pred, y=residuals, scatter=False,
            lowess=True, line_kws={'color': 'black', 'lw': 2}, ax=ax3)

# 4. Q-Q Plot
ax4 = plt.subplot(gs[1, 1])
qq = stats.probplot(residuals, dist="norm", plot=ax4)
ax4.set_title('Q-Q Plot (prueba de normalidad)', fontsize=16, fontweight='bold', pad=15)
ax4.get_lines()[0].set_markerfacecolor('#219ebc')
ax4.get_lines()[0].set_markersize(7)
ax4.get_lines()[0].set_alpha(0.7)
ax4.get_lines()[1].set_color('#9a031e')
ax4.get_lines()[1].set_linewidth(2)
ax4.set_xlabel('Cuantiles te√≥ricos', fontsize=12, labelpad=12)
ax4.set_ylabel('Valores ordenados', fontsize=12, labelpad=12)

# 5. Gr√°fico de coeficientes
ax5 = plt.subplot(gs[1, 2])
df_plot = coefficients.sort_values('Coefficient')

feature_colors = ['#bc4749' if x < 0 else '#6a994e' for x in df_plot['Coefficient']]

coef_plot = sns.barplot(
    x='Coefficient',
    y='Feature',
    data=df_plot,
    ax=ax5
)

for i, bar in enumerate(coef_plot.patches):
    bar.set_facecolor('#bc4749' if df_plot['Coefficient'].iloc[i] < 0 else '#6a994e')

ax5.axvline(x=0, color='k', linestyle='--', linewidth=1)
ax5.set_title('Coeficientes de caracter√≠sticas', fontsize=16, fontweight='bold', pad=15)
ax5.set_xlabel('Valor del coeficiente', fontsize=12, labelpad=12)
ax5.set_ylabel('Caracter√≠stica', fontsize=12, labelpad=12)

# Agregar valores de coeficientes como texto
for i, v in enumerate(df_plot['Coefficient']):
    ax5.text(v + 0.01 if v >= 0 else v - 0.06, i, f"{v:.4f}", va='center')

# 6. Gr√°ficos de residuales frente a caracter√≠sticas
top_features = coefficients.sort_values('Coefficient', ascending=False)['Feature'].iloc[:2].tolist()

colors = ['#90be6d', '#f8961e']

for i, feature in enumerate(top_features):
    ax = plt.subplot(gs[2, i])
    ax.scatter(X_test[feature], residuals, alpha=0.7, color=colors[i], s=70)
    ax.axhline(y=0, color='r', linestyle='--', linewidth=2)
    sns.regplot(x=X_test[feature], y=residuals, scatter=False,
                lowess=True, line_kws={'color': 'black', 'lw': 2}, ax=ax)

    ax.set_xlabel(feature, fontsize=12, labelpad=12)
    ax.set_ylabel('Residuales', fontsize=12, labelpad=12)
    ax.set_title(f'Residuales vs {feature}', fontsize=16, fontweight='bold', pad=15)

# 7. M√©tricas de rendimiento del modelo en una tabla
ax7 = plt.subplot(gs[2, 2])
ax7.axis('off')  # Apagar el eje

# Crear una tabla con m√©tricas de rendimiento del modelo
cell_text = [
    ['Error cuadr√°tico medio (MSE)', f"{mse:.4f}"],
    ['Ra√≠z del error cuadr√°tico medio (RMSE)', f"{rmse:.4f}"],
    ['Puntuaci√≥n R¬≤', f"{r2:.4f}"],
    ['Tama√±o de la muestra (test)', f"{len(y_test)}"]
]

table = ax7.table(
    cellText=cell_text,
    colLabels=['M√©trica', 'Valor'],
    loc='center',
    cellLoc='center',
    colWidths=[0.7, 0.25]
)

table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 2)
for key, cell in table.get_celld().items():
    if key[0] == 0:  # Fila de encabezado
        cell.set_facecolor('#457b9d')
        cell.set_text_props(color='white', fontweight='bold')
    elif key[1] == 0:  # Primera columna
        cell.set_text_props(fontweight='semibold')

ax7.set_title('M√©tricas de rendimiento del modelo', fontsize=16, fontweight='bold', pad=15)

# Ajustes finales de dise√±o
plt.tight_layout()
plt.subplots_adjust(top=0.95, wspace=0.3, hspace=0.4)
fig.suptitle('Evaluaci√≥n del primer modelo de regresi√≥n lineal con variables num√©ricas', fontsize=22, fontweight='bold', y=1.02)

plt.show()

"""A continuaci√≥n, analizaremos en detalle el comportamiento del modelo y sus salidas gr√°ficas, con el fin de evaluar la calidad del ajuste y validar los supuestos de regresi√≥n:

1. Relaci√≥n entre valores reales y predichos: el gr√°fico de "Valores reales vs. predichos" nos permite visualizar el grado de ajuste del modelo. Cada punto representa una observaci√≥n del conjunto de prueba, donde el eje X indica el valor real de la variable objetivo (`sqrt_bu`) y el eje Y su valor predicho por el modelo.
    *   La l√≠nea punteada diagonal representa el escenario de predicci√≥n perfecta, es decir, donde predicci√≥n = valor real.
    *   Los puntos est√°n coloreados seg√∫n el valor de su residuo (error), lo que proporciona una dimensi√≥n adicional sobre la magnitud del error en cada predicci√≥n.
    *   El coeficiente de determinaci√≥n R¬≤ = 0.6055: indica que aproximadamente el 60.55% de la variabilidad en la variable dependiente (`sqrt_bu`) se explica por el modelo. Es una puntuaci√≥n intermedia, lo que sugiere un ajuste moderado.
    *   El RMSE = 1.894 indica que, en promedio, las predicciones del modelo se desv√≠an alrededor de 1.89 unidades del valor real (en escala de ra√≠z cuadrada de urea en sangre).

 Esto sugiere un desempe√±o razonable para un modelo lineal simple, aunque hay margen de mejora al incorporar no linealidades o nuevas variables.

2. Distribuci√≥n de los residuos: el histograma de residuos muestra c√≥mo se distribuyen los errores cometidos por el modelo.
    *   La distribuci√≥n est√° centrada en torno a cero, lo cual es deseable en modelos de regresi√≥n, ya que sugiere que no hay sesgo sistem√°tico en las predicciones.
    *   La forma del histograma es aproximadamente normal, aunque con una leve asimetr√≠a hacia la izquierda.
    *   Se ha superpuesto una curva de densidad estimada junto a una curva de distribuci√≥n normal te√≥rica (ajustada al residuo), lo que permite comparar visualmente la normalidad.

 El comportamiento observado respalda en gran medida el supuesto de normalidad de los errores, aunque las peque√±as desviaciones podr√≠an indicar la necesidad de ajustes en futuros modelos.

3. Residuos vs. valores predichos: este gr√°fico ayuda a evaluar la homocedasticidad, es decir, que la varianza de los errores sea constante en todo el rango de predicciones.
    *   Idealmente, los puntos deber√≠an distribuirse de forma aleatoria y uniforme alrededor de la l√≠nea horizontal en cero. En nuestro caso, la mayor√≠a de los residuos se agrupan en torno a cero, lo cual es positivo.
    *   Sin embargo, se aprecian ciertos patrones de dispersi√≥n y una leve variaci√≥n en la amplitud de los errores a lo largo del eje horizontal. Esto podr√≠a indicar:
          *   Alg√∫n tipo de no linealidad que no est√° siendo capturada por el modelo.
          *   Presencia de valores at√≠picos que afectan la homogeneidad del error.

    Una posible mejora en este aspecto ser√≠a introducir transformaciones adicionales o modelos m√°s flexibles.

4. Q-Q Plot (prueba de normalidad de residuos): permite comparar la distribuci√≥n emp√≠rica de los residuos con la distribuci√≥n normal te√≥rica. Si los errores siguen una distribuci√≥n normal, los puntos deber√≠an alinearse sobre la l√≠nea roja diagonal.
    *   En general, los residuos del modelo se ajustan bien a esta l√≠nea, lo que refuerza el supuesto de normalidad.
    *   No obstante, se observan leves desviaciones en los extremos (colas), lo que podr√≠a indicar cierta presencia de valores extremos (outliers) o que los residuos no siguen exactamente una normal.

Como complemento al an√°lisis gr√°fico, hemos implementado un Modelo de Regresi√≥n Lineal por M√≠nimos Cuadrados Ordinarios (OLS) utilizando la librer√≠a `statsmodels`. Esta metodolog√≠a permite no solo estimar los coeficientes del modelo, sino tambi√©n realizar un an√°lisis estad√≠stico m√°s profundo de cada variable predictora.

Este tipo de modelo es especialmente √∫til como modelo base o de referencia, ya que nos proporciona m√©tricas clave como:
*   Errores est√°ndar de los coeficientes.
*   Valores p para evaluar la significancia estad√≠stica.
*   Intervalos de confianza para cada par√°metro.
*   M√©tricas globales del modelo como el R¬≤ ajustado, el estad√≠stico F y diversas pruebas de normalidad y homocedasticidad de residuos.

Adem√°s, su interpretaci√≥n clara facilita la comprensi√≥n de las relaciones entre las variables, lo cual es esencial antes de introducir modelos m√°s complejos o no lineales.

Utilizamos el siguiente c√≥digo para generar el resumen estad√≠stico completo del modelo, el cual incluir√° una tabla detallada con:
*   Los coeficientes estimados para cada variable (incluyendo la constante).
*   Su error est√°ndar asociado.
*   El estad√≠stico t y su p-valor correspondiente, que permiten identificar qu√© variables tienen un efecto significativo sobre `sqrt_bu`.
"""

print(sm_model.summary())

"""El siguiente paso en nuestro an√°lisis fue ajustar un modelo de regresi√≥n lineal mediante M√≠nimos Cuadrados Ordinarios (OLS) con el objetivo de explicar la variable dependiente `sqrt_bu`, a partir de un conjunto de cuatro variables predictoras. El modelo se entren√≥ sobre una muestra de 320 observaciones.

**Estad√≠sticas globales del modelo:**

*   R¬≤ = 0.636: el modelo logra explicar aproximadamente el 63.6% de la variabilidad observada en la variable dependiente. Este valor indica un ajuste moderadamente bueno, adecuado para un modelo lineal con pocas variables.
*   R¬≤ ajustado = 0.631: al corregir por el n√∫mero de predictores, este valor cercano al R¬≤ original confirma que las variables incluidas aportan valor real y no se produce sobreajuste.
*   F-statistic = 137.5 (p < 0.001): la prueba F indica que el modelo en su conjunto es estad√≠sticamente significativo, lo cual respalda la relevancia global de los predictores seleccionados.
*   AIC = 1203 y BIC = 1222: estas m√©tricas de penalizaci√≥n por complejidad permiten comparar modelos entre s√≠. En este caso, los valores relativamente bajos sugieren una buena relaci√≥n entre ajuste y simplicidad.

**Coeficientes e interpretaci√≥n de predictores:**

De los 4 predictores evaluados, algunos mostraron una asociaci√≥n estad√≠sticamente significativa con `sqrt_bu`, mientras que otros no lograron superar los umbrales cl√°sicos de significancia (p < 0.05):

*   `log_sc` (logaritmo de creatinina s√©rica):
    *   Coeficiente: 2.7317
    *   p < 0.001
    *   Es el predictor m√°s relevante del modelo, con un fuerte impacto positivo. Una unidad de aumento en `log_sc` se asocia, en promedio, con un incremento de 2.73 unidades en `sqrt_bu`, manteniendo las dem√°s variables constantes.
*   `sqrt_al` (ra√≠z cuadrada de alb√∫mina):
    *   Coeficiente: 0.2202
    *   p = 0.145
    *   Aunque el coeficiente es positivo, no alcanza significancia estad√≠stica. No obstante, su inclusi√≥n puede justificarse por razones cl√≠nicas o por su relevancia en modelos alternativos.
*   `log_pot` (logaritmo de potasio):
    *   Coeficiente: 0.9602
    *   p = 0.001
    *   Estad√≠sticamente significativo, sugiere una relaci√≥n positiva entre el nivel de potasio y la urea en sangre, posiblemente debido a alteraciones renales que afectan el equilibrio electrol√≠tico.
*   `sqrt_hemo` (ra√≠z cuadrada de hemoglobina):
    *   Coeficiente: -0.8540
    *   p = 0.002
    *   Presenta una asociaci√≥n negativa significativa. Este resultado sugiere que niveles m√°s altos de hemoglobina tienden a estar asociados con valores m√°s bajos de `sqrt_bu`, lo que resulta cl√≠nicamente coherente si se considera que pacientes con disfunci√≥n renal suelen tener anemia.

**Diagn√≥stico de los residuos:**

El diagn√≥stico de residuos permite evaluar la validez de los supuestos del modelo lineal (normalidad, homocedasticidad e independencia).

1. Normalidad:
    *   Omnibus test (p < 0.001) y Jarque-Bera = 117.3 (p < 0.00001): ambas pruebas indican que los residuos no siguen una distribuci√≥n normal.
    *   Skew = -0.524: se√±ala una ligera asimetr√≠a negativa.
    *   Kurtosis = 5.775: indica colas m√°s pesadas de lo esperado (leptocurtosis), posiblemente relacionadas con la presencia de outliers.

2. Independencia de errores:
    *   Durbin-Watson = 1.958: muy cercano a 2, lo que sugiere ausencia de autocorrelaci√≥n en los errores.

3. Multicolinealidad:
    *   Condition Number = 64.1: aunque no excesivamente alto, es un valor a vigilar, ya que podr√≠a indicar cierta correlaci√≥n entre las variables predictoras.

**Conclusi√≥n general:**

Este modelo OLS proporciona una base s√≥lida para entender la relaci√≥n entre ciertos indicadores cl√≠nicos y los niveles de urea en sangre. En particular:
*   Ofrece una capacidad explicativa razonable (R¬≤ = 0.636).
*   Identifica `log_sc` como el factor predictivo m√°s influyente.
*   Otros predictores, como `log_potv` y `sqrt_hemo`, tambi√©n resultan relevantes y estad√≠sticamente significativos.
*   Pese a ciertos desv√≠os en la normalidad de los residuos, el modelo cumple con la mayor√≠a de los supuestos de la regresi√≥n lineal.

### 5.2. Modelo de regresi√≥n lineal m√∫ltiple con variables categ√≥ricas

En esta secci√≥n nos centraremos en an√°lisis se centra en implementar un modelo de regresi√≥n lineal m√∫ltiple, utilizando variables tanto num√©ricas como categ√≥ricas.

Nuestro objetivo es evaluar la capacidad predictiva de un conjunto seleccionado de variables cl√≠nicas sobre los valores de urea, proporcionando interpretabilidad, verificaci√≥n de supuestos estad√≠sticos, y m√©tricas de rendimiento que permitan validar la utilidad del modelo en la pr√°ctica cl√≠nica.

La variables num√©ricas se han escogido en base a los resultados obtenidos con la previa exploraci√≥n, la qual incluye:
*   An√°lisis de correlaci√≥n entre predictores.
*   C√°lculo del Factor de Inflaci√≥n de la Varianza (VIF).
*   Significancia estad√≠stica (valores p) dentro de un modelo preliminar.

Adem√°s de las variables num√©ricas depuradas, esta vez integraremos al modelo diversas variables categ√≥ricas binarias, previamente codificadas como 0 o 1, que capturan la presencia o ausencia de ciertas condiciones cl√≠nicas relevantes.

Utilizando el siguinete c√≥digo, una vez entrenado el modelo con `sklearn`, se procede a:
*   Calcular m√©tricas clave de rendimiento: MSE, RMSE, MAE, R¬≤.
*   Visualizar la calidad del ajuste mediante gr√°ficos:
    *   Real vs. Predicho.
    *   Distribuci√≥n y comportamiento de los residuales.
    *   Q-Q Plot para verificar normalidad de errores.
    *   Importancia de los coeficientes.
*   Analizar la relaci√≥n entre las caracter√≠sticas m√°s influyentes y la variable objetivo mediante gr√°ficos individuales.

Tambi√©n realizamos un segundo ajuste con `statsmodels`, que proporciona un resumen estad√≠stico detallado del modelo (coeficientes, errores est√°ndar, intervalos de confianza y p-valores).
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Dividir datos y manejar variables categ√≥ricas
train_df, test_df = train_test_split(encoded_kidney_df, test_size=0.2, random_state=42)

# Definir predictores
numeric_predictors_2 = ['log_sc',  'sqrt_al', 'log_pot', 'sqrt_hemo']
categorical_predictors_2 = ['ane_no',  'pc_abnormal',  'classification_ckd']

# Combine todos los predictores para sklearn
X_cols = numeric_predictors_2 + categorical_predictors_2

# Preparar datos para el modelo de sklearn
X_train = train_df[X_cols]
y_train= train_df['sqrt_bu']
X_test = test_df[X_cols]
y_test = test_df['sqrt_bu']

# Entrenar el modelo de regresi√≥n lineal
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calcular residuales
residuals = y_test- y_pred

# Calcular diversas m√©tricas de rendimiento
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Configura un estilo visual consistente con una paleta de colores
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'figure.figsize': (12, 8),
    'figure.dpi': 100,
    'axes.grid': True,
    'grid.alpha': 0.3
})

# Definir una paleta de colores cohesiva
colors = {
    'primary': '#1f77b4',
    'secondary': '#ff7f0e',
    'tertiary': '#2ca02c',
    'negative': '#d62728',
    'neutral': '#7f7f7f',
    'background': '#f9f9f9',
    'grid': '#e0e0e0'
}

# Crear un panel con un aspecto limpio
fig = plt.figure(figsize=(14, 12), facecolor=colors['background'])
fig.suptitle('An√°lisis de regresi√≥n lineal de la enfermedad renal',
             fontsize=18, fontweight='bold', y=1, color='#000000')

# A√±adir un subt√≠tulo ligero con contexto adicional
plt.figtext(0.5, 0.96, 'Predicci√≥n de los valores de urea en sangre mediante indicadores cl√≠nicos',
            ha='center', fontsize=12, color='#000000')

# Crear una especificaci√≥n de cuadr√≠cula personalizada para un mejor dise√±o
gs = fig.add_gridspec(3, 6)

# Panel 1: Gr√°fico real vs. predicho
ax1 = fig.add_subplot(gs[0, :3])
ax1.set_facecolor(colors['background'])
scatter = ax1.scatter(y_test, y_pred, alpha=0.7, s=50,
                     c=colors['primary'], edgecolor='white', linewidth=0.5)
ax1.plot([min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())],
         [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())],
         '--', color=colors['neutral'], linewidth=1.5, label='Predicci√≥n perfecta')
ax1.set_xlabel('Valor real de urea en sangre (transformado en ra√≠z cuadrada)', labelpad=12)
ax1.set_ylabel('Valor previsto de urea en sangre', labelpad=12)
ax1.set_title('Valores reales vs. valores previstos', pad=15, fontweight='bold')
ax1.legend(frameon=True, framealpha=0.9, loc='lower right')

# Cree un cuadro de texto con m√©tricas del modelo
metrics_text = f"M√©tricas de rendimiento:\nMSE: {mse:.2f}\nRMSE: {rmse:.2f}\nMAE: {mae:.2f}\nR¬≤: {r2:.4f}"
props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=colors['neutral'])
ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, fontsize=11,
         verticalalignment='top', bbox=props)

# Panel 2: Residuales vs. Valores Predichos
ax2 = fig.add_subplot(gs[0, 3:])
ax2.set_facecolor(colors['background'])
ax2.scatter(y_pred, residuals, alpha=0.7, s=50,
           c=colors['secondary'], edgecolor='white', linewidth=0.5)
ax2.axhline(y=0, color=colors['neutral'], linestyle='-', linewidth=1.5)
ax2.set_xlabel('Valores predichos', labelpad=12)
ax2.set_ylabel('Residuales', labelpad=12)
ax2.set_title('Residuales vs. valores predichos', pad=15, fontweight='bold')

# Agregar una banda horizontal para mostrar la varianza residual
std_resid = np.std(residuals)
ax2.axhspan(-2*std_resid, 2*std_resid, alpha=0.1, color=colors['primary'])
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)
ax2.text(0.05, 0.05, f"¬±2œÉ: {2*std_resid:.2f}", transform=ax2.transAxes, fontsize=10,
         verticalalignment='bottom', bbox=props)

# Panel 3: Distribuci√≥n de residuales
ax3 = fig.add_subplot(gs[1, :3])
ax3.set_facecolor(colors['background'])
sns.histplot(residuals, kde=True, ax=ax3, color=colors['tertiary'], alpha=0.7)
ax3.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax3.set_xlabel('Valor residual', labelpad=12)
ax3.set_ylabel('Frecuencia', labelpad=12)
ax3.set_title('Distribuci√≥n de residuales', pad=15, fontweight='bold')

# A√±adir anotaciones sobre la distribuci√≥n
mean_resid = np.mean(residuals)
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor=colors['neutral'])
ax3.text(0.95, 0.95, f"Media: {mean_resid:.2f}\nStd Dev: {std_resid:.2f}",
         transform=ax3.transAxes, fontsize=10,
         horizontalalignment='right', verticalalignment='top', bbox=props)

# Panel 4: Q-Q Plot
ax4 = fig.add_subplot(gs[1, 3:])
ax4.set_facecolor(colors['background'])
sm.qqplot(residuals, line='45', fit=True, ax=ax4)
ax4.set_xlabel('Cuantiles te√≥ricos', labelpad=12)
ax4.set_ylabel('Residuos estandarizados', labelpad=12)
for item in ax4.get_children():
    if isinstance(item, plt.Line2D):
        if item.get_linestyle() == 'none':
            item.set_color(colors['primary'])
            item.set_alpha(0.7)
            item.set_markeredgecolor('white')
            item.set_markeredgewidth(0.5)
            item.set_markersize(5)

ax4.set_title('Q-Q Plot de residuales', pad=15, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Panel 5: Importancia de las caracter√≠sticas (coeficientes)
ax5 = fig.add_subplot(gs[2, :])
ax5.set_facecolor(colors['background'])

# Calcular la importancia de las caracter√≠sticas
coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': model.coef_
})
coefficients['Abs_Coefficient'] = coefficients['Coefficient'].abs()
coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)
top_features = coefficients.head(12)  # Mostrar las top 12 caracter√≠sticas

# Crear un gr√°fico de barras de importancia de caracter√≠sticas
bar_colors = [colors['primary'] if c > 0 else colors['negative'] for c in top_features['Coefficient']]
barplot = sns.barplot(x='Coefficient', y='Feature', data=top_features, hue='Feature',
                      palette=dict(zip(top_features['Feature'], bar_colors)),
                      legend=False, ax=ax5)
ax5.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax5.set_title('Coeficientes de caracter√≠sticas principales (impacto en la predicci√≥n)', pad=15, fontweight='bold')
ax5.set_xlabel('Valor del coeficiente', labelpad=12)
ax5.set_ylabel('Caracter√≠stica', labelpad=12)

# Agregar etiquetas de valores a las barras
for i, p in enumerate(barplot.patches):
    width = p.get_width()
    offset = 0.01 * (abs(width) / width) if width != 0 else 0
    x_pos = width + offset
    text_color = 'black'
    ax5.text(x_pos, p.get_y() + p.get_height()/2,
             f'{width:.3f}', ha='left' if width > 0 else 'right',
             va='center', color=text_color, fontweight='bold', fontsize=9)

# A√±adir una leyenda que explique los colores de los coeficientes
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor=colors['primary'], label='Efecto positivo'),
    Patch(facecolor=colors['negative'], label='Efecto negativo')
]
ax5.legend(handles=legend_elements, loc='lower right')

# Ajustar el dise√±o
plt.tight_layout()
plt.subplots_adjust(top=0.9, hspace=0.5, wspace=0.7)
plt.show()

print("\n")

# Crear un panel
plt.figure(figsize=(14, 10), facecolor=colors['background'])
plt.suptitle('Relaciones de caracter√≠sticas con la urea en sangre', fontsize=18, fontweight='bold', color='#000000')
plt.figtext(0.5, 0.93, 'An√°lisis de los predictores clave y sus relaciones con la variable objetivo',
            ha='center', fontsize=12, color='#000000')

numeric_features = [f for f in coefficients['Feature'] if f in numeric_predictors_2]
top_numeric = numeric_features[:min(4, len(numeric_features))]

# Crear subparcelas para cada caracter√≠stica num√©rica importante
for i, feature in enumerate(top_numeric):
    plt.subplot(2, 2, i+1)
    plt.grid(True, alpha=0.3)

    plt.scatter(X_train[feature], y_train, alpha=0.6, s=40,
                c=colors['primary'], label='Training Data', edgecolor='white', linewidth=0.5)
    plt.scatter(X_test[feature], y_test, alpha=0.6, s=40,
                c=colors['secondary'], label='Test Data', edgecolor='white', linewidth=0.5)

    try:
        # Crear un modelo lineal simple para esta caracter√≠stica
        slope, intercept = np.polyfit(X_train[feature], y_train, 1)
        x_range = np.linspace(X_train[feature].min(), X_train[feature].max(), 100)
        y_pred_line = slope * x_range + intercept

        # Trazar la l√≠nea de tendencia
        plt.plot(x_range, y_pred_line, '-', color=colors['neutral'], linewidth=2)

        # Agregar intervalo de confianza
        residuals = y_train - (slope * X_train[feature] + intercept)
        std_error = np.std(residuals)
        plt.fill_between(x_range, y_pred_line - 1.96*std_error, y_pred_line + 1.96*std_error,
                         alpha=0.2, color=colors['neutral'], label='95% Confidence')

        # A√±adir coeficiente de correlaci√≥n
        corr = np.corrcoef(X_train[feature], y_train)[0, 1]
        # Obtener el valor del coeficiente para esta caracter√≠stica
        coef_value = coefficients.loc[coefficients['Feature'] == feature, 'Coefficient'].values[0]

        # Agregar un cuadro de texto con estad√≠sticas
        stats_text = f"Correlaci√≥n: {corr:.3f}\nCoeficiente: {coef_value:.3f}"
        props = dict(boxstyle="round,pad=0.3", fc="white", alpha=0.8, ec=colors['neutral'])
        plt.annotate(stats_text, xy=(0.05, 0.95), xycoords='axes fraction',
                    fontsize=10, bbox=props, verticalalignment='top')

    except Exception as e:
        print(f"Error al crear la l√≠nea de tendencia para {feature}: {e}")

    feature_label_map = {
        'log_sc': 'log_sc',
        'log_pcv': 'log_pcv',
        'win_sg': 'win_sg',
        'sqrt_al': 'sqrt_al',
        'pot': 'pot',
        'win_sod': 'win_sod',
        'age': 'age'
    }

    feature_label = feature_label_map.get(feature, feature)
    plt.xlabel(feature_label, labelpad=12)
    plt.ylabel('Urea en sangre (transformada en sqrt)', labelpad=12)
    plt.title(f'Relaci√≥n: {feature_label} vs sqrt_bu', pad=15, fontweight='bold')
    plt.legend(frameon=True, framealpha=0.7)

plt.tight_layout()
plt.subplots_adjust(top=0.85, hspace=0.4, wspace=0.25)
plt.show()

"""Una vez entrenado el modelo de regresi√≥n lineal m√∫ltiple que incluye tanto variables num√©ricas como categ√≥ricas codificadas, el siguiente paso es evaluar su rendimiento y analizar la contribuci√≥n individual de las variables predictoras. Para ello, se calcularon diversas m√©tricas de error y se extrajo la lista de las caracter√≠sticas m√°s influyentes en la predicci√≥n de la variable objetivo.

El bloque de c√≥digo a continuaci√≥n imprime un resumen cuantitativo del rendimiento del modelo e informaci√≥n sobre los coeficientes:
"""

print("\nüîπ Resumen del rendimiento del modelo:")
print(f"Error cuadr√°tico medio (MSE): {mse:.4f}")
print(f"Ra√≠z del error cuadr√°tico medio: {rmse:.4f}")
print(f"Error absoluto medio: {mae:.4f}")
print(f"Puntuaci√≥n R¬≤: {r2:.4f}")
print(f"R¬≤ ajustado: {1 - (1-r2)*(len(y_train)-1)/(len(y_train)-len(X_train.columns)-1):.4f}")

print("\nüîπ Top 10 caracter√≠sticas por importancia:")
print(coefficients.head(10).to_string(index=False))

"""Para obtener un an√°lisis estad√≠stico m√°s detallado y comprensible del modelo de regresi√≥n, hemos empleado la librer√≠a `statsmodels` mediante su interfaz basada en f√≥rmulas. Esta API permite trabajar directamente con variables categ√≥ricas en su formato original, sin necesidad de codificarlas manualmente, ya que internamente realiza el tratamiento adecuado (por ejemplo, one-hot encoding).

En el siguiente bloque de c√≥digo, se construye una f√≥rmula de regresi√≥n que incluye tanto predictores num√©ricos como categ√≥ricos. Posteriormente, se ajusta un modelo OLS (m√≠nimos cuadrados ordinarios) sobre el conjunto de entrenamiento, y se imprime un resumen completo.
"""

formula = 'sqrt_bu ~ ' + ' + '.join(numeric_predictors_2 + categorical_predictors_2)
sm_model = smf.ols(formula=formula, data=train_df).fit()
print("\n")
print("üîπ Resumen estad√≠stico detallado:")
print("="*78)
print(sm_model.summary())

"""Una vez ajustado el modelo de regresi√≥n lineal m√∫ltiple utilizando tanto variables num√©ricas como categ√≥ricas originales, procedemos a evaluar su rendimiento y la relevancia de los predictores incluidos.

**Rendimiento del modelo:**

El modelo logra explicar el 66.5% de la varianza observada en la variable dependiente sqrt_bu (R¬≤ = 0.665), mientras que el R¬≤ ajustado se sit√∫a en 0.657, lo que confirma una buena capacidad explicativa incluso tras penalizar por el n√∫mero de predictores. Estas cifras representan un ajuste moderadamente s√≥lido, especialmente en el contexto de datos cl√≠nicos, donde es habitual encontrar cierta variabilidad no explicada debido a la complejidad de los factores fisiopatol√≥gicos.

Adem√°s, el valor de la estad√≠stica F (88.31 con p < 0.001) indica que el conjunto de predictores incluidos en el modelo tiene un efecto global estad√≠sticamente significativo sobre la variable objetivo.

**Relevancia e interpretaci√≥n de las variables predictoras:**

Entre los siete predictores incluidos, destacan los siguientes por su significancia estad√≠stica y magnitud de efecto:
*   `log_sc` (coef. = 2.7232, p < 0.001): es el predictor m√°s relevante y con mayor impacto positivo. Su interpretaci√≥n sugiere que un aumento en los niveles de creatinina s√©rica est√° asociado con un aumento en `sqrt_bu`, lo cual es coherente con el deterioro de la funci√≥n renal.
*   `sqrt_al` (coef. = 0.4430, p = 0.010): muestra una relaci√≥n positiva significativa. Una mayor excreci√≥n de alb√∫mina se asocia con niveles m√°s altos de `sqrt_bu`, lo cual concuerda con el patr√≥n de da√±o renal.
*   `log_pot` (coef. = 0.8247, p = 0.005): tambi√©n muestra un efecto positivo y significativo. Esto sugiere que niveles m√°s altos de potasio en sangre podr√≠an estar relacionados con disfunci√≥n renal, reflejada en un aumento de `sqrt_bu`.
*   `sqrt_hemo` (coef. = -1.0944, p < 0.001): tiene un efecto negativo significativo. Es decir, niveles m√°s bajos de hemoglobina (potencialmente indicativos de anemia por enfermedad renal cr√≥nica) se asocian con valores m√°s altos de urea en sangre.
*   `pc_abnormal` (coef. = 0.5661, p = 0.022): sugiere que la presencia de anomal√≠as en el sedimento urinario se asocia con un aumento de `sqrt_bu`, lo cual es cl√≠nicamente coherente.
*   `classification_ckd` (coef. = -1.1508, p < 0.001): este coeficiente negativo, aunque significativo, resulta contraintuitivo. Indica que, manteniendo el resto de variables constantes, los pacientes sin diagn√≥stico formal de ERC tienden a tener valores m√°s altos de `sqrt_bu`. Esto podr√≠a deberse a factores como diagn√≥sticos tard√≠os, presencia de enfermedad aguda, deshidrataci√≥n u otros procesos no clasificados cl√≠nicamente como ERC pero que impactan la urea en sangre.
*   `ane_no` (coef. = -0.4857, p = 0.084): su efecto es negativo, pero no estad√≠sticamente significativo.

**Consideraciones t√©cnicas:**

Las pruebas de normalidad de residuos (Omnibus y Jarque-Bera, con p < 0.001) sugieren que los errores no se distribuyen normalmente, y la kurtosis elevada (5.511) se√±ala colas pesadas, lo cual podr√≠a deberse a outliers o a una distribuci√≥n sesgada de la variable objetivo.

### 5.3. An√°lisis de multicolinealidad

En esta secci√≥n, realizamos un an√°lisis exhaustivo de multicolinealidad entre las variables independientes del modelo de regresi√≥n lineal. La multicolinealidad se refiere a una fuerte correlaci√≥n entre predictores (dos o m√°s), lo que puede afectar negativamente la estabilidad de las estimaciones, inflar los errores est√°ndar de los coeficientes y dificultar la interpretaci√≥n del modelo.

Para detectar y evaluar este problema, hemos aplicado dos enfoques complementarios:
*   Factor de Inflaci√≥n de la Varianza (VIF): cuantifica cu√°nto se incrementa la varianza de un coeficiente debido a la colinealidad con otros predictores.
*   Matriz de correlaci√≥n: permite identificar relaciones lineales fuertes (en nuestro caso, r > 0.7) entre pares de variables.
"""

import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
import seaborn as sns
from patsy import dmatrices
import warnings

def calculate_vif(X):
    """
    Calcular el factor de inflaci√≥n de varianza para cada caracter√≠stica en un marco de datos.

    -----------
    Par√°metros:
    -----------
    X : matriz de dise√±o que contiene variables independientes.

    --------
    Devuelve:
    --------
    DataFrame con caracter√≠sticas y sus valores VIF correspondientes, ordenados en orden descendente.
    """

    vif_data = pd.DataFrame()
    vif_data["Feature"] = X.columns

    # Manejar posibles problemas num√©ricos al calcular el VIF
    vif_values = []
    for i in range(X.shape[1]):
        try:
            vif = variance_inflation_factor(X.values, i)
            # Comprueba si VIF es extremadamente grande o infinito
            if np.isinf(vif) or vif > 1e10:
                vif_values.append(float('inf'))
                warnings.warn(f"Se detect√≥ multicolinealidad extrema para la caracter√≠stica '{X.columns[i]}'")
            else:
                vif_values.append(vif)
        except Exception as e:
            warnings.warn(f"No se pudo calcular el VIF para la caracter√≠stica '{X.columns[i]}': {str(e)}")
            vif_values.append(np.nan)

    vif_data["VIF"] = vif_values
    return vif_data.sort_values("VIF", ascending=False)

def analyze_multicollinearity(df, target, numeric_features, categorical_features, vif_threshold=5, corr_threshold=0.7,
                             plot=True):
    """
    Analiza la multicolinealidad en un conjunto de datos utilizando VIF y an√°lisis de correlaci√≥n.

    -----------
    Par√°metros:
    -----------
    df : conjunto de datos que contiene todas las caracter√≠sticas.
    target : nombre de la variable de destino.
    numeric_features : lista de nombres de caracter√≠sticas num√©ricas.
    categorical_features : lista de nombres de caracter√≠sticas categ√≥ricas
    vif_threshold : umbral para determinar valores altos de VIF (default=5).
    corr_threshold : umbral para determinar una alta correlaci√≥n entre caracter√≠sticas(default=0.7).
    plot : si se deben mostrar gr√°ficos de valores VIF (default=True).

    --------
    Devuelve:
    --------
    Diccionario que contiene datos VIF, matriz de dise√±o, caracter√≠sticas con alto VIF y pares altamente correlacionados.
    """

    # Comprobar si existen caracter√≠sticas en el marco de datos
    valid_numeric = [col for col in numeric_features if col in df.columns]
    valid_categorical = [col for col in categorical_features if col in df.columns]

    if not valid_numeric + valid_categorical:
        raise ValueError("No valid features found in the dataframe")

    # Crear f√≥rmula para patsy
    formula_parts = ([f"C({cat})" for cat in valid_categorical] + valid_numeric)
    formula = f"{target} ~ " + " + ".join(formula_parts)
    print(f"üîπ F√≥rmula: {formula}\n")

    # Intente crear matrices de dise√±o con patsy
    try:
        y, X = dmatrices(formula, data=df, return_type='dataframe')
        vif_data = calculate_vif(X)
    except Exception as e:
        print(f"Error con la f√≥rmula de Patsy: {e}")
        print("Volviendo al c√°lculo directo de VIF utilizando s√≥lo caracter√≠sticas num√©ricas...")
        if not valid_numeric:
            raise ValueError("Se necesita al menos una caracter√≠stica num√©rica v√°lida para el c√°lculo directo de VIF.")
        X = df[valid_numeric].copy()
        vif_data = calculate_vif(X)

    # Encuentra caracter√≠sticas problem√°ticas
    high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]["Feature"].tolist()

    # An√°lisis de correlaci√≥n
    corr_matrix = X.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    highly_correlated = [(col1, col2, corr_matrix.loc[col1, col2])
                         for col1 in upper_tri.index
                         for col2 in upper_tri.columns
                         if upper_tri.loc[col1, col2] > corr_threshold]

    # Visualizar si se solicita
    if plot:
        # Gr√°fico VIF con intersecci√≥n (si est√° presente)
        has_intercept = "Intercept" in vif_data["Feature"].values

        if has_intercept:
            plt.figure(figsize=(8, 4))
            data_to_plot = vif_data.head(20) if len(vif_data) > 20 else vif_data
            sns.barplot(x="VIF", y="Feature", data=data_to_plot)
            plt.axvline(x=vif_threshold, color='#d55d92', linestyle='--', label=f'Umbral (VIF={vif_threshold})')
            plt.title('An√°lisis VIF (con "Intercept")', fontweight='bold', fontsize=14, pad=15)
            plt.xlabel('Valor VIF', fontsize=12)
            plt.ylabel('Caracter√≠stica', fontsize=12, labelpad=12)
            plt.legend()
            plt.tight_layout()
            plt.show()

            print("\n")

            # Gr√°fico VIF sin intersecci√≥n
            vif_data_no_intercept = vif_data[vif_data["Feature"] != "Intercept"]
            plt.figure(figsize=(8, 4))
            data_to_plot = vif_data_no_intercept.head(20) if len(vif_data_no_intercept) > 20 else vif_data_no_intercept
            sns.barplot(x="VIF", y="Feature", data=data_to_plot)
            plt.axvline(x=vif_threshold, color='#d55d92', linestyle='--', label=f'Umbral (VIF={vif_threshold})')
            plt.title('An√°lisis VIF (sin "Intercept")', fontweight='bold', fontsize=14, pad=15)
            plt.xlabel('Valor VIF', fontsize=12, labelpad=10)
            plt.ylabel('Caracter√≠stica', fontsize=12, labelpad=10)
            plt.legend()
            plt.tight_layout()
            plt.show()
        else:
            # S√≥lo una plot si no hay intersecci√≥n
            plt.figure(figsize=(8, 4))
            data_to_plot = vif_data.head(20) if len(vif_data) > 20 else vif_data
            sns.barplot(x="VIF", y="Feature", data=data_to_plot)
            plt.axvline(x=vif_threshold, color='#d55d92', linestyle='--', label=f'Umbral (VIF={vif_threshold})')
            plt.title('An√°lisis VIF', fontweight='bold', fontsize=14, pad=15)
            plt.xlabel('Valor VIF', fontsize=12, labelpad=10)
            plt.ylabel('Valor VIF', fontsize=12, labelpad=10)
            plt.legend()
            plt.tight_layout()
            plt.show()

    # Imprimir resultados
    print("\nüîπ Resultados del an√°lisis VIF:")
    print(vif_data)

    if high_vif_features:
        print(f"\nüîπ Caracter√≠sticas con alto VIF (>{vif_threshold}):")
        for feature in high_vif_features:
            vif_value = vif_data[vif_data['Feature'] == feature]['VIF'].values[0]
            if np.isinf(vif_value):
                print(f"- {feature}: Multicolinealidad extrema (infinita)")
            else:
                print(f"- {feature}: {vif_value:.2f}")
    else:
        print(f"\nüîπ Sin funciones con VIF > {vif_threshold}")

    if highly_correlated:
        print(f"\nüîπ Pares de caracter√≠sticas altamente correlacionadas (correlaci√≥n > {corr_threshold}):")
        for col1, col2, corr in highly_correlated:
            print(f"- {col1} & {col2}: {corr:.3f}")
    else:
        print(f"\nüîπ No hay pares de caracter√≠sticas con correlaci√≥n > {corr_threshold}")

    return {
        'vif_data': vif_data,
        'design_matrix': X,
        'high_vif_features': high_vif_features,
        'highly_correlated_pairs': highly_correlated
    }

if __name__ == "__main__":
    numeric_features = ['log_sc',  'sqrt_al', 'log_pot', 'sqrt_hemo']
    categorical_features = ['ane_no',  'pc_abnormal', 'classification_ckd']
    target_variable = 'sqrt_bu'

    results = analyze_multicollinearity(
        df=encoded_kidney_df,
        target=target_variable,
        numeric_features=numeric_features,
        categorical_features=categorical_features,
        vif_threshold=5,
        corr_threshold=0.7
    )

"""Resultados obtenidos indican que la √∫nica variable que presenta multicolinealidad alta (VIF > 5) es el `Intercepto`, con un valor extremadamente elevado (259.47). Esto es esperado, ya que el intercepto no es una variable explicativa real, sino una constante, y no suele considerarse al evaluar la multicolinealidad entre predictores.

El resto de las variables presentan VIFs bajos (todos < 3), lo cual indica que no existe una colinealidad problem√°tica entre ellas y que las estimaciones de los coeficientes ser√°n estables y confiables.

Adem√°s, hemos verificado la matriz de correlaci√≥n entre las variables, y no se encontr√≥ ning√∫n par con una correlaci√≥n mayor a 0.7, lo que refuerza la conclusi√≥n anterior.

Por estas razones consideramos que no es necesario eliminar ni transformar variables por colinealidad en este caso, por lo que el modelo puede seguir adelante con esta selecci√≥n de caracter√≠sticas sin ajustes adicionales por este motivo.

### 5.4. M√©todos de regularizaci√≥n

En problemas de regresi√≥n multivariable, es frecuente encontrarse con colinealidad entre predictores, lo que puede provocar estimaciones inestables de los coeficientes y disminuir la capacidad predictiva del modelo. Tal y como se ha visto en el apartado anterior.

Para abordar este problema, se aplican t√©cnicas de regularizaci√≥n y tambi√©n se realiza la comparaci√≥n.

#### 5.4.1. Regresi√≥n Ridge (L2)

En esta secci√≥n, vamos a aplicar regresi√≥n Ridge (tambi√©n conocida como L2 regularization), que introduce una penalizaci√≥n proporcional al cuadrado de la magnitud de los coeficientes. Esta penalizaci√≥n no elimina variables, pero tiende a reducir sus magnitudes, ayudando a mitigar la multicolinealidad y mejorar la generalizaci√≥n del modelo.

El c√≥digo que hemos realizado ejecuta un an√°lisis siguiendo los pasos que tenemos a continuaci√≥n:
1. Estandariza las variables predictoras (paso clave previo a aplicar regularizaci√≥n).
2. Selecciona el hiperpar√°metro √≥ptimo `alpha` mediante b√∫squeda en rejilla (`GridSearchCV`).
3. Ajusta el modelo Ridge con el mejor valor de `alpha`.
4. Compara este modelo con regresi√≥n lineal sin regularizaci√≥n usando m√©tricas como RMSE y R¬≤.
5. Analiza los coeficientes para as√≠ nosotras poder ver qu√© variables son m√°s importantes y c√≥mo se reducen otras.
6. Evalua mediante validaci√≥n cruzada, distribuci√≥n de residuos, Q-Q plot y relaci√≥n entre valores reales y predichos.
"""

import statsmodels.api as sm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from statsmodels.nonparametric.smoothers_lowess import lowess
from scipy import stats


# Escalar los datos (recomendado para Ridge)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Encontrar el mejor alfa para Ridge mediante validaci√≥n cruzada
param_grid = {
    'alpha': np.logspace(-3, 3, 15)  # Amplia gama para alfa
}

ridge_cv = GridSearchCV(
    Ridge(random_state=42),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    return_train_score=True  # A√±adido para obtener las puntuaciones de entrenamiento
)
ridge_cv.fit(X_train_scaled, y_train)

# Obtener el mejor valor de alpha
best_alpha = ridge_cv.best_params_['alpha']
print(f"\nüîπ Mejor valor de alpha: {best_alpha}")

# Mostrar resultados de la validaci√≥n cruzada
cv_results = pd.DataFrame(ridge_cv.cv_results_)

# Verificar qu√© columnas est√°n disponibles antes de intentar acceder a ellas
print("\nüîπ Columnas disponibles en cv_results:")
print(cv_results.columns.tolist())

# Seleccionar las columnas correctas que existen en el DataFrame
cv_results_subset = cv_results[['param_alpha', 'mean_test_score']]
if 'mean_train_score' in cv_results.columns:
    cv_results_subset['mean_train_score'] = cv_results['mean_train_score']
else:
    print("\n‚ö†Ô∏è La columna 'mean_train_score' no est√° disponible")

cv_results_subset.columns = ['alpha', 'mean_test_score'] + (['mean_train_score'] if 'mean_train_score' in cv_results.columns else [])
cv_results_subset['mean_test_score'] = -cv_results_subset['mean_test_score']  # Convertir a MSE positivo
if 'mean_train_score' in cv_results_subset.columns:
    cv_results_subset['mean_train_score'] = -cv_results_subset['mean_train_score']  # Convertir a MSE positivo

print("\nüîπ Resultados de validaci√≥n cruzada para diferentes valores de alpha:")
print(cv_results_subset)

# Realizar validaci√≥n cruzada con el mejor alpha
cv_scores = cross_val_score(
    Ridge(alpha=best_alpha, random_state=42),
    X_train_scaled, y_train, cv=5, scoring='r2', n_jobs=-1
)
print(f"\nüîπ Puntuaciones de validaci√≥n cruzada (R¬≤): {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}")

# Entrenar el modelo Ridge con el mejor valor de alpha
ridge_model = Ridge(alpha=best_alpha, random_state=42)
ridge_model.fit(X_train_scaled, y_train)
y_pred_ridge = ridge_model.predict(X_test_scaled)

# Tambi√©n entrenar el modelo de regresi√≥n lineal para comparaci√≥n
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train)
y_pred_linear = linear_model.predict(X_test_scaled)

# Calcular residuales para ambos modelos
residuals_ridge = y_test - y_pred_ridge
residuals_linear = y_test - y_pred_linear

# Calcular diversas m√©tricas de rendimiento para ambos modelos
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
rmse_ridge = np.sqrt(mse_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)

mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_linear)
mae_linear = mean_absolute_error(y_test, y_pred_linear)

print("\nüîπ Comparaci√≥n de m√©tricas:")
print(f"{'-'*32}")
print(f"{'M√©trica':<10} {'Ridge':<12} {'Linear':<12}")
print(f"{'-'*32}")
print(f"{'MSE':<10} {mse_ridge:<12.4f} {mse_linear:<12.4f}")
print(f"{'RMSE':<10} {rmse_ridge:<12.4f} {rmse_linear:<12.4f}")
print(f"{'R¬≤':<10} {r2_ridge:<12.4f} {r2_linear:<12.4f}")
print(f"{'MAE':<10} {mae_ridge:<12.4f} {mae_linear:<12.4f}")

# Analizar coeficientes
coef_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Ridge_Coef': ridge_model.coef_,
    'Linear_Coef': linear_model.coef_
})

# Ajustar coeficientes a la escala original
for i, col in enumerate(X_train.columns):
    coef_df.loc[coef_df['Feature'] == col, 'Ridge_Coef'] = ridge_model.coef_[i] / scaler.scale_[i]
    coef_df.loc[coef_df['Feature'] == col, 'Linear_Coef'] = linear_model.coef_[i] / scaler.scale_[i]

coef_df['Abs_Ridge_Coef'] = np.abs(coef_df['Ridge_Coef'])

# Comprobaci√≥n de coeficientes peque√±os (por debajo del umbral)
threshold = 0.001
small_coefs = np.sum(np.abs(coef_df['Ridge_Coef']) < threshold)
print("\nüîπ Top 10 caracter√≠sticas por importancia:")
print(coef_df.sort_values('Abs_Ridge_Coef', ascending=False).head(10))
print("\n")

# Configura un estilo visual consistente con una paleta de colores
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'figure.figsize': (12, 8),
    'figure.dpi': 100,
    'axes.grid': True,
    'grid.alpha': 0.3
})

# Definir una paleta de colores cohesiva
colors = {
    'primary': '#1f77b4',
    'secondary': '#ff7f0e',
    'tertiary': '#2ca02c',
    'negative': '#d62728',
    'neutral': '#7f7f7f',
    'background': '#f9f9f9',
    'grid': '#e0e0e0',
    'ridge': '#a7c957',
    'linear': '#184e77'
}

# Crear un panel con un aspecto limpio para comparar Ridge vs Linear
fig = plt.figure(figsize=(14, 12), facecolor=colors['background'])
fig.suptitle('Comparaci√≥n de regresi√≥n Ridge vs regresi√≥n lineal para enfermedad renal',
             fontsize=18, fontweight='bold', y=1, color='#000000')

# A√±adir un subt√≠tulo ligero con contexto adicional
plt.figtext(0.5, 0.96, 'Predicci√≥n de los valores de urea en sangre mediante indicadores cl√≠nicos con regularizaci√≥n',
            ha='center', fontsize=12, color='#000000')

# Crear una especificaci√≥n de cuadr√≠cula personalizada para un mejor dise√±o
gs = fig.add_gridspec(3, 6)

# Panel 1: Gr√°fico real vs. predicho (ambos modelos)
ax1 = fig.add_subplot(gs[0, :3])
ax1.set_facecolor(colors['background'])
scatter_linear = ax1.scatter(y_test, y_pred_linear, alpha=0.6, s=50,
                     c=colors['linear'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Lineal')
scatter_ridge = ax1.scatter(y_test, y_pred_ridge, alpha=0.6, s=50,
                     c=colors['ridge'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Ridge')
min_val = min(y_test.min(), min(y_pred_linear.min(), y_pred_ridge.min()))
max_val = max(y_test.max(), max(y_pred_linear.max(), y_pred_ridge.max()))
ax1.plot([min_val, max_val], [min_val, max_val],
         '--', color=colors['neutral'], linewidth=1.5, label='Predicci√≥n perfecta')
ax1.set_xlabel('Valor real de urea en sangre (transformado en ra√≠z cuadrada)', labelpad=12)
ax1.set_ylabel('Valor previsto de urea en sangre', labelpad=12)
ax1.set_title('Valores reales vs. valores previstos', pad=15, fontweight='bold')
ax1.legend(frameon=True, framealpha=0.9, loc='lower right')

# Cree un cuadro de texto con m√©tricas del modelo
metrics_text = (f"M√©tricas Ridge (Œ±={best_alpha:.4f}):\n"
                f"MSE: {mse_ridge:.2f}            RMSE: {rmse_ridge:.2f}\n"
                f"MAE: {mae_ridge:.2f}            R¬≤: {r2_ridge:.4f}\n\n"
                f"M√©tricas Linear:\n"
                f"MSE: {mse_linear:.2f}           RMSE: {rmse_linear:.2f}\n"
                f"MAE: {mae_linear:.2f}           R¬≤: {r2_linear:.4f}")
props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=colors['neutral'])
ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, fontsize=8,
         verticalalignment='top', bbox=props)

# Panel 2: Residuales vs. Valores Predichos (ambos modelos)
ax2 = fig.add_subplot(gs[0, 3:])
ax2.set_facecolor(colors['background'])
ax2.scatter(y_pred_linear, residuals_linear, alpha=0.6, s=50,
           c=colors['linear'], edgecolor='white', linewidth=0.5, label='Residuales Linear')
ax2.scatter(y_pred_ridge, residuals_ridge, alpha=0.6, s=50,
           c=colors['ridge'], edgecolor='white', linewidth=0.5, label='Residuales Ridge')
ax2.axhline(y=0, color=colors['neutral'], linestyle='-', linewidth=1.5)
ax2.set_xlabel('Valores predichos', labelpad=12)
ax2.set_ylabel('Residuales', labelpad=12)
ax2.set_title('Residuales vs. valores predichos', pad=15, fontweight='bold')

# Agregar una banda horizontal para mostrar la varianza residual
std_resid_ridge = np.std(residuals_ridge)
# A√±adir lowess para la l√≠nea de tendencia Ridge
lowess_y = lowess(residuals_ridge, y_pred_ridge, frac=0.3)
ax2.plot(lowess_y[:, 0], lowess_y[:, 1], color='#5f0f40', linestyle='-', linewidth=2, label='Tendencia Ridge')
ax2.legend(frameon=True, framealpha=0.9, loc='lower right')

ax2.axhspan(-2*std_resid_ridge, 2*std_resid_ridge, alpha=0.1, color=colors['ridge'])
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)
ax2.text(0.05, 0.05, f"¬±2œÉ Ridge: {2*std_resid_ridge:.2f}", transform=ax2.transAxes, fontsize=10,
         verticalalignment='bottom', bbox=props)

# Panel 3: Distribuci√≥n de residuales de ambos modelos
ax3 = fig.add_subplot(gs[1, :3])
ax3.set_facecolor(colors['background'])
sns.histplot(residuals_linear, kde=True, ax=ax3, color=colors['linear'], alpha=0.5, label='Residuales Linear')
sns.histplot(residuals_ridge, kde=True, ax=ax3, color=colors['ridge'], alpha=0.5, label='Residuales Ridge')
ax3.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax3.set_xlabel('Valor residual', labelpad=12)
ax3.set_ylabel('Frecuencia', labelpad=12)
ax3.set_title('Distribuci√≥n de residuales', pad=15, fontweight='bold')

# A√±adir curva de distribuci√≥n normal para comparaci√≥n
x = np.linspace(min(residuals_ridge), max(residuals_ridge), 100)
params = stats.norm.fit(residuals_ridge)
pdf_fitted = stats.norm.pdf(x, *params)
ax3.plot(x, pdf_fitted * len(residuals_ridge) * (max(residuals_ridge) - min(residuals_ridge)) / 10,
                color='#9a031e', linestyle='--', linewidth=2, label='Distribuci√≥n normal')

ax3.legend(frameon=True)

# A√±adir anotaciones sobre la distribuci√≥n
mean_resid_ridge = np.mean(residuals_ridge)
mean_resid_linear = np.mean(residuals_linear)
std_resid_linear = np.std(residuals_linear)
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor=colors['neutral'])
ax3.text(0.85, 0.75,
         f"Ridge:\nMedia: {mean_resid_ridge:.2f}\nStd Dev: {std_resid_ridge:.2f}\n\n"
         f"Linear:\nMedia: {mean_resid_linear:.2f}\nStd Dev: {std_resid_linear:.2f}",
         transform=ax3.transAxes, fontsize=8.5,
         horizontalalignment='left', verticalalignment='center', bbox=props)

# Panel 4: Q-Q Plot de Ridge
ax4 = fig.add_subplot(gs[1, 3:])
ax4.set_facecolor(colors['background'])
sm.qqplot(residuals_ridge, line='45', fit=True, ax=ax4)
ax4.set_xlabel('Cuantiles te√≥ricos', labelpad=12)
ax4.set_ylabel('Residuos estandarizados', labelpad=12)
# Mejorar la consistencia del color en el QQ plot
for item in ax4.get_children():
    if isinstance(item, plt.Line2D):
        if item.get_linestyle() == 'none':
            item.set_markerfacecolor(colors['ridge'])  # Usar el color ridge para los marcadores
            item.set_alpha(0.7)
            item.set_markeredgecolor('white')
            item.set_markeredgewidth(0.5)
            item.set_markersize(5)

ax4.set_title('Q-Q Plot de residuales (Ridge)', pad=15, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Panel 5: Importancia de las caracter√≠sticas (coeficientes de Ridge)
ax5 = fig.add_subplot(gs[2, :])
ax5.set_facecolor(colors['background'])

# Crear un DataFrame para el gr√°fico de barras
plot_data = pd.melt(
    coef_df[['Feature', 'Ridge_Coef', 'Linear_Coef']].sort_values('Ridge_Coef', key=abs, ascending=False).head(12),
    id_vars=['Feature'],
    value_vars=['Ridge_Coef', 'Linear_Coef'],
    var_name='Modelo',
    value_name='Coefficient'
)

# Crear un gr√°fico de barras agrupadas
sns.barplot(x='Coefficient', y='Feature', hue='Modelo', data=plot_data,
            palette={'Ridge_Coef': colors['ridge'], 'Linear_Coef': colors['linear']},
            ax=ax5)
ax5.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax5.set_title('Comparaci√≥n de coeficientes: Ridge vs Linear', pad=15, fontweight='bold')
ax5.set_xlabel('Valor del coeficiente', labelpad=12)
ax5.set_ylabel('Caracter√≠stica', labelpad=12)

legend = ax5.legend(title="Modelo", frameon=True, fancybox=True, loc='lower right')

# Ajustar el dise√±o
plt.tight_layout()
plt.subplots_adjust(top=0.9, hspace=0.5, wspace=0.7)
plt.show()

"""Seguidamente, haremos un an√°lisis de los datos obtenidos:

**Selecci√≥n del par√°metro de regularizaci√≥n:**

Mediante una b√∫squeda sobre una secuencia de valores de alpha, el mejor rendimiento se alcanz√≥ con Œ± = 7.20, valor a partir del cual la penalizaci√≥n L2 comienza a afectar significativamente la magnitud de los coeficientes sin provocar una p√©rdida sustancial de precisi√≥n predictiva. El modelo mostr√≥ una notable estabilidad en la puntuaci√≥n de R¬≤ a lo largo de diferentes valores peque√±os de Œ±, pero comenz√≥ a deteriorarse progresivamente con valores m√°s altos, lo cual es consistente con un fen√≥meno de subajuste inducido por la sobre-regularizaci√≥n.

**Evaluaci√≥n mediante validaci√≥n cruzada:**

La puntuaci√≥n media de validaci√≥n cruzada fue de R¬≤ = 0.6516 ¬± 0.0372, lo cual indica un rendimiento s√≥lido y relativamente estable del modelo Ridge a trav√©s de distintos subconjuntos del conjunto de datos. La baja desviaci√≥n est√°ndar refuerza la idea de que el modelo generaliza bien sin estar excesivamente influido por muestras particulares.

**Comparaci√≥n con el modelo lineal cl√°sico:**

Las m√©tricas de evaluaci√≥n sobre el conjunto de validaci√≥n revelan una ligera superioridad del modelo Ridge respecto a la regresi√≥n lineal ordinaria. Aunque las diferencias son modestas, son sistem√°ticamente favorables a Ridge.

Estas mejoras, aunque peque√±as, son estad√≠sticamente relevantes al estar sustentadas por un proceso de validaci√≥n cruzada. Adem√°s, Ridge tiene la ventaja de manejar mejor problemas de colinealidad.

**Interpretaci√≥n de los coeficientes:**

En cuanto a la importancia de las variables, las principales caracter√≠sticas predictoras seg√∫n el valor absoluto de sus coeficientes en Ridge fueron:
*   `log_sc` (2.63): la creatinina s√©rica es nuevamente la variable m√°s influyente, lo cual es cl√≠nicamente coherente con su relaci√≥n con el da√±o renal.
*   `sqrt_hemo` (-1.08): niveles m√°s bajos de hemoglobina predicen mayores valores de `sqrt_bu`, reflejando anemia secundaria al deterioro renal.
*   `classification_ckd` (-1.05): la clasificaci√≥n como CKD sigue siendo una variable predictora relevante y significativa, con efecto negativo sobre la respuesta.

En comparaci√≥n con el modelo lineal, los coeficientes son ligeramente m√°s atenuados en Ridge, lo cual es esperado dado el efecto de regularizaci√≥n L2. Aun as√≠, la jerarqu√≠a de importancia entre las variables se mantiene pr√°cticamente inalterada.

Para evaluar y comparar modelos estad√≠sticos m√°s all√° de las m√©tricas tradicionales de error, es fundamental considerar criterios que penalicen la complejidad del modelo. En este sentido, el Criterio de Informaci√≥n de Akaike (AIC) y el Criterio de Informaci√≥n Bayesiano (BIC) se utilizan para medir el equilibrio entre el ajuste del modelo y su simplicidad. Ambos criterios se calculan a partir del `log-likelihood` del modelo y penalizan la inclusi√≥n de par√°metros adicionales, favoreciendo modelos que logren una buena explicaci√≥n de los datos con la menor complejidad posible.
"""

def calculate_aic_bic(model, X, y):
    """Calcula AIC y BIC para modelos de sklearn"""

    n = len(y)  # N√∫mero de observaciones
    mse = mean_squared_error(y, model.predict(X))  # Error cuadr√°tico medio
    k = X.shape[1] + 1  # N√∫mero de par√°metros (features + intercept)

    # Calcular log-likelihood (asumiendo distribuci√≥n normal de errores)
    log_likelihood = -n/2 * np.log(2*np.pi) - n/2 * np.log(mse) - 1/(2*mse) * np.sum((y - model.predict(X))**2)

    # F√≥rmulas AIC y BIC
    aic = -2 * log_likelihood + 2 * k
    bic = -2 * log_likelihood + np.log(n) * k

    return aic, bic

# Calcular AIC y BIC para el modelo Ridge
aic_ridge, bic_ridge = calculate_aic_bic(ridge_model, X_train_scaled, y_train)

# Calcular AIC y BIC para el modelo Lineal
aic_linear, bic_linear = calculate_aic_bic(linear_model, X_train_scaled, y_train)

# Mostrar resultados
print("\nüîπ Criterios de informaci√≥n para selecci√≥n de modelos:")
print(f"{'-'*32}")
print(f"{'Criterio':<10} {'Ridge':<12} {'Linear':<12}")
print(f"{'-'*32}")
print(f"{'AIC':<10} {aic_ridge:<12.2f} {aic_linear:<12.2f}")
print(f"{'BIC':<10} {bic_ridge:<12.2f} {bic_linear:<12.2f}")

"""Aunque las diferencias son muy peque√±as, ambos criterios favorecen ligeramente al modelo de regresi√≥n lineal ordinaria. La diferencia es de 0.38 puntos tanto en AIC como en BIC, por lo que ambos modelos pueden considerarse comparables desde el punto de vista del balance entre ajuste y complejidad.

Sin embargo, es importante destacar que el modelo Ridge fue capaz de obtener m√©tricas predictivas ligeramente mejores que el modelo lineal. Esta aparente contradicci√≥n puede explicarse por el hecho de que Ridge introduce una penalizaci√≥n a los coeficientes sin aumentar el n√∫mero de par√°metros. Adem√°s, la diferencia es m√≠nima y poco significativa, por lo que la elecci√≥n del modelo deber√≠a guiarse tambi√©n por la estabilidad de los residuales y la capacidad de generalizaci√≥n, aspectos en los que Ridge ha mostrado ventajas

#### 5.4.2. Regresi√≥n Lasso (L1)

Despu√©s de explorar la regresi√≥n lineal tradicional y su variante regularizada mediante Ridge (L2), continuamos nuestro an√°lisis con otro m√©todo ampliamente utilizado en problemas de regresi√≥n multivariable: la regresi√≥n Lasso (Least Absolute Shrinkage and Selection Operator), tambi√©n conocida como regularizaci√≥n L1.

Mientras que Ridge penaliza los coeficientes grandes sin eliminarlos por completo, Lasso introduce una penalizaci√≥n basada en el valor absoluto de los coeficientes, lo que provoca que algunos de ellos se reduzcan exactamente a cero. Esta propiedad convierte a Lasso no solo en una herramienta para mitigar el sobreajuste, sino tambi√©n en una t√©cnica efectiva de selecci√≥n autom√°tica de variables.

En el siguiente bloque de c√≥digo:
*   Estandarizamos los datos, paso esencial previo a aplicar m√©todos de regularizaci√≥n.
*   Utilizamos `GridSearchCV` para encontrar el valor √≥ptimo del par√°metro `alpha` (fuerza de regularizaci√≥n).
*   Ajustamos un modelo Lasso con el mejor `alpha` encontrado.
*   Calculamos m√©tricas clave como RMSE y R¬≤ para evaluar el rendimiento del modelo sobre el conjunto de prueba.
*   Comparamos con la regresi√≥n lineal tradicional para observar los beneficios de Lasso.
*   Analizamos la importancia de las caracter√≠sticas, destacando aquellas que Lasso mantiene (coeficientes distintos de cero).
*   Visualizamos los residuales del modelo para verificar supuestos estad√≠sticos como la normalidad y la homocedasticidad.
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from scipy import stats
from statsmodels.nonparametric.smoothers_lowess import lowess

# Escalar los datos (recomendado para Lasso)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Encontrar el mejor alfa para Lasso mediante validaci√≥n cruzada
alphas = np.logspace(-3, 1, 15)  # B√∫squeda m√°s granular para alfa
lasso_cv = GridSearchCV(
    Lasso(random_state=42, max_iter=10000),
    {'alpha': alphas},
    cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True,
    n_jobs=-1
)
lasso_cv.fit(X_train_scaled, y_train)

# Mostrar resultados de la validaci√≥n cruzada
cv_results = pd.DataFrame(lasso_cv.cv_results_)
cv_results = cv_results[['param_alpha', 'mean_test_score', 'mean_train_score']]
cv_results['mean_test_score'] = -cv_results['mean_test_score']  # Convertir a MSE positivo
cv_results['mean_train_score'] = -cv_results['mean_train_score']  # Convertir a MSE positivo
print("üîπ Resultados de validaci√≥n cruzada para diferentes valores de alpha:")
print(cv_results)

# Obtener el mejor valor de alpha
best_alpha = lasso_cv.best_params_['alpha']
print(f"\nüîπ Mejor valor de alpha: {best_alpha}")

# Entrenar el modelo Lasso con el mejor valor de alpha
lasso_model = Lasso(alpha=best_alpha, random_state=42, max_iter=10000)
lasso_model.fit(X_train_scaled, y_train)
y_pred_lasso = lasso_model.predict(X_test_scaled)

# Tambi√©n entrenar el modelo de regresi√≥n lineal para comparaci√≥n
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train)
y_pred_linear = linear_model.predict(X_test_scaled)

# Calcular residuales para ambos modelos
residuals_lasso = y_test - y_pred_lasso
residuals_linear = y_test - y_pred_linear

# Calcular diversas m√©tricas de rendimiento para ambos modelos
mse_lasso = mean_squared_error(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mse_lasso)
r2_lasso = r2_score(y_test, y_pred_lasso)
mae_lasso = mean_absolute_error(y_test, y_pred_lasso)

mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_linear)
mae_linear = mean_absolute_error(y_test, y_pred_linear)

# Contar coeficientes distintos de cero en Lasso
non_zero_coefs = np.sum(lasso_model.coef_ != 0)
total_coefs = len(lasso_model.coef_)
print(f"\nüîπ Selecci√≥n de caracter√≠sticas: {non_zero_coefs} de {total_coefs} caracter√≠sticas seleccionadas")

print("\nüîπ Comparaci√≥n de m√©tricas:")
print(f"{'-'*32}")
print(f"{'M√©trica':<10} {'Lasso':<12} {'Linear':<12}")
print(f"{'-'*32}")
print(f"{'MSE':<10} {mse_lasso:<12.4f} {mse_linear:<12.4f}")
print(f"{'RMSE':<10} {rmse_lasso:<12.4f} {rmse_linear:<12.4f}")
print(f"{'R¬≤':<10} {r2_lasso:<12.4f} {r2_linear:<12.4f}")
print(f"{'MAE':<10} {mae_lasso:<12.4f} {mae_linear:<12.4f}")

# Analizar coeficientes
coef_df = pd.DataFrame({
    'Caracter√≠stica': X_train.columns,
    'Lasso_Coef': lasso_model.coef_,
    'Linear_Coef': linear_model.coef_
})
coef_df['Abs_Lasso_Coef'] = np.abs(coef_df['Lasso_Coef'])

# Contar coeficientes distintos de cero
non_zero_coefs = np.sum(coef_df['Lasso_Coef'] != 0)
print("\nüîπ Top 10 caracter√≠sticas por importancia:")
print(coef_df.sort_values('Abs_Lasso_Coef', ascending=False).head(10))
print("\n")

# Configura un estilo visual consistente con una paleta de colores
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'figure.figsize': (12, 8),
    'figure.dpi': 100,
    'axes.grid': True,
    'grid.alpha': 0.3
})

# Definir una paleta de colores cohesiva
colors = {
    'primary': '#1f77b4',
    'secondary': '#ff7f0e',
    'tertiary': '#2ca02c',
    'negative': '#d62728',
    'neutral': '#7f7f7f',
    'background': '#f9f9f9',
    'grid': '#e0e0e0',
    'lasso': '#f8961e',
    'linear': '#184e77'
}

# Crear un panel con un aspecto limpio para comparar Lasso vs Linear
fig = plt.figure(figsize=(14, 12), facecolor=colors['background'])
fig.suptitle('Comparaci√≥n de regresi√≥n Lasso vs regresi√≥n lineal para enfermedad renal',
             fontsize=18, fontweight='bold', y=1, color='#000000')

# A√±adir un subt√≠tulo ligero con contexto adicional
plt.figtext(0.5, 0.96, 'Predicci√≥n de los valores de urea en sangre mediante indicadores cl√≠nicos con regularizaci√≥n Lasso',
            ha='center', fontsize=12, color='#000000')

# Crear una especificaci√≥n de cuadr√≠cula personalizada para un mejor dise√±o
gs = fig.add_gridspec(3, 6)

# Panel 1: Gr√°fico real vs. predicho (ambos modelos)
ax1 = fig.add_subplot(gs[0, :3])
ax1.set_facecolor(colors['background'])
scatter_linear = ax1.scatter(y_test, y_pred_linear, alpha=0.6, s=50,
                     c=colors['linear'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Lineal')
scatter_lasso = ax1.scatter(y_test, y_pred_lasso, alpha=0.6, s=50,
                     c=colors['lasso'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Lasso')
ax1.plot([min(y_test.min(), min(y_pred_linear.min(), y_pred_lasso.min())),
          max(y_test.max(), max(y_pred_linear.max(), y_pred_lasso.max()))],
         [min(y_test.min(), min(y_pred_linear.min(), y_pred_lasso.min())),
          max(y_test.max(), max(y_pred_linear.max(), y_pred_lasso.max()))],
         '--', color=colors['neutral'], linewidth=1.5, label='Predicci√≥n perfecta')
ax1.set_xlabel('Valor real de urea en sangre (transformado en ra√≠z cuadrada)', labelpad=12)
ax1.set_ylabel('Valor previsto de urea en sangre', labelpad=12)
ax1.set_title('Valores reales vs. valores previstos', pad=15, fontweight='bold')
ax1.legend(frameon=True, framealpha=0.9, loc='lower right')

# Cree un cuadro de texto con m√©tricas del modelo
metrics_text = (f"M√©tricas Lasso (Œ±={best_alpha:.6f}):\n"
                f"MSE: {mse_lasso:.2f}            RMSE: {rmse_lasso:.2f}\n"
                f"MAE: {mae_lasso:.2f}            R¬≤: {r2_lasso:.4f}\n"
                f"Coef. no nulos: {non_zero_coefs}/{total_coefs}\n\n"
                f"M√©tricas Linear:\n"
                f"MSE: {mse_linear:.2f}           RMSE: {rmse_linear:.2f}\n"
                f"MAE: {mae_linear:.2f}           R¬≤: {r2_linear:.4f}")
props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=colors['neutral'])
ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, fontsize=8,
         verticalalignment='top', bbox=props)

# Panel 2: Residuales vs. Valores Predichos (ambos modelos)
ax2 = fig.add_subplot(gs[0, 3:])
ax2.set_facecolor(colors['background'])
ax2.scatter(y_pred_linear, residuals_linear, alpha=0.6, s=50,
           c=colors['linear'], edgecolor='white', linewidth=0.5, label='Residuales Linear')
ax2.scatter(y_pred_lasso, residuals_lasso, alpha=0.6, s=50,
           c=colors['lasso'], edgecolor='white', linewidth=0.5, label='Residuales Lasso')
ax2.axhline(y=0, color=colors['neutral'], linestyle='-', linewidth=1.5)

# A√±adir suavizado LOWESS para verificar patrones
lowess_y = lowess(residuals_lasso, y_pred_lasso, frac=0.3)
ax2.plot(lowess_y[:, 0], lowess_y[:, 1], color='#5f0f40', linestyle='-', linewidth=2, label='Tendencia Lasso')

ax2.set_xlabel('Valores predichos', labelpad=12)
ax2.set_ylabel('Residuales', labelpad=12)
ax2.set_title('Residuales vs. valores predichos', pad=15, fontweight='bold')
ax2.legend(frameon=True, framealpha=0.9, loc='lower right')

# Agregar una banda horizontal para mostrar la varianza residual
std_resid_lasso = np.std(residuals_lasso)
ax2.axhspan(-2*std_resid_lasso, 2*std_resid_lasso, alpha=0.1, color=colors['lasso'])
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)
ax2.text(0.05, 0.05, f"¬±2œÉ Lasso: {2*std_resid_lasso:.2f}", transform=ax2.transAxes, fontsize=10,
         verticalalignment='bottom', bbox=props)

# Panel 3: Distribuci√≥n de residuales de ambos modelos
ax3 = fig.add_subplot(gs[1, :3])
ax3.set_facecolor(colors['background'])
sns.histplot(residuals_linear, kde=True, ax=ax3, color=colors['linear'], alpha=0.5, label='Residuales Linear')
sns.histplot(residuals_lasso, kde=True, ax=ax3, color=colors['lasso'], alpha=0.5, label='Residuales Lasso')
ax3.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax3.set_xlabel('Valor residual', labelpad=12)
ax3.set_ylabel('Frecuencia', labelpad=12)
ax3.set_title('Distribuci√≥n de residuales', pad=15, fontweight='bold')

# A√±adir curva de distribuci√≥n normal para comparaci√≥n
x = np.linspace(min(residuals_lasso), max(residuals_lasso), 100)
params = stats.norm.fit(residuals_lasso)
pdf_fitted = stats.norm.pdf(x, *params)
ax3.plot(x, pdf_fitted * len(residuals_lasso) * (max(residuals_lasso) - min(residuals_lasso)) / 10,
                color='#9a031e', linestyle='--', linewidth=2, label='Distribuci√≥n normal')

ax3.legend(frameon=True)

# A√±adir anotaciones sobre la distribuci√≥n
mean_resid_lasso = np.mean(residuals_lasso)
mean_resid_linear = np.mean(residuals_linear)
std_resid_linear = np.std(residuals_linear)
skew_resid = stats.skew(residuals_lasso)
kurt_resid = stats.kurtosis(residuals_lasso)

stats_text = (f"Lasso:\nMedia: {mean_resid_lasso:.2f}\nStd Dev: {std_resid_lasso:.2f}\n"
              f"Skew: {skew_resid:.2f}\nKurt: {kurt_resid:.2f}\n\n"
              f"Linear:\nMedia: {mean_resid_linear:.2f}\nStd Dev: {std_resid_linear:.2f}")
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor=colors['neutral'])
ax3.text(0.85, 0.70, stats_text,
         transform=ax3.transAxes, fontsize=8.5,
         horizontalalignment='left', verticalalignment='center', bbox=props)

# Panel 4: Q-Q Plot de Lasso
ax4 = fig.add_subplot(gs[1, 3:])
ax4.set_facecolor(colors['background'])
sm.qqplot(residuals_lasso, line='45', fit=True, ax=ax4)
ax4.set_xlabel('Cuantiles te√≥ricos', labelpad=12)
ax4.set_ylabel('Residuos estandarizados', labelpad=12)
for item in ax4.get_children():
    if isinstance(item, plt.Line2D):
        if item.get_linestyle() == 'none':
            item.set_color(colors['lasso'])
            item.set_alpha(0.7)
            item.set_markeredgecolor('white')
            item.set_markeredgewidth(0.5)
            item.set_markersize(5)

ax4.set_title('Q-Q Plot de residuales (Lasso)', pad=15, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Panel 5: Importancia de las caracter√≠sticas y comparaci√≥n entre Lasso y Linear
ax5 = fig.add_subplot(gs[2, :])
ax5.set_facecolor(colors['background'])

# Calcular la importancia de las caracter√≠sticas para Lasso
lasso_coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': lasso_model.coef_
})
lasso_coefficients['Abs_Coefficient'] = lasso_coefficients['Coefficient'].abs()
lasso_coefficients = lasso_coefficients.sort_values('Abs_Coefficient', ascending=False)

# Caracter√≠sticas con coeficientes no nulos en Lasso
features_selected = lasso_coefficients[lasso_coefficients['Coefficient'] != 0]

# Calcular la importancia de las caracter√≠sticas para Linear
linear_coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': linear_model.coef_
})
linear_coefficients['Abs_Coefficient'] = linear_coefficients['Coefficient'].abs()

# Fusionar coeficientes para comparaci√≥n
merged_coefs = pd.merge(
    lasso_coefficients[['Feature', 'Coefficient']].rename(columns={'Coefficient': 'Lasso_Coef'}),
    linear_coefficients[['Feature', 'Coefficient']].rename(columns={'Coefficient': 'Linear_Coef'}),
    on='Feature'
)

# Solo mostrar coeficientes no nulos de Lasso + los equivalentes de Linear
merged_coefs_filtered = merged_coefs[merged_coefs['Lasso_Coef'] != 0].sort_values('Lasso_Coef', key=abs, ascending=False)

if len(merged_coefs_filtered) > 0:
    # Crear un DataFrame para el gr√°fico de barras
    plot_data = pd.melt(
        merged_coefs_filtered,
        id_vars=['Feature'],
        value_vars=['Lasso_Coef', 'Linear_Coef'],
        var_name='Modelo',
        value_name='Coefficient'
    )

    # Crear un gr√°fico de barras agrupadas
    sns.barplot(x='Coefficient', y='Feature', hue='Modelo', data=plot_data,
                palette={'Lasso_Coef': colors['lasso'], 'Linear_Coef': colors['linear']},
                ax=ax5)
    ax5.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
    ax5.set_title('Comparaci√≥n de coeficientes: Lasso vs Linear (s√≥lo coef. no nulos de Lasso)', pad=15, fontweight='bold')
    ax5.set_xlabel('Valor del coeficiente', labelpad=12)
    ax5.set_ylabel('Caracter√≠stica', labelpad=12)

    legend = ax5.legend(title="Modelo", frameon=True, fancybox=True, loc='lower right')
else:
    # Si Lasso elimina todas las caracter√≠sticas (improbable pero posible)
    ax5.text(0.5, 0.5, "Lasso elimin√≥ todas las caracter√≠sticas",
             horizontalalignment='center', verticalalignment='center',
             fontsize=14, transform=ax5.transAxes)

# Ajustar el dise√±o
plt.tight_layout()
plt.subplots_adjust(top=0.9, hspace=0.5, wspace=0.7)
plt.show()

"""A continuaci√≥n vamos a analizar los resultados obtenidos:

**Evaluaci√≥n mediante validaci√≥n cruzada:**

Para determinar el valor √≥ptimo del par√°metro de regularizaci√≥n, se llev√≥ a cabo una validaci√≥n cruzada empleando una grilla de valores de alpha que abarc√≥ desde valores cercanos a cero hasta valores m√°s elevados. El an√°lisis de los resultados mostr√≥ que para valores peque√±os de alpha (entre 0.00 y 0.03), tanto el error de entrenamiento como el de validaci√≥n permanecieron bajos y estables (alrededor de 2.23-2.24), indicando que el modelo logra un buen ajuste sin incurrir en sobreajuste.

Sin embargo, conforme se increment√≥ el valor de alpha, se observ√≥ un deterioro progresivo en el rendimiento, con incrementos marcados en MSE, especialmente a partir de alpha = 0.1. Este comportamiento sugiere que el modelo comienza a subajustarse, perdiendo capacidad explicativa al penalizar excesivamente los coeficientes. A valores extremos de alpha, el error alcanz√≥ 6.70, evidenciando una clara p√©rdida de calidad predictiva.

El valor √≥ptimo encontrado fue alpha = 0.001, que minimiza el error de validaci√≥n sin comprometer la parsimonia del modelo. Este valor logra el mejor equilibrio entre ajuste y regularizaci√≥n, permitiendo preservar todas las variables relevantes sin incurrir en sobreajuste.

**Comparaci√≥n de desempe√±o con la regresi√≥n lineal cl√°sica:**

Al comparar el modelo Lasso con un modelo de regresi√≥n lineal m√∫ltiple cl√°sico, los resultados obtenidos muestran diferencias m√≠nimas entre ambos enfoques.

Las cifras obtenidas indican que Lasso consigue un rendimiento pr√°cticamente id√©ntico al modelo lineal sin penalizaci√≥n, lo cual puede atribuirse a la adecuada selecci√≥n de variables y a la estabilidad de las relaciones lineales en los datos. No obstante, se destaca que Lasso aporta una ventaja estructural al evitar la posible inflaci√≥n de varianza por colinealidad y al controlar la magnitud de los coeficientes.

**Selecci√≥n y relevancia de variables:**

A pesar de la capacidad de Lasso para reducir coeficientes a cero, en este caso, se conservaron las 7 variables originales en el modelo final. Esto sugiere que, bajo el nivel de penalizaci√≥n √≥ptimo, todas las variables evaluadas poseen un grado de relevancia suficiente para justificar su inclusi√≥n.

Las variables con mayor importancia, seg√∫n la magnitud de sus coeficientes en el modelo Lasso, fueron:
*   `log_sc` (1.64): se confirma como el predictor m√°s relevante. Este resultado concuerda con la fisiopatolog√≠a de la funci√≥n renal, donde niveles elevados de creatinina reflejan una disminuci√≥n del filtrado glomerular.
*   `classification_ckd` (-0.55): su relaci√≥n negativa con la variable dependiente puede estar influenciada por la forma en que se codific√≥ la categor√≠a. Aun as√≠, resulta cl√≠nicamente relevante como indicador de diagn√≥stico.
*   `sqrt_hemo` (-0.48): la anemia es com√∫n en pacientes con enfermedad renal, y su relaci√≥n inversa con los niveles de urea es coherente con los hallazgos cl√≠nicos.

El siguiente bloque de c√≥digo realiza el c√°lculo de AIC y BIC tanto para el modelo de regresi√≥n Lasso como para el modelo de regresi√≥n lineal cl√°sico, utilizando los datos de entrenamiento previamente estandarizados. Esto permite comparar objetivamente ambos enfoques bajo un marco te√≥rico m√°s riguroso para la selecci√≥n de modelos.
"""

# Calcular AIC y BIC para el modelo Lasso
aic_lasso, bic_lasso = calculate_aic_bic(lasso_model, X_train_scaled, y_train)

# Calcular AIC y BIC para el modelo Lineal
aic_linear, bic_linear = calculate_aic_bic(linear_model, X_train_scaled, y_train)

# Mostrar resultados
print("\nüîπ Criterios de informaci√≥n para selecci√≥n de modelos:")
print(f"{'-'*32}")
print(f"{'Criterio':<10} {'Lasso':<12} {'Linear':<12}")
print(f"{'-'*32}")
print(f"{'AIC':<10} {aic_lasso:<12.2f} {aic_linear:<12.2f}")
print(f"{'BIC':<10} {bic_lasso:<12.2f} {bic_linear:<12.2f}")

"""Los resultados muestran que ambos modelos presentan exactamente los mismos valores de AIC y BIC, lo que indica que, desde el punto de vista de estos criterios de informaci√≥n, no existe una diferencia significativa entre ellos.

Sin embargo, es importante destacar que el modelo Lasso mantiene su utilidad como m√©todo preventivo contra la sobreajuste y la colinealidad.

#### 5.4.3. Evaluaci√≥n de los m√©todos

La regresi√≥n regularizada tiene como objetivo mejorar la capacidad predictiva del modelo, reducir el sobreajuste y estabilizar los coeficientes cuando hay multicolinealidad o ruido en los datos. Ridge y Lasso logran esto penalizando la magnitud de los coeficientes, pero de maneras distintas, lo que influye directamente en su comportamiento.

Es por ello que vamos a analizar los resultados obtenidos, y as√≠ entender c√≥mo cada m√©todo ha trabajado sobre el conjunto.

**Regresi√≥n Ridge (L2):**

*   Su mejor par√°metro Œ± es de 7.20.
*   Su rendimiento sobre test es:
    *   RMSE: 1.8855 (menor error de predicci√≥n de los tres modelos).
    *   MAE: 1.3705.
    *   R¬≤: 0.6091 (ligeramente superior al modelo lineal y Lasso).
    *   Validaci√≥n cruzada R¬≤: 0.6516 ¬± 0.0372 (rendimiento estable y consistente).
*   Selecci√≥n de variables: No elimina variables (todos los coeficientes son distintos de cero).
*   El top 4 caracter√≠sticas m√°s relevantes (por magnitud absoluta de coeficiente) es:
    *   `log_sc` (2.63): es altamente influyente y tienen un coeficiente grande.
    *   `sqrt_hemo` (-1.08): relaci√≥n negativa significativa, posiblemente reflejando anemia.
    *   `classification_ckd` (-1.05): indica menor `sqrt_bu` en pacientes sin CKD.
    *   `log_pot` (0.82): sugiere que el potasio s√©rico tambi√©n tiene un papel en la predicci√≥n.

**Regresi√≥n Lasso (L1):**

*   Su mejor par√°metro Œ± es de 0.001 (regularizaci√≥n muy leve, ajustada finamente).
*   Su rendimiento sobre test es:
    *   RMSE: 1.8920 (ligeramente mayor que Ridge e inferior que Lineal).
    *   MAE: 1.3727.
    *   R¬≤: 0.6064 (inferior a Ridge).
    *   Validaci√≥n cruzada R¬≤: 0.6064 (muy estable, con baja desviaci√≥n)
*   Selecci√≥n de variables: se conservaron 7 de 7 variables, lo que significa que no elimin√≥ variables en este caso, indicando que todas aportan informaci√≥n relevante bajo esta regularizaci√≥n.
*   El top 4 caracter√≠sticas m√°s relevantes (por magnitud absoluta) es:
    *   `log_sc`	(1.64): esta variable sigue siendo la m√°s influyente.
    *   `classification_ckd`	(-0.55): influye negativamente, con implicaciones diagn√≥sticas.
    *   `sqrt_hemo` (-0.48): relevancia fisiol√≥gica clara.
    *   `sqrt_al`	(0.34): indicador del grado de da√±o renal.

**Conclusiones finales:**

*   Ridge (L2) es la mejor opci√≥n si el objetivo es maximizar precisi√≥n y estabilidad, sin importar la cantidad de variables. Este aprovecha toda la informaci√≥n disponible, incluso de variables menos relevantes. Es el modelo con menor RMSE y mayor R¬≤, indicando mejor ajuste general y menor error de predicci√≥n.
*   Lasso (L1) es m√°s √∫til si se desea un modelo m√°s simple, con buena capacidad explicativa y menos variables. Este reduce dimensionalidad autom√°ticamente, lo cual es √∫til si se busca un modelo m√°s interpretable o eficiente. Sin embargo, sacrifica algo de capacidad predictiva frente a Ridge, como indica su RMSE m√°s alto. Aun as√≠, la validaci√≥n cruzada muestra que es un modelo consistente y robusto.
*   Ambos modelos superan a la regresi√≥n lineal en estabilidad, gesti√≥n de ruido en los datos y capacidad de generalizaci√≥n.
*   La coincidencia en las variables m√°s relevantes entre ambos m√©todos sugiere una alta concordancia en los factores que influyen en la variable objetivo, lo que aporta mayor confianza en la interpretaci√≥n de los resultados.

### 5.5. Modelos predictivos

El siguiente paso es evaluar modelos predictivos que permitan capturar patrones significativos en los datos sin caer en el sobreajuste y ajustarlo a nuestro conjunto de entrenamiento.

#### 5.5.1. ElasticNet

En este contexto, hemos optado por aplicar `ElasticNet`, un poderoso algoritmo de regresi√≥n lineal regularizada, dise√±ado para prevenir el sobreajuste y mejorar la capacidad de generalizaci√≥n del modelo. Su principal fortaleza radica en que fusiona dos m√©todos cl√°sicos de regularizaci√≥n: Lasso (L1) y Ridge (L2), lo que le permite:
*   Reducir la magnitud de los coeficientes (como Ridge), lo cual es √∫til en presencia de multicolinealidad entre variables.
*   Realizar selecci√≥n de variables autom√°tica (como Lasso), estableciendo algunos coeficientes exactamente en cero.
*   Ser especialmente efectivo en situaciones donde existen grupos de variables correlacionadas o el n√∫mero de variables supera al n√∫mero de observaciones.

El c√≥digo que tenemos a continuaci√≥n desarrolla la implementaci√≥n completa de este enfoque. A trav√©s del uso de `GridSearchCV`, se optimizan los hiperpar√°metros del modelo (`alpha` y `l1_ratio`), permitiendo un ajuste m√°s fino y adaptado al problema espec√≠fico. El c√≥digo tambi√©n aborda:
*   La estandarizaci√≥n de las variables predictoras con `StandardScaler`.
*   El entrenamiento del modelo final con los mejores par√°metros.
*   La evaluaci√≥n comparativa del desempe√±o frente a una regresi√≥n lineal tradicional usando m√©tricas como RMSE y R¬≤.
*   El an√°lisis de los coeficientes, incluyendo cu√°ntos fueron seleccionados (no nulos) por `ElasticNet`.
*   La validaci√≥n cruzada para estimar la estabilidad del modelo en distintos subconjuntos de datos.
*   El an√°lisis visual e inferencial de los residuales comparando con Lasso y Linear. Este incluye gr√°ficos de dispersi√≥n, distribuci√≥n, pruebas de normalidad (como Shapiro-Wilk) y Q-Q plots.
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, LinearRegression, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from scipy import stats
from statsmodels.nonparametric.smoothers_lowess import lowess

# Escalar los datos (recomendado para ElasticNet)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Encontrar los mejores hiperpar√°metros para ElasticNet mediante validaci√≥n cruzada
alphas = np.logspace(-3, 1, 10)  # Valores de alfa para probar
l1_ratios = np.linspace(0.1, 1.0, 10)  # Valores de l1_ratio (0 = Ridge, 1 = Lasso)

elasticnet_cv = GridSearchCV(
    ElasticNet(random_state=42, max_iter=10000),
    {'alpha': alphas, 'l1_ratio': l1_ratios},
    cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True,
    n_jobs=-1
)
elasticnet_cv.fit(X_train_scaled, y_train)

# Mostrar resultados de la validaci√≥n cruzada
cv_results = pd.DataFrame(elasticnet_cv.cv_results_)
best_params = cv_results[cv_results['rank_test_score'] == 1][['param_alpha', 'param_l1_ratio', 'mean_test_score', 'mean_train_score']]
best_params['mean_test_score'] = -best_params['mean_test_score']  # Convertir a MSE positivo
best_params['mean_train_score'] = -best_params['mean_train_score']  # Convertir a MSE positivo
print("üîπ Mejores par√°metros para ElasticNet:")
print(best_params)

# Obtener los mejores hiperpar√°metros
best_alpha = elasticnet_cv.best_params_['alpha']
best_l1_ratio = elasticnet_cv.best_params_['l1_ratio']
print(f"\nüîπ Mejor valor de alpha: {best_alpha}")
print(f"üîπ Mejor valor de l1_ratio: {best_l1_ratio}")

# Entrenar modelos: ElasticNet, Lasso y Lineal para comparaci√≥n
elasticnet_model = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio, random_state=42, max_iter=10000)
elasticnet_model.fit(X_train_scaled, y_train)
y_pred_elasticnet = elasticnet_model.predict(X_test_scaled)

# Tambi√©n entrenar el modelo Lasso con el mejor valor de alpha
lasso_cv = GridSearchCV(
    Lasso(random_state=42, max_iter=10000),
    {'alpha': alphas},
    cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True,
    n_jobs=-1
)
lasso_cv.fit(X_train_scaled, y_train)
best_alpha_lasso = lasso_cv.best_params_['alpha']
lasso_model = Lasso(alpha=best_alpha_lasso, random_state=42, max_iter=10000)
lasso_model.fit(X_train_scaled, y_train)
y_pred_lasso = lasso_model.predict(X_test_scaled)

# Tambi√©n entrenar el modelo de regresi√≥n lineal para comparaci√≥n
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train)
y_pred_linear = linear_model.predict(X_test_scaled)

# Calcular residuales para los tres modelos
residuals_elasticnet = y_test - y_pred_elasticnet
residuals_lasso = y_test - y_pred_lasso
residuals_linear = y_test - y_pred_linear

# Calcular diversas m√©tricas de rendimiento para los tres modelos
mse_elasticnet = mean_squared_error(y_test, y_pred_elasticnet)
rmse_elasticnet = np.sqrt(mse_elasticnet)
r2_elasticnet = r2_score(y_test, y_pred_elasticnet)
mae_elasticnet = mean_absolute_error(y_test, y_pred_elasticnet)

mse_lasso = mean_squared_error(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mse_lasso)
r2_lasso = r2_score(y_test, y_pred_lasso)
mae_lasso = mean_absolute_error(y_test, y_pred_lasso)

mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_linear)
mae_linear = mean_absolute_error(y_test, y_pred_linear)

# Contar coeficientes distintos de cero en ElasticNet y Lasso
non_zero_coefs_elasticnet = np.sum(elasticnet_model.coef_ != 0)
non_zero_coefs_lasso = np.sum(lasso_model.coef_ != 0)
total_coefs = len(elasticnet_model.coef_)
print(f"\nüîπ Selecci√≥n de caracter√≠sticas ElasticNet: {non_zero_coefs_elasticnet} de {total_coefs} caracter√≠sticas seleccionadas")
print(f"üîπ Selecci√≥n de caracter√≠sticas Lasso: {non_zero_coefs_lasso} de {total_coefs} caracter√≠sticas seleccionadas")

print("\nüîπ Comparaci√≥n de m√©tricas:")
print(f"{'-'*45}")
print(f"{'M√©trica':<10} {'ElasticNet':<12} {'Lasso':<12} {'Linear':<12}")
print(f"{'-'*45}")
print(f"{'MSE':<10} {mse_elasticnet:<12.4f} {mse_lasso:<12.4f} {mse_linear:<12.4f}")
print(f"{'RMSE':<10} {rmse_elasticnet:<12.4f} {rmse_lasso:<12.4f} {rmse_linear:<12.4f}")
print(f"{'R¬≤':<10} {r2_elasticnet:<12.4f} {r2_lasso:<12.4f} {r2_linear:<12.4f}")
print(f"{'MAE':<10} {mae_elasticnet:<12.4f} {mae_lasso:<12.4f} {mae_linear:<12.4f}")

# Analizar coeficientes
coef_df = pd.DataFrame({
    'Caracter√≠stica': X_train.columns,
    'ElasticNet_Coef': elasticnet_model.coef_,
    'Lasso_Coef': lasso_model.coef_,
    'Linear_Coef': linear_model.coef_
})
coef_df['Abs_ElasticNet_Coef'] = np.abs(coef_df['ElasticNet_Coef'])

# Mostrar top caracter√≠sticas por importancia
print("\nüîπ Top 10 caracter√≠sticas por importancia (ElasticNet):")
print(coef_df.sort_values('Abs_ElasticNet_Coef', ascending=False).head(10))
print("\n")

# Configura un estilo visual consistente con una paleta de colores
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'figure.figsize': (12, 8),
    'figure.dpi': 100,
    'axes.grid': True,
    'grid.alpha': 0.3
})

# Definir una paleta de colores cohesiva
colors = {
    'primary': '#1f77b4',
    'secondary': '#ff7f0e',
    'tertiary': '#2ca02c',
    'negative': '#d62728',
    'neutral': '#7f7f7f',
    'background': '#f9f9f9',
    'grid': '#e0e0e0',
    'elasticnet': '#9c27b0',  # Purple for ElasticNet
    'lasso': '#f8961e',
    'linear': '#184e77'
}

# Crear un panel con un aspecto limpio para comparar ElasticNet vs Lasso vs Linear
fig = plt.figure(figsize=(14, 18), facecolor=colors['background'])  # Aumentado la altura para una fila adicional
fig.suptitle('Comparaci√≥n de regresi√≥n ElasticNet vs Lasso vs Lineal para enfermedad renal',
             fontsize=18, fontweight='bold', y=0.985, color='#000000')  # Ajustado 'y' para el nuevo layout

# A√±adir un subt√≠tulo ligero con contexto adicional
plt.figtext(0.5, 0.96, f'Predicci√≥n de los valores de urea en sangre con ElasticNet (Œ±={best_alpha:.6f}, l1_ratio={best_l1_ratio:.2f})',
            ha='center', fontsize=12, color='#000000')  # Ajustado 'y' para el nuevo layout

# Crear una especificaci√≥n de cuadr√≠cula personalizada para un mejor dise√±o con 4 filas y 6 columnas
gs = fig.add_gridspec(4, 6)  # Ahora 4 filas en lugar de 3

# Panel 1: Gr√°fico real vs. predicho (tres modelos)
ax1 = fig.add_subplot(gs[0, :3])
ax1.set_facecolor(colors['background'])
scatter_linear = ax1.scatter(y_test, y_pred_linear, alpha=0.6, s=50,
                     c=colors['linear'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Lineal')
scatter_lasso = ax1.scatter(y_test, y_pred_lasso, alpha=0.6, s=50,
                     c=colors['lasso'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Lasso')
scatter_elasticnet = ax1.scatter(y_test, y_pred_elasticnet, alpha=0.6, s=50,
                     c=colors['elasticnet'], edgecolor='white', linewidth=0.5, label='ElasticNet')
ax1.plot([min(y_test.min(), min(y_pred_linear.min(), y_pred_lasso.min(), y_pred_elasticnet.min())),
          max(y_test.max(), max(y_pred_linear.max(), y_pred_lasso.max(), y_pred_elasticnet.max()))],
         [min(y_test.min(), min(y_pred_linear.min(), y_pred_lasso.min(), y_pred_elasticnet.min())),
          max(y_test.max(), max(y_pred_linear.max(), y_pred_lasso.max(), y_pred_elasticnet.max()))],
         '--', color=colors['neutral'], linewidth=1.5, label='Predicci√≥n perfecta')
ax1.set_xlabel('Valor real de urea en sangre (transformado en ra√≠z cuadrada)', labelpad=12)
ax1.set_ylabel('Valor previsto de urea en sangre', labelpad=12)
ax1.set_title('Valores reales vs. valores previstos', pad=15, fontweight='bold')
ax1.legend(frameon=True, framealpha=0.9, loc='lower right')

# Cree un cuadro de texto con m√©tricas del modelo
metrics_text = (f"M√©tricas ElasticNet (Œ±={best_alpha:.6f}, l1_ratio={best_l1_ratio:.2f}):\n"
                f"MSE: {mse_elasticnet:.2f}     RMSE: {rmse_elasticnet:.2f}\n"
                f"MAE: {mae_elasticnet:.2f}     R¬≤: {r2_elasticnet:.4f}\n"
                f"Coef. no nulos: {non_zero_coefs_elasticnet}/{total_coefs}\n\n"
                f"M√©tricas Lasso (Œ±={best_alpha_lasso:.6f}):\n"
                f"MSE: {mse_lasso:.2f}     RMSE: {rmse_lasso:.2f}\n"
                f"MAE: {mae_lasso:.2f}     R¬≤: {r2_lasso:.4f}\n"
                f"Coef. no nulos: {non_zero_coefs_lasso}/{total_coefs}")
props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=colors['neutral'])
ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, fontsize=8,
         verticalalignment='top', bbox=props)

# Panel 2: Residuales vs. Valores Predichos (tres modelos)
ax2 = fig.add_subplot(gs[0, 3:])
ax2.set_facecolor(colors['background'])
ax2.scatter(y_pred_linear, residuals_linear, alpha=0.6, s=50,
           c=colors['linear'], edgecolor='white', linewidth=0.5, label='Residuales Linear')
ax2.scatter(y_pred_lasso, residuals_lasso, alpha=0.6, s=50,
           c=colors['lasso'], edgecolor='white', linewidth=0.5, label='Residuales Lasso')
ax2.scatter(y_pred_elasticnet, residuals_elasticnet, alpha=0.6, s=50,
           c=colors['elasticnet'], edgecolor='white', linewidth=0.5, label='Residuales ElasticNet')
ax2.axhline(y=0, color=colors['neutral'], linestyle='-', linewidth=1.5)

# A√±adir suavizado LOWESS para verificar patrones (ElasticNet)
lowess_y = lowess(residuals_elasticnet, y_pred_elasticnet, frac=0.3)
ax2.plot(lowess_y[:, 0], lowess_y[:, 1], color='#5f0f40', linestyle='-', linewidth=2, label='Tendencia ElasticNet')

ax2.set_xlabel('Valores predichos', labelpad=12)
ax2.set_ylabel('Residuales', labelpad=12)
ax2.set_title('Residuales vs. valores predichos', pad=15, fontweight='bold')
ax2.legend(frameon=True, framealpha=0.9, loc='lower right')

# Agregar una banda horizontal para mostrar la varianza residual
std_resid_elasticnet = np.std(residuals_elasticnet)
ax2.axhspan(-2*std_resid_elasticnet, 2*std_resid_elasticnet, alpha=0.1, color=colors['elasticnet'])
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)
ax2.text(0.05, 0.05, f"¬±2œÉ ElasticNet: {2*std_resid_elasticnet:.2f}", transform=ax2.transAxes, fontsize=10,
         verticalalignment='bottom', bbox=props)

# Panel 3: Distribuci√≥n de residuales de los tres modelos
ax3 = fig.add_subplot(gs[1, :3])
ax3.set_facecolor(colors['background'])
sns.histplot(residuals_linear, kde=True, ax=ax3, color=colors['linear'], alpha=0.4, label='Residuales Linear')
sns.histplot(residuals_lasso, kde=True, ax=ax3, color=colors['lasso'], alpha=0.4, label='Residuales Lasso')
sns.histplot(residuals_elasticnet, kde=True, ax=ax3, color=colors['elasticnet'], alpha=0.4, label='Residuales ElasticNet')
ax3.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax3.set_xlabel('Valor residual', labelpad=12)
ax3.set_ylabel('Frecuencia', labelpad=12)
ax3.set_title('Distribuci√≥n de residuales', pad=15, fontweight='bold')

# A√±adir curva de distribuci√≥n normal para comparaci√≥n (ElasticNet)
x = np.linspace(min(residuals_elasticnet), max(residuals_elasticnet), 100)
params = stats.norm.fit(residuals_elasticnet)
pdf_fitted = stats.norm.pdf(x, *params)
ax3.plot(x, pdf_fitted * len(residuals_elasticnet) * (max(residuals_elasticnet) - min(residuals_elasticnet)) / 10,
                color='#9a031e', linestyle='--', linewidth=2, label='Distribuci√≥n normal')

ax3.legend(frameon=True)

# A√±adir anotaciones sobre la distribuci√≥n
mean_resid_elasticnet = np.mean(residuals_elasticnet)
mean_resid_lasso = np.mean(residuals_lasso)
mean_resid_linear = np.mean(residuals_linear)
std_resid_elasticnet = np.std(residuals_elasticnet)
std_resid_lasso = np.std(residuals_lasso)
std_resid_linear = np.std(residuals_linear)
skew_resid = stats.skew(residuals_elasticnet)
kurt_resid = stats.kurtosis(residuals_elasticnet)

stats_text = (f"ElasticNet:\nMedia: {mean_resid_elasticnet:.2f}\nStd Dev: {std_resid_elasticnet:.2f}\n"
              f"Skew: {skew_resid:.2f}\nKurt: {kurt_resid:.2f}\n\n"
              f"Lasso:\nMedia: {mean_resid_lasso:.2f}\nStd Dev: {std_resid_lasso:.2f}\n\n"
              f"Linear:\nMedia: {mean_resid_linear:.2f}\nStd Dev: {std_resid_linear:.2f}")
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor=colors['neutral'])
ax3.text(0.85, 0.70, stats_text,
         transform=ax3.transAxes, fontsize=8.5,
         horizontalalignment='left', verticalalignment='center', bbox=props)

# Panel 4: Q-Q Plot de ElasticNet
ax4 = fig.add_subplot(gs[1, 3:])
ax4.set_facecolor(colors['background'])
sm.qqplot(residuals_elasticnet, line='45', fit=True, ax=ax4)
ax4.set_xlabel('Cuantiles te√≥ricos', labelpad=12)
ax4.set_ylabel('Residuos estandarizados', labelpad=12)
for item in ax4.get_children():
    if isinstance(item, plt.Line2D):
        if item.get_linestyle() == 'none':
            item.set_color(colors['elasticnet'])
            item.set_alpha(0.7)
            item.set_markeredgecolor('white')
            item.set_markeredgewidth(0.5)
            item.set_markersize(5)

ax4.set_title('Q-Q Plot de residuales (ElasticNet)', pad=15, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Panel 5: Trayectoria de regularizaci√≥n (recorrido del par√°metro alpha)
ax5 = fig.add_subplot(gs[2, :])
ax5.set_facecolor(colors['background'])

# Trayectorias de regularizaci√≥n para diferentes valores de l1_ratio
l1_ratios_to_plot = [0.1, 0.5, 0.9, 1.0]  # 1.0 es equivalente a Lasso
alphas_range = np.logspace(-3, 1, 100)

for l1_ratio in l1_ratios_to_plot:
    coefs = []
    for alpha in alphas_range:
        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=10000)
        model.fit(X_train_scaled, y_train)
        coefs.append(model.coef_)

    coefs = np.array(coefs)

    for i in range(coefs.shape[1]):
        if np.any(coefs[:, i] != 0):  # Solo plotear coeficientes que no son siempre cero
            if l1_ratio == 1.0:
                label = f"Lasso" if i == 0 else ""
                linestyle = '-'
                alpha = 0.9
            else:
                label = f"ElasticNet (l1_ratio={l1_ratio})" if i == 0 else ""
                linestyle = '--' if l1_ratio < 0.5 else '-.'
                alpha = 0.5 if l1_ratio < 0.9 else 0.7

            ax5.plot(alphas_range, coefs[:, i],
                    linestyle=linestyle, alpha=alpha,
                    label=label)

ax5.axvline(x=best_alpha, color=colors['elasticnet'], linestyle='--', linewidth=1.5,
            label=f'Mejor Œ± ElasticNet: {best_alpha:.6f}')
ax5.axvline(x=best_alpha_lasso, color=colors['lasso'], linestyle='--', linewidth=1.5,
            label=f'Mejor Œ± Lasso: {best_alpha_lasso:.6f}')
ax5.set_xscale('log')
ax5.set_xlabel('Valor de alpha (escala log)', labelpad=12)
ax5.set_ylabel('Valor de coeficiente', labelpad=12)
ax5.set_title('Trayectorias de coeficientes para regularizaci√≥n ElasticNet y Lasso', pad=15, fontweight='bold')
ax5.legend(frameon=True, loc='upper right')

# Panel 6: Importancia de las caracter√≠sticas y comparaci√≥n entre ElasticNet, Lasso y Linear
ax6 = fig.add_subplot(gs[3, :])
ax6.set_facecolor(colors['background'])

# Solo mostrar coeficientes no nulos de ElasticNet + los equivalentes de otros modelos
merged_coefs_filtered = coef_df[coef_df['ElasticNet_Coef'] != 0].sort_values('ElasticNet_Coef', key=abs, ascending=False)

if len(merged_coefs_filtered) > 0:
    # Limitar a las top caracter√≠sticas para una mejor visualizaci√≥n
    top_n = min(10, len(merged_coefs_filtered))
    merged_coefs_filtered = merged_coefs_filtered.head(top_n)

    # Crear un DataFrame para el gr√°fico de barras
    plot_data = pd.melt(
        merged_coefs_filtered,
        id_vars=['Caracter√≠stica'],
        value_vars=['ElasticNet_Coef', 'Lasso_Coef', 'Linear_Coef'],
        var_name='Modelo',
        value_name='Coefficient'
    )

    # Crear un gr√°fico de barras agrupadas
    sns.barplot(x='Coefficient', y='Caracter√≠stica', hue='Modelo', data=plot_data,
                palette={'ElasticNet_Coef': colors['elasticnet'],
                         'Lasso_Coef': colors['lasso'],
                         'Linear_Coef': colors['linear']},
                ax=ax6)
    ax6.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
    ax6.set_title('Comparaci√≥n de coeficientes: ElasticNet vs Lasso vs Linear', pad=15, fontweight='bold')
    ax6.set_xlabel('Valor del coeficiente', labelpad=12)
    ax6.set_ylabel('Caracter√≠stica', labelpad=12)

    legend = ax6.legend(title="Modelo", frameon=True, fancybox=True, loc='lower right')
else:
    # Si ElasticNet elimina todas las caracter√≠sticas (improbable pero posible)
    ax6.text(0.5, 0.5, "ElasticNet elimin√≥ todas las caracter√≠sticas",
             horizontalalignment='center', verticalalignment='center',
             fontsize=14, transform=ax6.transAxes)

# Ajustar el dise√±o
plt.tight_layout()
plt.subplots_adjust(top=0.92, hspace=0.4, wspace=0.3)
plt.show()

# Calcular AIC y BIC para el modelo ElasticNet
aic_elasticnet, bic_elasticnet = calculate_aic_bic(elasticnet_model, X_train_scaled, y_train)

# Calcular AIC y BIC para el modelo Lineal
aic_linear, bic_linear = calculate_aic_bic(linear_model, X_train_scaled, y_train)

# Mostrar resultados
print("\nüîπ Criterios de informaci√≥n para selecci√≥n de modelos:")
print(f"{'-'*32}")
print(f"{'Criterio':<10} {'ElasticNet':<12} {'Linear':<12}")
print(f"{'-'*32}")
print(f"{'AIC':<10} {aic_elasticnet:<12.2f} {aic_linear:<12.2f}")
print(f"{'BIC':<10} {bic_elasticnet:<12.2f} {bic_linear:<12.2f}")

"""Los mejores hiperpar√°metros obtenidos mediante validaci√≥n cruzada fueron:
*   Alpha: 0.0215
*   L1_ratio: 0.10

Este valor sugiere que el modelo se apoya principalmente en la penalizaci√≥n tipo Ridge (90%), manteniendo una peque√±a contribuci√≥n Lasso (10%). Este equilibrio favorece la estabilidad del modelo sin perder capacidad de selecci√≥n de variables.

Adem√°s, `ElasticNet` retuvo las 7 variables utilizadas, al igual que el modelo Lasso, lo cual indica que todas las caracter√≠sticas aportan informaci√≥n relevante para la predicci√≥n.

Por lo que hace al desempe√±o del modelo, este fue comparado con Lasso y la regresi√≥n lineal est√°ndar. Los resultados fueron los siguientes:

  <table border="1">
    <tr>
      <th>M√©trica</th>
      <th>ElasticNet</th>
      <th>Lasso</th>
      <th>Linear</th>
    </tr>
    <tr>
      <td>MSE</td>
      <td>3.5569</td>
      <td>3.5797</td>
      <td>3.5806</td>
    </tr>
    <tr>
      <td>RMSE</td>
      <td>1.8860</td>
      <td>1.8920</td>
      <td>1.8922</td>
    </tr>
    <tr>
      <td>R¬≤</td>
      <td>0.6089</td>
      <td>0.6064</td>
      <td>0.6063</td>
    </tr>
    <tr>
      <td>MAE</td>
      <td>1.3697</td>
      <td>1.3727</td>
      <td>1.3732</td>
    </tr>
  </table>

`ElasticNet` mostr√≥ una mejora marginal pero consistente en todas las m√©tricas, explicando aproximadamente un 61% de la variabilidad en los niveles de urea. Esta ligera superioridad sugiere que la combinaci√≥n de regularizaciones mejora la capacidad predictiva sin incrementar el sobreajuste.

En t√©rminos de criterios de informaci√≥n, los resultados fueron similares:
  <table border="1">
    <tr>
      <th></th>
      <th>ElasticNet</th>
      <th>Linear</th>
    </tr>
    <tr>
      <td>AIC</td>
      <td>1183.07</td>
      <td>1182.72</td>
    </tr>
    <tr>
      <td>BIC</td>
      <td>1213.22</td>
      <td>1212.87</td>
    <tr>
  </table>

Aunque los valores son muy cercanos, `ElasticNet` ofrece un equilibrio m√°s robusto entre ajuste y penalizaci√≥n.

**An√°lisis de coeficientes:**

Las variables m√°s influyentes en `ElasticNet`, seg√∫n el valor absoluto de sus coeficientes, fueron:
*   `log_sc` (1.60): es el mejor indicador de los niveles de urea en la sangre.
*   `classification_ckd` (-0.51): fuerte relaci√≥n negativa con la variable objetivo.
*   `sqrt_hemo` (-0.47): refleja posible impacto de la anemia.
*   `sqrt_al` (0.33): la alb√∫mina elevada se asocia positivamente con la severidad de la enfermedad.

Los signos y magnitudes de los coeficientes presentan una coherencia cl√≠nica y fisiopatol√≥gica con la enfermedad renal, reforzando la interpretabilidad del modelo.

**An√°lisis visual y diagn√≥stico de residuales:**

*   Distribuci√≥n de residuales: aproximadamente normal, con media cercana a 0 (0.07) y desviaci√≥n est√°ndar de 1.88. La curtosis (3.77) sugiere colas ligeramente m√°s pesadas que la distribuci√≥n normal.
*   Q-Q plot: los residuales siguen en gran medida la l√≠nea te√≥rica, aunque se detectan leves desviaciones en los valores extremos negativos.
*   Valores reales vs. predichos: se observa un buen ajuste general, con ligera tendencia a la sobrepredicci√≥n en valores altos.

El an√°lisis gr√°fico refuerza la calidad del ajuste y la estabilidad del modelo frente a variaciones en los datos.

**Conclusiones:**

`ElasticNet` demostr√≥ ser un modelo ligeramente superior a Lasso y a la regresi√≥n lineal tradicional, tanto en t√©rminos de precisi√≥n como de estabilidad. Su capacidad para mantener todas las variables relevantes, evitar el sobreajuste y ofrecer interpretabilidad cl√≠nica lo convierten en una alternativa s√≥lida.

En resumen:
*   La regularizaci√≥n mixta aporta flexibilidad y robustez.
*   Las variables seleccionadas son cl√≠nicamente significativas.
*   El modelo mantiene un buen equilibrio entre ajuste y simplicidad, siendo apto para aplicaciones en contextos cl√≠nicos donde se requiere tanto precisi√≥n como interpretabilidad.

#### 5.5.2. Regresi√≥n polin√≥mica

Una vez implementados modelos lineales como `ElasticNet`, puede resultar √∫til investigar si ciertas relaciones entre las variables predictoras y la variable objetivo presentan comportamientos no lineales.

Para ello, hemos recurrido a la regresi√≥n polin√≥mica, una extensi√≥n del modelo lineal que permite capturar curvaturas y tendencias m√°s complejas en los datos. Esta es una t√©cnica que modela la relaci√≥n entre la variable independiente y la dependiente como un polinomio de grado n. En nuestro caso, hemos seleccionado un polinomio de segundo grado (cuadr√°tico) para detectar posibles curvaturas suaves en la relaci√≥n entre las variables predictoras y la respuesta transformada "sqrt_bu".

El siguiente fragmento de c√≥digo genera una serie de gr√°ficos de regresi√≥n polin√≥mica que exploran la relaci√≥n entre cada variable predictora num√©rica y la variable objetivo "sqrt_bu". Esto permite detectar visualmente patrones curvil√≠neos que podr√≠an justificar el uso de transformaciones no lineales o modelos m√°s flexibles en etapas posteriores del an√°lisis.
"""

plt.figure(figsize=(12, 10))

plt.suptitle("An√°lisis de regresi√≥n polinomial", fontsize=16, fontweight='bold')

# Generar gr√°ficos de regresi√≥n polinomial para cada predictor num√©rico frente a "sqrt_bu"
for i, predictor in enumerate(numeric_predictors_2, 1):
    plt.subplot(5, 2, i)
    sns.regplot(x=encoded_kidney_df[predictor], y=encoded_kidney_df["sqrt_bu"],
                order=2, scatter_kws={"alpha": 0.5})
    plt.title(f"{predictor} vs sqrt_bu", fontweight='bold', pad=15)

plt.tight_layout()
plt.subplots_adjust(top=0.9, hspace=0.7, wspace=0.3)
plt.show()

"""Aunque este enfoque ofrece una visi√≥n preliminar √∫til, tiene varias limitaciones cuando las interacciones entre variables son complejas o multidimensionales, ya que requerir√≠a combinar manualmente distintas variables y probar m√∫ltiples transformaciones. Modelar cada relaci√≥n individualmente no captura interacciones entre variables y puede resultar tedioso y redundante cuando se tienen muchas variables predictoras.

#### 5.5.3. Extreme Gradient Boosting (XGBoost)

Tras evaluar los anteriores pasos realizados, nos ha surgido una reflexi√≥n importante: aunque algunos patrones curvil√≠neos pueden ser detectables visualmente, muchas relaciones entre variables predictoras y la respuesta pueden ser m√°s complejas o estar determinadas por interacciones entre m√∫ltiples variables. En este contexto, hemos considerado que una alternativa poderosa y flexible es el uso de modelos basados en √°rboles de decisi√≥n, como XGBoost (Extreme Gradient Boosting).

Este es un potente algoritmo de aprendizaje supervisado basado en el m√©todo de boosting de gradiente. Se trata de un modelo predictivo no lineal, que combina m√∫ltiples √°rboles de decisi√≥n "d√©biles" (poco profundos) para formar un modelo "fuerte" a trav√©s de un proceso iterativo de correcci√≥n de errores.

Adem√°s, a diferencia de los modelos lineales tradicionales, XGBoost no requiere suposiciones estrictas sobre la distribuci√≥n de los datos o sobre la forma funcional de las relaciones entre variables. Esto lo hace especialmente √∫til en contextos como el nuestro, donde la relaci√≥n entre indicadores cl√≠nicos y la variable objetivo "sqrt_bu" podr√≠a estar gobernada por m√∫ltiples factores no lineales y combinaciones de atributos.

En el c√≥digo que tenemos a continuaci√≥n entrenamos un modelo XGBoost para predecir los niveles de "sqrt_bu" utilizando una combinaci√≥n de variables cl√≠nicas num√©ricas y categ√≥ricas. Posteriormente, evaluamos el desempe√±o del modelo mediante m√∫ltiples m√©tricas (MSE, RMSE, MAE, R¬≤) y visualizaciones diagn√≥sticas de los residuales, as√≠ como la interpretaci√≥n de los predictores clave mediante gr√°ficos de importancia y valores SHAP.
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from scipy import stats
from statsmodels.nonparametric.smoothers_lowess import lowess
import xgboost as xgb

# Escalar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Encontrar los mejores hiperpar√°metros para XGBoost mediante validaci√≥n cruzada
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

xgb_cv = GridSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True,
    n_jobs=-1
)
xgb_cv.fit(X_train_scaled, y_train)

# Mostrar resultados de la validaci√≥n cruzada
cv_results = pd.DataFrame(xgb_cv.cv_results_)
cv_results = cv_results[['params', 'mean_test_score', 'mean_train_score']]
cv_results['mean_test_score'] = -cv_results['mean_test_score']  # Convertir a MSE positivo
cv_results['mean_train_score'] = -cv_results['mean_train_score']  # Convertir a MSE positivo
print("üîπ Resultados de validaci√≥n cruzada para diferentes hiperpar√°metros:")
print(cv_results.sort_values('mean_test_score').head(5))  # Mostrar los 5 mejores conjuntos de par√°metros

# Obtener los mejores hiperpar√°metros
best_params = xgb_cv.best_params_
print(f"\nüîπ Mejores hiperpar√°metros: {best_params}")

# Entrenar el modelo XGBoost con los mejores hiperpar√°metros
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    random_state=42,
    **best_params
)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb = xgb_model.predict(X_test_scaled)

# Tambi√©n entrenar el modelo de regresi√≥n lineal para comparaci√≥n
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train)
y_pred_linear = linear_model.predict(X_test_scaled)

# Calcular residuales para ambos modelos
residuals_xgb = y_test - y_pred_xgb
residuals_linear = y_test - y_pred_linear

# Calcular diversas m√©tricas de rendimiento para ambos modelos
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)

mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_linear)
mae_linear = mean_absolute_error(y_test, y_pred_linear)

# Obtener la importancia de caracter√≠sticas de XGBoost
feature_importance = xgb_model.feature_importances_
total_features = len(feature_importance)
important_features = np.sum(feature_importance > 0)
print(f"\nüîπ Caracter√≠sticas importantes: {important_features} de {total_features} caracter√≠sticas")

print("\nüîπ Comparaci√≥n de m√©tricas:")
print(f"{'-'*32}")
print(f"{'M√©trica':<10} {'XGBoost':<12} {'Linear':<12}")
print(f"{'-'*32}")
print(f"{'MSE':<10} {mse_xgb:<12.4f} {mse_linear:<12.4f}")
print(f"{'RMSE':<10} {rmse_xgb:<12.4f} {rmse_linear:<12.4f}")
print(f"{'R¬≤':<10} {r2_xgb:<12.4f} {r2_linear:<12.4f}")
print(f"{'MAE':<10} {mae_xgb:<12.4f} {mae_linear:<12.4f}")

# Analizar importancia de caracter√≠sticas
importance_df = pd.DataFrame({
    'Caracter√≠stica': X_train.columns,
    'Importancia': xgb_model.feature_importances_
})
importance_df = importance_df.sort_values('Importancia', ascending=False)

print("\nüîπ Top 10 caracter√≠sticas por importancia (XGBoost):")
print(importance_df.head(10))
print("\n")

# Configura un estilo visual consistente con una paleta de colores
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'figure.figsize': (12, 8),
    'figure.dpi': 100,
    'axes.grid': True,
    'grid.alpha': 0.3
})

# Definir una paleta de colores cohesiva
colors = {
    'primary': '#1f77b4',
    'secondary': '#ff7f0e',
    'tertiary': '#2ca02c',
    'negative': '#d62728',
    'neutral': '#7f7f7f',
    'background': '#f9f9f9',
    'grid': '#e0e0e0',
    'xgboost': '#9c2ca0',  # Color para XGBoost
    'linear': '#184e77'
}

# Crear un panel con un aspecto limpio para comparar XGBoost vs Linear
fig = plt.figure(figsize=(14, 12), facecolor=colors['background'])
fig.suptitle('Comparaci√≥n de XGBoost vs regresi√≥n lineal para enfermedad renal',
             fontsize=18, fontweight='bold', y=1, color='#000000')

# A√±adir un subt√≠tulo ligero con contexto adicional
plt.figtext(0.5, 0.96, 'Predicci√≥n de los valores de urea en sangre mediante indicadores cl√≠nicos con XGBoost',
            ha='center', fontsize=12, color='#000000')

# Crear una especificaci√≥n de cuadr√≠cula personalizada para un mejor dise√±o
gs = fig.add_gridspec(3, 6)

# Panel 1: Gr√°fico real vs. predicho (ambos modelos)
ax1 = fig.add_subplot(gs[0, :3])
ax1.set_facecolor(colors['background'])
scatter_linear = ax1.scatter(y_test, y_pred_linear, alpha=0.6, s=50,
                     c=colors['linear'], edgecolor='white', linewidth=0.5, label='Regresi√≥n Lineal')
scatter_xgb = ax1.scatter(y_test, y_pred_xgb, alpha=0.6, s=50,
                     c=colors['xgboost'], edgecolor='white', linewidth=0.5, label='XGBoost')
ax1.plot([min(y_test.min(), min(y_pred_linear.min(), y_pred_xgb.min())),
          max(y_test.max(), max(y_pred_linear.max(), y_pred_xgb.max()))],
         [min(y_test.min(), min(y_pred_linear.min(), y_pred_xgb.min())),
          max(y_test.max(), max(y_pred_linear.max(), y_pred_xgb.max()))],
         '--', color=colors['neutral'], linewidth=1.5, label='Predicci√≥n perfecta')
ax1.set_xlabel('Valor real de urea en sangre (transformado en ra√≠z cuadrada)', labelpad=12)
ax1.set_ylabel('Valor previsto de urea en sangre', labelpad=12)
ax1.set_title('Valores reales vs. valores previstos', pad=15, fontweight='bold')
ax1.legend(frameon=True, framealpha=0.9, loc='lower right')

# Cree un cuadro de texto con m√©tricas del modelo
metrics_text = (f"M√©tricas XGBoost:\n"
                f"MSE: {mse_xgb:.2f}            RMSE: {rmse_xgb:.2f}\n"
                f"MAE: {mae_xgb:.2f}            R¬≤: {r2_xgb:.4f}\n\n"
                f"M√©tricas Linear:\n"
                f"MSE: {mse_linear:.2f}           RMSE: {rmse_linear:.2f}\n"
                f"MAE: {mae_linear:.2f}           R¬≤: {r2_linear:.4f}")
props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=colors['neutral'])
ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, fontsize=8,
         verticalalignment='top', bbox=props)

# Panel 2: Residuales vs. Valores Predichos (ambos modelos)
ax2 = fig.add_subplot(gs[0, 3:])
ax2.set_facecolor(colors['background'])
ax2.scatter(y_pred_linear, residuals_linear, alpha=0.6, s=50,
           c=colors['linear'], edgecolor='white', linewidth=0.5, label='Residuales Linear')
ax2.scatter(y_pred_xgb, residuals_xgb, alpha=0.6, s=50,
           c=colors['xgboost'], edgecolor='white', linewidth=0.5, label='Residuales XGBoost')
ax2.axhline(y=0, color=colors['neutral'], linestyle='-', linewidth=1.5)

# A√±adir suavizado LOWESS para verificar patrones
lowess_y = lowess(residuals_xgb, y_pred_xgb, frac=0.3)
ax2.plot(lowess_y[:, 0], lowess_y[:, 1], color='#5f0f40', linestyle='-', linewidth=2, label='Tendencia XGBoost')

ax2.set_xlabel('Valores predichos', labelpad=12)
ax2.set_ylabel('Residuales', labelpad=12)
ax2.set_title('Residuales vs. valores predichos', pad=15, fontweight='bold')
ax2.legend(frameon=True, framealpha=0.9, loc='lower right')

# Agregar una banda horizontal para mostrar la varianza residual
std_resid_xgb = np.std(residuals_xgb)
ax2.axhspan(-2*std_resid_xgb, 2*std_resid_xgb, alpha=0.1, color=colors['xgboost'])
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)
ax2.text(0.05, 0.05, f"¬±2œÉ XGBoost: {2*std_resid_xgb:.2f}", transform=ax2.transAxes, fontsize=10,
         verticalalignment='bottom', bbox=props)

# Panel 3: Distribuci√≥n de residuales de ambos modelos
ax3 = fig.add_subplot(gs[1, :3])
ax3.set_facecolor(colors['background'])
sns.histplot(residuals_linear, kde=True, ax=ax3, color=colors['linear'], alpha=0.5, label='Residuales Linear')
sns.histplot(residuals_xgb, kde=True, ax=ax3, color=colors['xgboost'], alpha=0.5, label='Residuales XGBoost')
ax3.axvline(x=0, color=colors['neutral'], linestyle='--', linewidth=1.5)
ax3.set_xlabel('Valor residual', labelpad=12)
ax3.set_ylabel('Frecuencia', labelpad=12)
ax3.set_title('Distribuci√≥n de residuales', pad=15, fontweight='bold')

# A√±adir curva de distribuci√≥n normal para comparaci√≥n
x = np.linspace(min(residuals_xgb), max(residuals_xgb), 100)
params = stats.norm.fit(residuals_xgb)
pdf_fitted = stats.norm.pdf(x, *params)
ax3.plot(x, pdf_fitted * len(residuals_xgb) * (max(residuals_xgb) - min(residuals_xgb)) / 10,
                color='#9a031e', linestyle='--', linewidth=2, label='Distribuci√≥n normal')

ax3.legend(frameon=True)

# A√±adir anotaciones sobre la distribuci√≥n
mean_resid_xgb = np.mean(residuals_xgb)
mean_resid_linear = np.mean(residuals_linear)
std_resid_linear = np.std(residuals_linear)
skew_resid = stats.skew(residuals_xgb)
kurt_resid = stats.kurtosis(residuals_xgb)

stats_text = (f"XGBoost:\nMedia: {mean_resid_xgb:.2f}\nStd Dev: {std_resid_xgb:.2f}\n"
              f"Skew: {skew_resid:.2f}\nKurt: {kurt_resid:.2f}\n\n"
              f"Linear:\nMedia: {mean_resid_linear:.2f}\nStd Dev: {std_resid_linear:.2f}")
props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor=colors['neutral'])
ax3.text(0.85, 0.70, stats_text,
         transform=ax3.transAxes, fontsize=8.5,
         horizontalalignment='left', verticalalignment='center', bbox=props)

# Panel 4: Q-Q Plot de XGBoost
ax4 = fig.add_subplot(gs[1, 3:])
ax4.set_facecolor(colors['background'])
sm.qqplot(residuals_xgb, line='45', fit=True, ax=ax4)
ax4.set_xlabel('Cuantiles te√≥ricos', labelpad=12)
ax4.set_ylabel('Residuos estandarizados', labelpad=12)
for item in ax4.get_children():
    if isinstance(item, plt.Line2D):
        if item.get_linestyle() == 'none':
            item.set_color(colors['xgboost'])
            item.set_alpha(0.7)
            item.set_markeredgecolor('white')
            item.set_markeredgewidth(0.5)
            item.set_markersize(5)

ax4.set_title('Q-Q Plot de residuales (XGBoost)', pad=15, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Panel 5: Importancia de las caracter√≠sticas para XGBoost
ax5 = fig.add_subplot(gs[2, :])
ax5.set_facecolor(colors['background'])

# Crear un gr√°fico de barras para la importancia de caracter√≠sticas
importance_plot = importance_df.head(15)  # Tomar las 15 caracter√≠sticas m√°s importantes
sns.barplot(x='Importancia', y='Caracter√≠stica', data=importance_plot, color=colors['xgboost'], ax=ax5)
ax5.set_title('Top 15 caracter√≠sticas por importancia (XGBoost)', pad=15, fontweight='bold')
ax5.set_xlabel('Importancia', labelpad=12)
ax5.set_ylabel('Caracter√≠stica', labelpad=12)

# Agregar valores num√©ricos a las barras
for i, v in enumerate(importance_plot['Importancia']):
    ax5.text(v + 0.01, i, f"{v:.4f}", va='center')

# Ajustar el dise√±o
plt.tight_layout()
plt.subplots_adjust(top=0.9, hspace=0.5, wspace=0.7)
plt.show()

"""Mediante validaci√≥n cruzada, se determinaron los siguientes hiperpar√°metros √≥ptimos:
*   `learning_rate: 0.05` este par√°metro regula la tasa de aprendizaje del modelo, controlando cu√°nto se ajustan los pesos en cada iteraci√≥n. Un valor bajo como 0.05 permite una convergencia m√°s suave y reduce el riesgo de sobreajuste, aunque requiere un mayor n√∫mero de iteraciones para lograr una buena capacidad predictiva.
*   `max_depth: 3` limita la profundidad m√°xima de cada √°rbol individual. Una profundidad reducida restringe la complejidad del modelo, favoreciendo su capacidad de generalizaci√≥n y previniendo el ajuste excesivo a los datos de entrenamiento
*   `n_estimators: 50` representa el n√∫mero total de √°rboles generados mediante boosting. En combinaci√≥n con un learning rate bajo, este n√∫mero de √°rboles resulta suficiente para que el modelo alcance un equilibrio entre sesgo y varianza, sin incurrir en sobreajuste.
*   `colsample_bytree: 1.0` especifica la proporci√≥n de caracter√≠sticas (features) utilizadas al construir cada √°rbol. El valor de 1.0 implica que se consideraron todas las variables disponibles, lo cual maximiza el aprovechamiento de la informaci√≥n presente en los datos sin introducir un sesgo por muestreo.

Estos par√°metros configuran un modelo de complejidad moderada, con buena capacidad de generalizaci√≥n y resistencia al sobreajuste. El modelo alcanz√≥ un mean test score de 2.38, frente a un mean train score de 1.31, lo cual evidencia un equilibrio adecuado entre bias y varianza.

Por lo que hace a los resultados obtenidos, estos indican que XGBoost supera consistentemente a la regresi√≥n lineal en todas las m√©tricas evaluadas:


M√©trica	XGBoost	Lineal	Mejora relativa
MSE	3.2358	3.5806	9.6%
RMSE	1.7988	1.8922	4.9%
R¬≤	0.6442	0.6063	6.3%
MAE	1.3314	1.3732	3.0%
El modelo explica aproximadamente el 64.4% de la variabilidad en los niveles de urea, mejorando el poder explicativo en m√°s de seis puntos porcentuales respecto a la regresi√≥n lineal. Esta ventaja destaca la capacidad del modelo para capturar relaciones no lineales y estructuras de interacci√≥n complejas.

Adem√°s, el an√°lisis de los residuos apoya el buen ajuste del modelo:
*   Media: 0.12
*   Desviaci√≥n est√°ndar: 1.78
*   Asimetr√≠a: -0.46
*   Kurtosis: 1.94

Los residuos est√°n m√°s concentrados en torno a cero, con menor dispersi√≥n que en modelos lineales. El Q-Q plot indica un ajuste razonable a la normalidad, salvo en valores extremos negativos. En el gr√°fico de residuales vs predicci√≥n se observa una distribuci√≥n homog√©nea sin patrones claros, aunque con leve heterocedasticidad. Adem√°s, el 95% de los residuales se encuentran dentro de ¬±2 desviaciones est√°ndar (¬±3.59), lo cual respalda la robustez del modelo.

**Importancia de las variables:**

XGBoost identific√≥ como m√°s relevantes las siguientes variables:


Variable	Importancia
log_sc	0.57
sqrt_al	0.18
sqrt_hemo	0.11
pc_abnormal	0.05
log_pot	0.05
classification_ckd	0.04
ane_no	0.00
El modelo seleccion√≥ autom√°ticamente 6 de las 7 variables, descartando "ane_no" por su escasa relevancia predictiva. A diferencia de los modelos lineales, XGBoost no proporciona informaci√≥n sobre la direcci√≥n del efecto (positivo o negativo), sino sobre su contribuci√≥n relativa a las predicciones.

Interpretaci√≥n Cl√≠nica
Desde una perspectiva cl√≠nica, los resultados del modelo son coherentes con los mecanismos fisiopatol√≥gicos subyacentes:

log_sc (0.57): es el predictor m√°s fuerte de la urea, lo que refleja su papel como marcador clave de la funci√≥n renal.
sqrt_al (0.18): se asocia positivamente con la gravedad de la enfermedad renal.

sqrt_hemo (hemoglobina) subraya la relevancia de los niveles bajos de hemoglobina como manifestaci√≥n de anemia en pacientes con ERC.

Las variables bioqu√≠micas mostraron mayor influencia que las clasificaciones cl√≠nicas binarias, lo que resalta la utilidad de medidas continuas y cuantificables en la predicci√≥n cl√≠nica.

Conclusiones del An√°lisis con XGBoost
XGBoost demostr√≥ ser la alternativa m√°s precisa entre los modelos evaluados, con mejoras sustanciales en todas las m√©tricas y un ajuste m√°s refinado de los residuos. Su capacidad para modelar relaciones no lineales, manejar valores at√≠picos y seleccionar autom√°ticamente las variables m√°s relevantes lo convierte en una herramienta potente en contextos cl√≠nicos donde se prioriza la precisi√≥n.

Aunque su interpretaci√≥n es menos directa que en modelos lineales, la coherencia de las variables seleccionadas y su relevancia cl√≠nica refuerzan la viabilidad de XGBoost en la predicci√≥n de marcadores de enfermedad renal. Por tanto, este modelo representa una opci√≥n robusta y efectiva para apoyar decisiones m√©dicas basadas en datos, especialmente en aplicaciones donde la complejidad de las relaciones entre variables puede limitar el desempe√±o de los enfoques tradicionales.

Para una comparaci√≥n m√°s exhaustiva entre modelos, especialmente en contextos donde los extremos del rango de valores pueden ser cl√≠nicamente relevantes, se propone un an√°lisis basado en la evaluaci√≥n por cuartiles y en el comportamiento de los residuos. Esta estrategia permite no solo medir el rendimiento global de un modelo, sino tambi√©n su consistencia local a lo largo de diferentes segmentos del rango de predicci√≥n.

*   Como primer paso, se aplica una validaci√≥n cruzada de 5 pliegues, donde:
    *   Se entrena y eval√∫a cada modelo cinco veces en distintos subconjuntos de los datos.
    *   Se promedian los resultados para obtener una estimaci√≥n m√°s robusta del rendimiento general.

 Este procedimiento ayuda a reducir el sesgo asociado a una sola partici√≥n de entrenamiento/prueba, y da una mejor idea del comportamiento del modelo frente a datos no vistos.

*   A continuaci√≥n, se analizan tres aspectos clave:
    *   Error medio cuadr√°tico (MSE) con desviaci√≥n est√°ndar, para comparar precisi√≥n promedio.
    *   Matriz de confusi√≥n por cuartiles, para evaluar qu√© tan bien los modelos capturan distintos niveles de la variable objetivo.
    *   Distribuci√≥n de errores (residuos) por cuartiles, con estad√≠sticas resumen y visualizaci√≥n mediante diagramas de caja.
"""

print(f"{'-'*50}")
print(" üìä AN√ÅLISIS ADICIONAL DE COMPARACI√ìN DE MODELOS")
print(f"{'-'*50}"+"\n")

# 1. Validaci√≥n cruzada para ambos modelos
from sklearn.model_selection import cross_val_score

# Definir los modelos sin ajustar
xgb_model_cv = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)
linear_model_cv = LinearRegression()

# Realizar validaci√≥n cruzada con 5 pliegues
cv_results_xgb = cross_val_score(xgb_model_cv, X_train_scaled, y_train, cv=5,
                                scoring='neg_mean_squared_error')
cv_results_linear = cross_val_score(linear_model_cv, X_train_scaled, y_train, cv=5,
                                   scoring='neg_mean_squared_error')

# Convertir a MSE positivo
cv_mse_xgb = -cv_results_xgb
cv_mse_linear = -cv_results_linear

# Mostrar resultados de validaci√≥n cruzada
print("üîπ Validaci√≥n cruzada (5-fold):")
print(f"XGBoost - MSE medio: {cv_mse_xgb.mean():.4f},     Desv. est√°ndar: {cv_mse_xgb.std():.4f}")
print(f"Linear  - MSE medio: {cv_mse_linear.mean():.4f},      Desv. est√°ndar: {cv_mse_linear.std():.4f}")

# 2. Matriz de confusi√≥n de cuartiles
# Esta t√©cnica compara c√≥mo se sit√∫an las predicciones en cuartiles respecto a los valores reales
def quartile_confusion_matrix(y_true, y_pred):
    # Calcular cuartiles
    q_true = pd.qcut(y_true, 4, labels=False)
    q_pred = pd.qcut(y_pred, 4, labels=False)

    # Crear matriz de confusi√≥n
    conf_matrix = pd.crosstab(q_true, q_pred,
                             rownames=['Real (Cuartil)'],
                             colnames=['Predicho (Cuartil)'])

    # Calcular precisi√≥n por cuartil
    accuracy = np.sum(q_true == q_pred) / len(q_true)

    return conf_matrix, accuracy

# Calcular matrices de confusi√≥n de cuartiles
conf_matrix_xgb, acc_xgb = quartile_confusion_matrix(y_test, y_pred_xgb)
conf_matrix_linear, acc_linear = quartile_confusion_matrix(y_test, y_pred_linear)

print("\nüîπ Matriz de confusi√≥n de cuartiles (XGBoost):")
print(conf_matrix_xgb)
print(f"Precisi√≥n de cuartiles (XGBoost): {acc_xgb:.4f}")

print("\nüîπ Matriz de confusi√≥n de cuartiles (Linear):")
print(conf_matrix_linear)
print(f"Precisi√≥n de cuartiles (Linear): {acc_linear:.4f}")

# 3. An√°lisis residual por cuartiles
def residual_analysis_by_quartile(y_true, y_pred):
    # Crear DataFrame con valores reales, predicciones y residuos
    df_analysis = pd.DataFrame({
        'real': y_true,
        'pred': y_pred,
        'residual': y_true - y_pred
    })

    # Asignar cuartiles basados en valores reales
    df_analysis['quartile'] = pd.qcut(df_analysis['real'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])

    # Calcular estad√≠sticas por cuartil
    quartile_stats = df_analysis.groupby('quartile')['residual'].agg(['mean', 'std', 'median'])

    return quartile_stats

# Realizar an√°lisis residual por cuartiles
res_quartile_xgb = residual_analysis_by_quartile(y_test, y_pred_xgb)
res_quartile_linear = residual_analysis_by_quartile(y_test, y_pred_linear)

print("\nüîπ An√°lisis residual por cuartiles (XGBoost):")
print(res_quartile_xgb)

print("\nüîπ An√°lisis residual por cuartiles (Linear):")
print(res_quartile_linear)
print("\n")

# 4. Diagrama de cajas comparativo de residuos por cuartiles
plt.figure(figsize=(12, 6))

# Crear DataFrames para la visualizaci√≥n
df_resid_xgb = pd.DataFrame({
    'real': y_test,
    'residual': residuals_xgb,
    'Modelo': 'XGBoost'
})
df_resid_linear = pd.DataFrame({
    'real': y_test,
    'residual': residuals_linear,
    'Modelo': 'Linear'
})
df_combined = pd.concat([df_resid_xgb, df_resid_linear])
df_combined['quartile'] = pd.qcut(df_combined['real'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])

# Definir colores para los modelos
colors = {'XGBoost': '#9c2ca0', 'Linear': '#184e77'}

# Crear boxplot
sns.boxplot(x='quartile', y='residual', hue='Modelo', data=df_combined, palette=[colors['XGBoost'], colors['Linear']])
plt.axhline(y=0, color='gray', linestyle='--')
plt.title('Comparaci√≥n de la distribuci√≥n de residuos por cuartiles de valores reales', fontsize=18, fontweight='bold', pad=20)
plt.xlabel('Cuartil del valor real', fontsize=12, labelpad=12)
plt.ylabel('Residuo', fontsize=12, labelpad=12)
plt.legend(title="Modelo", frameon=True, fancybox=True, loc='lower right')
plt.tight_layout()
plt.show()

"""Observando los resultados obtenido podemos concluir lo siguiente:

**Validaci√≥n cruzada (desempe√±o general):**

En t√©rminos de MSE obtenido a partir de una validaci√≥n cruzada de 5 pliegues, el modelo lineal presenta un valor medio de 2.3460 con una desviaci√≥n est√°ndar de 0.4882, lo que indica un rendimiento general s√≥lido y estable. Por otro lado, XGBoost obtiene un MSE medio de 3.0744 con una desviaci√≥n est√°ndar ligeramente inferior (0.3945), lo que sugiere un comportamiento menos preciso en promedio, pero tambi√©n consistente. No obstante, el menor error cuadr√°tico no necesariamente se traduce en un mejor comportamiento en todos los segmentos del rango de predicci√≥n, lo cual se analiza a continuaci√≥n.

**Precisi√≥n por cuartiles:**

Observamos que la precisi√≥n de clasificaci√≥n por cuartiles es ligeramente superior en el modelo lineal (0.5750) frente a XGBoost (0.5375). Sin embargo, es importante destacar que XGBoost muestra un mejor equilibrio en los errores de predicci√≥n a lo largo de los distintos cuartiles, lo cual es esperable debido a su capacidad para modelar relaciones no lineales y detectar interacciones entre variables.

Por ejemplo, mientras el modelo lineal tiende a mantener una buena precisi√≥n en el cuartil superior (Q4: 16 casos correctamente clasificados), presenta cierta confusi√≥n en los cuartiles intermedios (Q2 y Q3). XGBoost, en cambio, muestra una distribuci√≥n m√°s homog√©nea en la matriz de confusi√≥n, aunque con ligeros sacrificios en la precisi√≥n total.

**Distribuci√≥n de residuos por cuartiles:**

El an√°lisis de residuos por cuartiles refuerza estos hallazgos. Ambos modelos tienden a subestimar sistem√°ticamente los valores bajos (Q1) y sobreestimar los valores altos (Q4), lo cual se evidencia en los valores promedio de los errores. No obstante, los residuos del modelo XGBoost son menos extremos en t√©rminos de desviaci√≥n est√°ndar. Esto puede indicar que XGBoost realiza predicciones m√°s centradas y menos sensibles a valores at√≠picos.

**Conclusi√≥n general:**

Ambos modelos presentan fortalezas complementarias. La regresi√≥n lineal destaca por su simplicidad y precisi√≥n general. No obstante, XGBoost ofrece una alternativa m√°s robusta frente a patrones no lineales y relaciones complejas entre variables, lo cual se traduce en un comportamiento m√°s equilibrado en los extremos de la distribuci√≥n.

### 5.6. Conclusi√≥n comparativa de los modelos evaluados

En este estudio se evaluaron diversos modelos de regresi√≥n para predecir `sqrt_bu`, con el objetivo de mejorar la normalidad de la distribuci√≥n y facilitar el ajuste del modelo. La variable tiene una importancia m√©dica reconocida en el diagn√≥stico y seguimiento de la ERC.

A lo largo del apartado 5 se compararon enfoques de regresi√≥n lineal cl√°sica (Regresi√≥n Lineal, Ridge, Lasso, ElasticNet) y modelos no lineales basados en √°rboles (XGBoost). Para valorar su rendimiento global, se utilizaron m√©tricas de error (MSE, RMSE, MAE), el coeficiente de determinaci√≥n (R¬≤), criterios de informaci√≥n (AIC y BIC cuando aplicable), as√≠ como validaci√≥n cruzada y an√°lisis por cuartiles de la variable objetivo.

| Modelo      | MSE    | RMSE   | MAE    | R¬≤     | AIC      | Precisi√≥n Cuartiles |
|-------------|--------|--------|--------|--------|----------|----------------------|
| Linear      | 3.5806 | 1.8922 | 1.3732 | 0.6063 | 1182.72  | 0.5750               |
| Ridge       | 3.5551 | 1.8855 | 1.3705 | 0.6091 | ‚Äî        | ‚Äî                    |
| Lasso       | 3.5797 | 1.8920 | 1.3727 | 0.6064 | 1182.72  | ‚Äî                    |
| ElasticNet  | 3.5569 | 1.8860 | 1.3697 | 0.6089 | ‚Äî        | ‚Äî                    |
| XGBoost     | 3.2358 | 1.7988 | 1.3314 | 0.6442 | ‚Äî        | 0.5375               |

En validaci√≥n cruzada (5-fold), XGBoost obtuvo un MSE medio de 2.3782 (œÉ = 0.4224), mientras que la regresi√≥n lineal tuvo un MSE medio de 2.3460 (œÉ = 0.4882), lo que indica un desempe√±o comparable en cuanto a capacidad de generalizaci√≥n.

Por lo que respecta al an√°lisis por tipo de modelo:
*   Modelos lineales y regularizados: los modelos lineales cl√°sicos y sus variantes regularizadas mostraron un rendimiento muy similar, con una diferencia m√≠nima en error y capacidad predictiva. Ridge y ElasticNet destacaron con una ligera mejora en MSE y MAE respecto al modelo lineal simple, lo que sugiere un mejor control de la varianza y mayor estabilidad sin sacrificar interpretabilidad. Estos modelos resultan apropiados cuando se prioriza la transparencia.
*   XGBoost: este modelo present√≥ la mejor capacidad predictiva global, con el menor MSE (3.2358) y mayor R¬≤ (0.6442). Tambi√©n obtuvo el menor MAE (1.3314), indicando mayor precisi√≥n media. Sin embargo, su precisi√≥n en clasificaci√≥n por cuartiles fue ligeramente menor que la del modelo lineal (0.5375 vs. 0.5750), lo cual podr√≠a atribuirse a cierta sobrepredicci√≥n en los valores intermedios. A pesar de ello, el an√°lisis residual mostr√≥ un comportamiento m√°s equilibrado en todos los cuartiles, reforzando su robustez global.

**Conclusi√≥n final:**

Aunque XGBoost se posiciona como el modelo con mejor desempe√±o cuantitativo, las diferencias con los modelos lineales fueron relativamente peque√±as, lo cual destaca la efectividad del preprocesamiento.

Por tanto, podriamos conluir que:
*   XGBoost es recomendable cuando se busca m√°xima precisi√≥n predictiva, especialmente si se cuenta con capacidad computacional suficiente y herramientas adecuadas para interpretar modelos complejos.
*   ElasticNet y Ridge son alternativas robustas, con excelente equilibrio entre rendimiento, interpretabilidad y simplicidad. Resultan especialmente adecuados en contextos donde la transparencia del modelo es prioritaria.

## 6. Clasificaci√≥n

### 6.1. An√°lisis de selecci√≥n de caracter√≠sticas

Con el objetivo de mejorar la precisi√≥n del modelo de clasificaci√≥n y reducir la complejidad del conjunto de datos, hemos llevado a cabo un an√°lisis exhaustivo de selecci√≥n de caracter√≠sticas. Esta etapa es clave para eliminar variables redundantes o irrelevantes, mejorar la interpretabilidad del modelo y minimizar el riesgo de sobreajuste.

Para ello, hemos implementado dos enfoques complementarios de selecci√≥n de variables:
1. ANOVA F-test (an√°lisis de varianza): es un m√©todo estad√≠stico que eval√∫a qu√© tan significativamente var√≠an los valores de una caracter√≠stica num√©rica entre las distintas clases de la variable objetivo. Para cada caracter√≠stica se calcula:
    *   F-score: mide la capacidad discriminativa de la variable. Cuanto mayor sea, mayor es su capacidad para separar las clases.
    *   p-value: eval√∫a la significancia estad√≠stica de esa variaci√≥n. Valores bajos (p < 0.05) indican una diferencia significativa entre grupos.

 Para implementar este m√©todo, hemos utilizado `SelectKBest` junto con la funci√≥n `f_classif` del paquete `Scikit-learn`, aplic√°ndolo √∫nicamente al conjunto de entrenamiento para evitar fugas de informaci√≥n. El resultado es un ranking completo de caracter√≠sticas seg√∫n su F-score, del cual se extraen las 20 variables m√°s relevantes desde una perspectiva puramente estad√≠stica.

2. Importancia de caracter√≠sticas con Random Forest: este segundo enfoque se basa en el modelo de Random Forest, que permite estimar la importancia de cada variable a partir de su impacto en la capacidad predictiva del modelo. Es decir, mide cu√°nto disminuye el error del modelo cuando una caracter√≠stica espec√≠fica est√° presente.

 Se entrena un `RandomForestClassifier` sobre el conjunto de entrenamiento, y se extraen las importancias de cada caracter√≠stica utilizando el atributo `feature_importances_`. Estas m√°s tarde son ordenadas para identificar las 20 variables que mayor influencia ejercen sobre la predicci√≥n.

 A diferencia del ANOVA, este enfoque tiene la ventaja de capturar relaciones no lineales e interacciones entre variables, que pueden pasar desapercibidas con m√©todos estad√≠sticos cl√°sicos.

Adem√°s, antes de aplicar ambos m√©todos de selecci√≥n, hemos llevado a cabo un cuidadoso proceso de limpieza y depuraci√≥n de variables:
*   Se eliminan columnas transformadas y variables altamente correlacionadas.
*   Esto filtrado evita duplicidades, reduciendo la multicolinealidad y facilitando una evaluaci√≥n m√°s clara de las variables originales.

El resultado que obtenemos es un conjunto de caracter√≠sticas m√°s conciso (`X_filtered`), que incluye √∫nicamente las variables independientes limpias. La variable dependiente corresponde con la columna `classification_ckd`.
"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

def plot_feature_importance(importance_df, title, top_n=20, fontweight=None, pad=None):
    """Representar la importancia de las caracter√≠sticas en un gr√°fico de barras horizontales"""

    plt.figure(figsize=(8, 6))
    sns.barplot(x='Importance', y='Feature',
                data=importance_df.head(top_n),
                palette='viridis')
    plt.title(title, fontweight=fontweight, pad=pad)
    plt.tight_layout()
    plt.show()

def feature_selection_analysis(X, y, k_features=20, n_estimators=100, random_state=42):
    """
    Realiza un an√°lisis exhaustivo de selecci√≥n de caracter√≠sticas mediante:
    1. Prueba F de ANOVA.
    2. Importancia de las caracter√≠sticas de Random Forest.

    Devuelve DataFrames con la clasificaci√≥n de las caracter√≠sticas de ambos m√©todos.
    """

    # Dividir datos para evitar fugas de datos en la selecci√≥n de funciones
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state)

    # 1. Prueba F de ANOVA para selecci√≥n de caracter√≠sticas
    print("\n" + "="*58)
    print("üìù Selecci√≥n de caracter√≠sticas de la prueba F de ANOVA")
    print("="*58)

    selector = SelectKBest(f_classif, k='all')  # Obtener puntuaciones para todas las funciones
    selector.fit(X_train, y_train)

    # Crear un DataFrame de datos con los resultados de la prueba F de ANOVA
    anova_results = pd.DataFrame({
        'Feature': X.columns,
        'F_Score': selector.scores_,
        'p_Value': selector.pvalues_
    }).sort_values('F_Score', ascending=False)

    print(f"\nüîπ Top {k_features} caracter√≠sticas seg√∫n la prueba F de ANOVA:")
    print(anova_results.head(k_features))

    # 2. Importancia de las caracter√≠sticas de Random Forest
    print("\n" + "="*56)
    print("üìù Importancia de las caracter√≠sticas de Random Forest")
    print("="*56)

    rf = RandomForestClassifier(n_estimators=n_estimators,
                               random_state=random_state)
    rf.fit(X_train, y_train)

    # Crear un DataFrame de importancia de caracter√≠sticas
    rf_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf.feature_importances_
    }).sort_values('Importance', ascending=False)

    print(f"\nüîπ Top {k_features} caracter√≠sticas seg√∫n Random Forest:")
    print(rf_importance.head(k_features))
    print("\n")

    # Visualizaci√≥n - Fixed to include the fontweight and pad parameters
    plot_feature_importance(anova_results.rename(columns={'F_Score': 'Importance'}),
                          'Top caracter√≠sticas seg√∫n la puntuaci√≥n F del ANOVA', fontweight='bold', pad=15)
    print("\n")
    plot_feature_importance(rf_importance,
                          'Top caracter√≠sticas seg√∫n la importancia de Random Forest', fontweight='bold', pad=15)

    return anova_results, rf_importance

# Hacer una copia del DataFrame de datos original
encoded_kidney_df_copy = encoded_kidney_df.copy()

# Lista de columnas a eliminar
columns_to_drop = [
    'log_bu', 'bu', 'win_bu', 'sqrt_sc', 'win_sc', 'sc', 'win_pcv', 'pcv', 'sqrt_pcv', 'hemo', 'win_hemo','log_hemo',
                                         'log_rc', 'sqrt_rc', 'rc', 'al', 'win_al', 'log_sg', 'sqrt_sg', 'sg','log_age', 'win_age', 'sqrt_age', 'sod', 'sqrt_sod',
                                         'su', 'win_su', 'log_sod', 'sqrt_sod','pot','win_pot','sqrt_pot','sqrt_bp', 'bp',
                                         'log_al','win_bgr', 'wc', 'sqrt_wc', 'log_wc','bgr','log_bp','sqrt_bgr'
]

# Filtrar el DataFrame para eliminar las columnas especificadas
X_filtered = encoded_kidney_df_copy.drop(['classification_ckd'] + columns_to_drop, axis=1, errors='ignore')
y = encoded_kidney_df_copy['classification_ckd']

# Imprimir las columnas restantes para verificaci√≥n
print("üîπ Caracter√≠sticas restantes despu√©s de eliminar las columnas transformadas:")
print(X_filtered.columns.tolist())
print(f"\nüîπ N√∫mero de caracter√≠sticas: {X_filtered.shape[1]}")

"""Una vez completado el proceso de depuraci√≥n del conjunto de datos, contamos con un subconjunto optimizado de variables independientes (`X_filtered`) y la variable objetivo (`y`), que indica la presencia o ausencia de enfermedad renal cr√≥nica.

Con estos datos, estamos listos para aplicar los m√©todos de selecci√≥n de caracter√≠sticas descritos previamente. El resultado se almacena en dos estructuras (`anova_results` y `rf_importance`), que contienen los rankings y m√©tricas asociadas a la importancia de cada variable:
"""

anova_results, rf_importance = feature_selection_analysis(X_filtered, y, k_features=20)

"""Tras aplicar los m√©todos de selecci√≥n ANOVA F-test y Random Forest, el siguiente bloque de c√≥digo tiene como objetivo combinar y comparar los resultados obtenidos por ambos enfoques en un tabla para facilitar su an√°lisis conjunto."""

print("\n" + "="*60)
print("üîé Comparaci√≥n de m√©todos de selecci√≥n de caracter√≠sticas")
print("="*60)

# DataFrame de comparaci√≥n
comparison_df = anova_results.merge(rf_importance, on='Feature', suffixes=('_ANOVA', '_RF'))
comparison_df['Rank_ANOVA'] = range(1, len(comparison_df)+1)
comparison_df['Rank_RF'] = comparison_df['Importance'].rank(ascending=False).astype(int)

print("\nüîπ Comparaci√≥n de las principales caracter√≠sticas:")
print(comparison_df.sort_values('Rank_ANOVA').head(20))

"""Uno de los hallazgos m√°s relevantes es la coincidencia de variables destacadas en ambos m√©todos. Esta convergencia refuerza la robustez de dichas variables, ya que su importancia se valida tanto desde un enfoque estad√≠stico como desde uno predictivo.

Es por ello, que si analizamos los resultados obtenidos podemos observar que entre las variables m√°s destacadas en ambos m√©todos encontramos:
<table border="1" style="border-collapse:collapse; width:90%; text-align:left;">
  <thead style="background-color:#f0f8ff;">
    <tr>
      <th>Feature</th>
      <th>Rank_ANOVA</th>
      <th>Rank_RF</th>
    </tr>
  </thead>
  <tbody>
    </tr>
      <td>win_sg</td>
      <td>1</td>
      <td>2</td>
    </tr>
    </tr>
      <td>sqrt_hemo</td>
      <td>3</td>
      <td>1</td>
    <tr>
    <tr>
      <td>sqrt_al</td>
      <td>2</td>
      <td>4</td>
    <tr>
    <tr>
      <td>log_pcv</td>
      <td>4</td>
      <td>3</td>
    <tr>
    <tr>
      <td>win_rc</td>
      <td>5</td>
      <td>6</td>
    <tr>
  </tbody>
</table>

Estas coincidencias indican que estas caracter√≠sticas tienen una alta capacidad discriminativa y aportan valor real al modelo, ya que su relevancia se confirma tanto desde el punto de vista estad√≠stico como predictivo. Por esta raz√≥n, deber√≠an considerarse prioritarias en el dise√±o del modelo final.
"""

df = encoded_kidney_df

"""Ahora comprobamos que la variable target est√© balanceada:"""

df['classification_ckd'].value_counts()

"""Al analizar la distribuci√≥n de clases en la variable objetivo, observamos que una de las clases representa m√°s del 60-70% del total de observaciones. Esta situaci√≥n se considera un desbalance moderado, ya que la representaci√≥n de una clase es sustancialmente mayor que la de la otra, lo que puede afectar negativamente el rendimiento de los modelos de clasificaci√≥n.

Este tipo de desbalance puede provocar que los clasificadores tiendan a favorecer la clase mayoritaria, minimizando la penalizaci√≥n por predecir incorrectamente la clase minoritaria. Como consecuencia, se obtienen m√©tricas de evaluaci√≥n enga√±osas.

### 6.2. An√°lisis Discriminante Lineal (LDA)

El An√°lisis Discriminante Lineal (LDA) es una t√©cnica estad√≠stica supervisada utilizada tanto para clasificaci√≥n como para reducci√≥n de la dimensionalidad. Su objetivo principal es encontrar una proyecci√≥n lineal de los datos que maximice la separabilidad entre las distintas clases, conservando al mismo tiempo la mayor cantidad posible de informaci√≥n discriminativa.

Desde la perspectiva de clasificaci√≥n, LDA busca identificar una o m√°s direcciones √≥ptimas (componentes discriminantes) en el espacio original de caracter√≠sticas, en las cuales las diferentes clases est√©n lo m√°s separadas posible. Esto se consigue maximizando la varianza entre clases y minimizando la varianza intra-clase.

En otras palabras, LDA trata de encontrar un subespacio donde los grupos sean internamente compactos pero est√©n externamente distantes entre s√≠. Esta t√©cnica resulta especialmente √∫til cuando se dispone de m√∫ltiples variables predictoras y se desea reducir la complejidad del modelo sin sacrificar capacidad de discriminaci√≥n.


Matem√°ticamente, LDA se basa en la construcci√≥n de dos matrices clave:
<ul>
  <li>
    Matriz de dispersi√≥n intra-clase (S<sub>W</sub>): mide la variabilidad de los datos dentro de cada clase.
  </li>
  <li>
    Matriz de dispersi√≥n entre clases (S<sub>B</sub>): cuantifica la distancia entre las medias de cada clase.
  </li>
</ul>

El objetivo es encontrar una matriz de proyecci√≥n `W` que maximice la siguiente raz√≥n de Fisher:

$$J(W)=\frac{|W^{T}S_{B}W|}{|W^{T}S_{W}W|}$$

La optimizaci√≥n de esta expresi√≥n conduce a un problema de valores propios, cuya soluci√≥n permite obtener las direcciones m√°s discriminantes del espacio de caracter√≠sticas.

Una vez determinadas las direcciones √≥ptimas, los datos originales se proyectan en ese nuevo subespacio discriminante. Para clasificar nuevas observaciones, se puede utilizar una regla de decisi√≥n basada en la distancia (por ejemplo, distancia eucl√≠dea) o en m√©todos de m√°xima verosimilitud, dependiendo de las distribuciones estimadas para cada clase.

LDA asume que las clases siguen distribuciones normales multivariantes con matrices de covarianza iguales. Esta suposici√≥n permite que las fronteras de decisi√≥n sean lineales, lo que lo convierte en un clasificador particularmente eficiente cuando dichas condiciones se cumplen.

Algunas consideraciones que debemos tener en mente son las siguientes:
*   Ventajas:
    *   Permite interpretar con claridad qu√© variables son m√°s relevantes para separar las clases.
    *   Es computacionalmente eficiente y robusto en presencia de muchas variables.
    *   Ofrece buenos resultados cuando las suposiciones del modelo se aproximan bien a la realidad del conjunto de datos.

*   Limitaciones:
    *   Puede perder efectividad cuando las matrices de covarianza de las clases son significativamente diferentes.
    *   Su rendimiento disminuye si las relaciones entre variables y clases son no lineales.
    *   Es sensible a outliers, ya que se basa en la media y la varianza.

En el bloque de c√≥digo que encontramos a continuaci√≥n implementamos un pipeline de clasificaci√≥n utilizando LDA (An√°lisis Discriminante Lineal) como clasificador principal, complementado con SMOTE (Synthetic Minority Over-sampling Technique) para abordar el desbalance de clases.

Adem√°s, incorporamos la b√∫squeda de hiperpar√°metros con validaci√≥n cruzada y la evaluaci√≥n del rendimiento del modelo sobre el conjunto de prueba. Este flujo es particularmente √∫til en problemas donde hay un desequilibrio entre clases y se desea maximizar m√©tricas como el F1-score.

Explicado brevemente el pipeline tenemos:
*   `SMOTE`: que genera nuevas muestras sint√©ticas de la clase minoritaria.
*   `StandardScaler`: se encarga de normalizar los datos, crucial para LDA.
*   `LDA`: es el clasificador lineal, el qual esta basado en maximizar la separabilidad entre clases.

Como `FEATURES` escogemos las variables que se encuentran en el ranking de Rank_ANOVA	y Rank_RF.

Adem√°s, se incorpor√≥ expl√≠citamente la variable `log_sc` (logaritmo de la concentraci√≥n s√©rica de creatinina), debido a su reconocido valor cl√≠nico en el diagn√≥stico y seguimiento de la enfermedad renal cr√≥nica (Chronic Kidney Disease, CKD). La creatinina s√©rica es uno de los principales biomarcadores de la funci√≥n renal y se utiliza habitualmente para estimar la tasa de filtraci√≥n glomerular (TFG), una medida clave en la clasificaci√≥n de la CKD. La transformaci√≥n logar√≠tmica permite estabilizar su varianza y mejorar su comportamiento estad√≠stico en modelos predictivos. Por tanto, su inclusi√≥n responde tanto a criterios m√©dicos como anal√≠ticos.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.metrics import f1_score, recall_score, precision_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as imba_Pipeline
import warnings
warnings.filterwarnings('ignore')

# Constantes
RANDOM_STATE = 42
TEST_SIZE = 0.2
FEATURES = ['win_sg', 'sqrt_hemo','sqrt_al','log_pcv','win_rc','log_sc']
CV_FOLDS = 5

# Preparar datos
X = df[FEATURES]
y = df['classification_ckd']

# Divisi√≥n de prueba para el entrenamiento
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Crear pipeline SMOTE y LDA
smote = SMOTE(random_state=RANDOM_STATE)
lda_pipeline = imba_Pipeline([
    ('smote', smote),
    ('scaler', StandardScaler()),
    ('lda', LinearDiscriminantAnalysis())
])

# Cuadr√≠cula de hiperpar√°metros
param_grid = {
    'lda__solver': ['lsqr', 'eigen'],
    'lda__shrinkage': [None, 'auto', 0.5],
    'smote__sampling_strategy': [0.8, 1.0]
}

# B√∫squeda en cuadr√≠cula con validaci√≥n cruzada
grid_search = GridSearchCV(
    lda_pipeline, param_grid, cv=CV_FOLDS,
    scoring='f1', n_jobs=-1, verbose=1
)
grid_search.fit(X_train, y_train)

# Obtener el mejor modelo y par√°metros
lda_best = grid_search.best_estimator_
print(f"\nüîπ Mejores par√°metros: {grid_search.best_params_}")

# Validaci√≥n cruzada en el conjunto de entrenamiento
cv_scores = cross_val_score(lda_best, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')
print(f"\nüîπ Validaci√≥n cruzada (n={CV_FOLDS}):")
print(f"Precisi√≥n media: {cv_scores.mean():.4f}")
print(f"Desviaci√≥n est√°ndar: {cv_scores.std():.4f}")

# Evaluar en el conjunto de pruebas
y_pred = lda_best.predict(X_test)
y_proba = lda_best.predict_proba(X_test)[:, 1]

def print_evaluation_report(y_test, y_pred):
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    specificity = recall_score(y_test, y_pred, pos_label=0)

    print("\n" + "="*54)
    print("üìä M√©tricas de evaluaci√≥n del conjunto de pruebas")
    print("="*54)
    print(f"{'Exactitud:':<30}{accuracy:.4f}")
    print(f"{'F1-Score:':<30}{f1:.4f}")
    print(f"{'Recordatorio (sensibilidad):':<30}{recall:.4f}")
    print(f"{'Precisi√≥n:':<30}{precision:.4f}")
    print(f"{'Especificidad:':<30}{specificity:.4f}")

    print("\n" + "="*30)
    print("üìù Informe de clasificaci√≥n")
    print("="*30)
    print(classification_report(y_test, y_pred, digits=4))

print_evaluation_report(y_test, y_pred)

"""El modelo de LDA fue sometido a una evaluaci√≥n exhaustiva utilizando validaci√≥n cruzada con cinco particiones, acompa√±ado por la t√©cnica de sobremuestreo `SMOTE` para abordar el desbalance de clases. En total, se exploraron 12 combinaciones distintas de hiperpar√°metros, resultando en 60 ajustes de modelo, lo cual permiti√≥ valorar su comportamiento bajo diferentes configuraciones de regularizaci√≥n y resoluci√≥n.

Tras el proceso de b√∫squeda, los mejores par√°metros seleccionados fueron los siguientes:
*   `solver = 'lsqr'`: este solucionador resulta eficiente para problemas de clasificaci√≥n con alta dimensionalidad, permitiendo el uso opcional de la t√©cnica de shrinkage, lo cual lo hace m√°s flexible frente a problemas de colinealidad en los datos.
*   `shrinkage = None`: la ausencia de regularizaci√≥n adicional indica que las matrices de covarianza fueron estimadas directamente de los datos sin penalizaci√≥n, presuponiendo una muestra suficientemente representativa y estable.
*   `sampling_strategy = 0.8`: se utiliz√≥ `SMOTE` para aumentar la representaci√≥n de la clase minoritaria hasta alcanzar el 80% del tama√±o de la clase mayoritaria, mejorando as√≠ la sensibilidad del modelo sin comprometer la especificidad.

Durante la validaci√≥n cruzada, el modelo alcanz√≥ una precisi√≥n media de 91.87%, con una desviaci√≥n est√°ndar de ¬±4.12%. Aunque esta variabilidad es moderadamente mayor que en los modelos que veremos a coninuaci√≥n (SVM y QDA), sigue indicando una buena estabilidad general entre diferentes particiones, sin se√±ales de sobreajuste graves.

La evaluaci√≥n final del modelo sobre datos no utilizados en el entrenamiento arroj√≥ los siguientes resultados:
*   Exactitud general: 95.00%
*   F1-score: 95.83%, denotando un balance sobresaliente entre precisi√≥n y sensibilidad.
*   Sensibilidad: 92.00%, lo que implica una fuerte capacidad de detecci√≥n de la clase positiva.
*   Precisi√≥n: 100.00%, indicando que no se produjeron falsos positivos.
*   Especificidad: 100.00%, lo que reafirma su capacidad para identificar correctamente todos los casos negativos.

Por lo que respecta al desempe√±o por clase:

<table>
    <thead>
      <tr>
        <th>Clase</th>
        <th>Precisi√≥n</th>
        <th>Recall</th>
        <th>F1-Score</th>
        <th>Soporte</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0 (No enfermo)</td>
        <td>88.24%</td>
        <td>100.00%</td>
        <td>93.75%</td>
        <td>30</td>
      </tr>
      <tr>
        <td>1 (Enfermo)</td>
        <td>100.00%</td>
        <td>92.00%</td>
        <td>95.83%</td>
        <td>50</td>
      </tr>
    </tbody>
  </table>

Se observa un rendimiento s√≥lido en ambas clases. En particular, destaca la ausencia de falsos positivos, lo que es cr√≠tico en contextos donde una clasificaci√≥n err√≥nea de pacientes sanos como enfermos podr√≠a derivar en intervenciones innecesarias. A su vez, la alta sensibilidad confirma su eficacia para detectar correctamente los casos positivos (pacientes enfermos), aunque con un peque√±o margen de error comparado con modelos no lineales.

Podriamos concluir que el modelo LDA, ha demostrado ser una soluci√≥n eficaz y eficiente para tareas de clasificaci√≥n binaria con estructuras lineales bien definidas. A pesar de su simplicidad relativa frente a modelos m√°s complejos como SVM o QDA, ofrece un rendimiento altamente competitivo, especialmente por su precisi√≥n perfecta (100%) y especificidad total (100%).

Su principal fortaleza radica en su eficiencia computacional, interpretabilidad estad√≠stica y capacidad de generalizaci√≥n sin necesidad de regularizaci√≥n. Por ende, LDA se presenta como una alternativa id√≥nea en entornos cl√≠nicos donde los recursos son limitados o se prioriza la transparencia del modelo, sin sacrificar un desempe√±o robusto y confiable.

En el contexto de problemas cl√≠nicos como la detecci√≥n de Enfermedad Renal Cr√≥nica (ERC), es fundamental no solo evaluar el rendimiento global de un modelo, sino tambi√©n comprender c√≥mo se comporta ante errores tipo I (falsos positivos) y tipo II (falsos negativos). Para ello, la matriz de confusi√≥n es una herramienta esencial.

El siguiente bloque de c√≥digo implementa una funci√≥n personalizada para visualizar la matriz de confusi√≥n con un enfoque cl√≠nico. Adem√°s de mostrar el conteo de predicciones correctas e incorrectas, se destacan tres m√©tricas clave directamente sobre la gr√°fica:
*   Exactitud (accuracy): proporci√≥n de predicciones correctas.
*   Sensibilidad (recall): proporci√≥n de verdaderos positivos (capacidad del modelo para detectar la ERC).
*   Especificidad: proporci√≥n de verdaderos negativos (capacidad del modelo para descartar casos sanos correctamente).
"""

def plot_confusion_matrix(y_true, y_pred, model_name="Model", figsize=(8, 6)):
    """
    Matriz de confusi√≥n cl√≠nicamente enfocada para la clasificaci√≥n de la ERC.
    Muestra precisi√≥n, sensibilidad y especificidad.
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import ListedColormap
    from sklearn.metrics import confusion_matrix, accuracy_score, recall_score

    class_names = ['No CKD', 'CKD']
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    accuracy = accuracy_score(y_true, y_pred)
    sensitivity = recall_score(y_true, y_pred)  # Tasa de verdaderos positivos
    specificity = tn / (tn + fp)  # Tasa de verdaderos negativos

    plt.figure(figsize=figsize, facecolor='white')
    ax = plt.gca()

    # Crear un colormap personalizado para la matriz
    colors = ['#b7b7a4', '#b7b7a4', '#588157', '#14213d']
    cmap = ListedColormap(colors)

    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,
                annot_kws={'size': 16, 'weight': 'bold', 'color': 'white'},
                xticklabels=class_names, yticklabels=class_names,
                cbar=False, square=True, linewidths=1, linecolor='white')

    # Etiquetas de eje
    ax.set_xlabel('Diagn√≥stico previsto', fontsize=12, labelpad=10)
    ax.set_ylabel('Diagn√≥stico real', fontsize=12, labelpad=10)

    # T√≠tulo
    plt.suptitle(f'{model_name}', fontsize=14, fontweight='bold', y=1, color='#000000')

    # A√±adir un subt√≠tulo con m√©tricas
    plt.figtext(0.5, 0.94, f'Exactitud: {accuracy:.1%} | Sensibilidad: {sensitivity:.1%} | Especificidad: {specificity:.1%}',
            ha='center', fontsize=10, color='#000000')

    ax.set_facecolor('white')
    for spine in ax.spines.values():
        spine.set_visible(False)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    plt.tight_layout()
    plt.show()

plot_confusion_matrix(y_test, y_pred, model_name="Modelo LDA")

"""Adem√°s, en el √°mbito del an√°lisis predictivo resulta fundamental no solo predecir correctamente, sino evaluar c√≥mo var√≠a el rendimiento del modelo ante diferentes umbrales de decisi√≥n.

Una de las herramientas m√°s poderosas para este prop√≥sito es la Curva ROC (Receiver Operating Characteristic), que permite visualizar gr√°ficamente la relaci√≥n entre:
*   Tasa de verdaderos positivos (TPR o sensibilidad).
*   Tasa de falsos positivos (FPR o especificidad).

Un aspecto que hemos tenido en cuenta es a√±adir a la representaci√≥n la l√≠nea de rendimiento aleatorio ya que sirve como punto de referencia m√≠nimo. Si un modelo tiene una curva ROC por debajo o muy cerca de esa l√≠nea, significa que el modelo no est√° haciendo predicciones corectas y en algunos casos, peores que al azar.

A parte, hemos decidido calcular el AUC (√Årea Bajo la Curva) como un indicador del desempe√±o global del modelo, ya que cuanto m√°s cercano a 1, mejor es la capacidad de discriminaci√≥n del clasificador.
"""

def plot_roc_curve(y_true, y_prob, model_name="Model", figsize=(6, 5), fontsize=10):
    """
    Crea un gr√°fico de curva ROC para evaluar el rendimiento de un modelo de clasificaci√≥n.

    ============
    Par√°metros:
    ============
    y_true: etiquetas binarias verdaderas.
    y_prob: probabilidades predichas para la clase positiva.
    model_name: nombre para mostrar en el t√≠tulo.
    figsize: dimensiones de la figura.
    fontsize: tama√±o de fuente base.
    """

    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)

    # Figura de configuraci√≥n con fondo gris claro
    plt.figure(figsize=figsize, facecolor='#f5f5f5')
    ax = plt.gca()
    ax.set_facecolor('#ffffff')

    # Curva ROC principal (utilizando gradiente azul)
    plt.plot(fpr, tpr, color='#1a759f', lw=3, alpha=0.8,
             label=f'{model_name} (AUC = {roc_auc:.3f})')

    # L√≠nea de referencia diagonal
    plt.plot([0, 1], [0, 1], color='#9b2226', linestyle='--', lw=2,
             label='Rendimiento aleatorio')

    # Referencia de clasificador perfecta (l√≠nea discontinua)
    plt.plot([0, 0, 1], [0, 1, 1], color='#aaaaaa', linestyle=':', lw=1.5)

    # Personalizar ejes
    plt.xlim([-0.02, 1.02])
    plt.ylim([-0.02, 1.02])
    plt.xlabel('Tasa de falsos positivos', fontsize=fontsize, labelpad=10)
    plt.ylabel('Tasa de verdaderos positivos', fontsize=fontsize, labelpad=10)

    # T√≠tulo y leyenda
    plt.title(f'Curva ROC: {model_name}',
              fontsize=fontsize+2, pad=20, weight='bold')
    plt.legend(loc="lower right", framealpha=1, facecolor='white')

    # Estilo de cuadr√≠cula y marca
    ax.grid(True, linestyle='--', alpha=0.3, color='gray')
    ax.tick_params(axis='both', which='major', labelsize=fontsize-1)

    # Quitar los spines superiores/derechas
    for spine in ['top', 'right']:
        ax.spines[spine].set_visible(False)

    plt.tight_layout()
    plt.show()

plot_roc_curve(y_test, y_proba, model_name='Modelo LDA')

"""Para finalizar esta secci√≥n del an√°lisis, es fundamental comprender qu√© variables est√°n contribuyendo m√°s a las decisiones del modelo. Esta interpretaci√≥n es especialmente valiosa en contextos donde no solo importa la precisi√≥n, sino tambi√©n la transparencia y la explicabilidad de los resultados.

En el nuestro caso los coeficientes del modelo representan las direcciones que maximizan la separaci√≥n entre clases. La magnitud absoluta de estos coeficientes nos indica qu√© tan influyente es cada variable para diferenciar entre pacientes con y sin enfermedad renal cr√≥nica (CKD).

Para visualizarlo, el siguiente c√≥digo crea un gr√°fico de barras horizontales que muestra la importancia relativa de cada caracter√≠stica utilizada por el modelo LDA, ordenadas de menor a mayor impacto.
"""

def plot_feature_importance(model, X_test):
    lda = model.named_steps['lda']
    feature_importance = np.abs(lda.coef_)[0]

    plt.figure(figsize=(8, 6))
    features = X_test.columns
    indices = np.argsort(feature_importance)

    plt.title('Importancia de las caracter√≠sticas (coeficientes LDA)', fontweight='bold', pad=15)
    plt.barh(range(len(indices)), feature_importance[indices], align='center')
    plt.yticks(range(len(indices)), [features[i] for i in indices])
    plt.xlabel('Magnitud del coeficiente absoluto', labelpad=12)
    plt.tight_layout()
    plt.show()

plot_feature_importance(lda_best, X_test)

print("\n")
import joblib
joblib.dump(lda_best, 'lda_model.pkl')

"""Observando el gr√°fico, podemos hacer la siguiente interpretaci√≥n:

*   Variables m√°s relevantes: las variables `win_sg`, `sqrt_hemo`, `sqrt_al`, `win_rc` son las que presentan mayor magnitud de coeficiente, lo que indica que tienen mayor capacidad discriminativa para identificar pacientes con CKD.
*   Relaci√≥n con la fisiopatolog√≠a de CKD: estas variables est√°n directamente relacionadas con la funci√≥n renal y las alteraciones metab√≥licas asociadas a la CKD, como la disminuci√≥n de la capacidad de concentraci√≥n urinaria, anemia, alteraciones en prote√≠nas plasm√°ticas, y la presencia de hipertensi√≥n.

*   Implicaciones cl√≠nicas: el modelo sugiere que la evaluaci√≥n de CKD debe centrarse en par√°metros de laboratorio cl√°sicos (hemoglobina, alb√∫mina, urea, densidad urinaria) y antecedentes de hipertensi√≥n, lo cual es consistente con las gu√≠as internacionales para el diagn√≥stico y manejo de CKD.

**Relaci√≥n con la clasificaci√≥n y diagn√≥stico de CKD:**

La clasificaci√≥n de CKD se basa principalmente en el filtrado glomerular estimado (eGFR) y la presencia de albuminuria, pero otras variables como las que aparecen en el gr√°fico contribuyen a la evaluaci√≥n integral del paciente y a la predicci√≥n de complicaciones y progresi√≥n de la enfermedad.

### 6.3. An√°lisis Discriminante Cuadr√°tico (QDA)

El An√°lisis Discriminante Cuadr√°tico (QDA) es una t√©cnica de clasificaci√≥n supervisada basada en modelos probabil√≠sticos. Se utiliza cuando se asume que las clases siguen distribuciones normales multivariadas, y permite que cada clase tenga su propia matriz de covarianza. Esto le otorga una mayor flexibilidad para modelar fronteras de decisi√≥n no lineales, adapt√°ndose mejor a distribuciones complejas.

QDA pertenece a la familia de modelos generativos, ya que estima la distribuci√≥n de los datos condicional a cada clase `(P(x|y))` y luego aplica la regla de Bayes para obtener la probabilidad posterior `P(y|x)`, sobre la cual se basa la clasificaci√≥n.

Su fundamento te√≥rico consiste en que dado un conjunto de clases `y‚àà{1,...,K}`, QDA asume que los datos de cada clase est√°n distribuidos seg√∫n una normal multivariada con media `Œº_k` y matriz de covarianza `Œ£_k`:

$$P(x|y=k)=\frac{1}{(2\pi)^{d/2}|\sum_{k}|^{1/2}}exp(-\frac{1}{2}(x-\mu_{k})^{T}\sum_{k}^{-1}(x-\mu_{k}))$$


QDA no impone la restricci√≥n de matrices de covarianza iguales para todas las clases, lo que permite que las fronteras de decisi√≥n sean cuadr√°ticas en lugar de lineales.

Para clasificar una observaci√≥n `x`, QDA calcula una funci√≥n discriminante para cada clase `k` de la forma:

$$\delta_{k}(x)=-\frac{1}{2}log|\sum_{k}|-\frac{1}{2}(x-\mu_{k})^T\sum_{k}^{-1}(x-\mu_{k})+log\pi_k$$

Donde:
*   `Œ£_k` es la matriz de covarianza de la clase `k`.
*   `Œº_k` es la media de la clase `k`.
*   `œÄ_k` es la probabilidad a priori de la clase `k`.

Algunas consideraciones que debemos tener en cuenta son las siguientes:
*   Ventajas:
    *   Modelado no lineal: es capaz de generar fronteras de decisi√≥n curvas, lo que lo hace m√°s adecuado para problemas en los que las clases no son separables linealmente.
    *   Flexibilidad estructural: al permitir una matriz de covarianza diferente por clase, QDA captura de manera m√°s fiel la estructura de los datos cuando las clases tienen diferentes varianzas o correlaciones entre variables.
    *   Base probabil√≠stica: su formulaci√≥n probabil√≠stica permite no solo clasificar, sino tambi√©n estimar probabilidades de pertenencia.
*   Limitaciones:
    *   Requiere m√°s datos: como debe estimar una matriz de covarianza distinta para cada clase, QDA necesita una cantidad considerable de datos para evitar sobreajuste, especialmente en espacios de alta dimensi√≥n.
    *   Sensibilidad a la normalidad: QDA asume que los datos siguen distribuciones normales multivariadas. Si esta suposici√≥n no se cumple, su rendimiento puede degradarse notablemente.
    *   Problemas de estabilidad: en conjuntos de datos peque√±os o con multicolinealidad, la estimaci√≥n de las matrices de covarianza puede ser inestable o incluso no invertible, lo que impide aplicar el modelo.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.metrics import f1_score, recall_score, precision_score
from sklearn.inspection import permutation_importance
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as imba_Pipeline
import warnings
warnings.filterwarnings('ignore')

# Constantes
RANDOM_STATE = 42
TEST_SIZE = 0.2
FEATURES = ['win_sg', 'sqrt_hemo','sqrt_al','log_pcv','win_rc','log_sc']
CV_FOLDS = 5

# Preparar datos
X = df[FEATURES]
y = df['classification_ckd']

# Divisi√≥n de prueba para el entrenamiento
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Crear pipeline de SMOTE y QDA
smote = SMOTE(random_state=RANDOM_STATE)
qda_pipeline = imba_Pipeline([
    ('smote', smote),
    ('scaler', StandardScaler()),
    ('qda', QuadraticDiscriminantAnalysis())
])

# Cuadr√≠cula de hiperpar√°metros para QDA
param_grid = {
    'qda__reg_param': [0.0, 0.1, 0.2, 0.5],  # Par√°metro de regularizaci√≥n
    'qda__store_covariance': [True, False],
    'smote__sampling_strategy': [0.8, 1.0]
}

# B√∫squeda en cuadr√≠cula con validaci√≥n cruzada
grid_search = GridSearchCV(
    qda_pipeline, param_grid, cv=CV_FOLDS,
    scoring='f1', n_jobs=-1, verbose=1
)
grid_search.fit(X_train, y_train)

# Obtenga el mejor modelo y par√°metros
qda_best = grid_search.best_estimator_
print(f"\nüîπ Mejores par√°metros: {grid_search.best_params_}")

# Validaci√≥n cruzada en el conjunto de entrenamiento
cv_scores = cross_val_score(qda_best, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')
print(f"\nüîπ Validaci√≥n cruzada (n={CV_FOLDS}):")
print(f"Precisi√≥n media: {cv_scores.mean():.4f}")
print(f"Desviaci√≥n est√°ndar: {cv_scores.std():.4f}")

# Evaluar en el conjunto de pruebas
y_pred = qda_best.predict(X_test)
y_proba = qda_best.predict_proba(X_test)[:, 1]

print_evaluation_report(y_test, y_pred)

"""El modelo QDA fue evaluado mediante un riguroso proceso de validaci√≥n cruzada estratificada con cinco particiones. Durante este proceso se probaron 16 combinaciones distintas de hiperpar√°metros, lo que dio lugar a un total de 80 ajustes individuales del modelo, permitiendo as√≠ una evaluaci√≥n exhaustiva y controlada de su comportamiento bajo diversas configuraciones.

A partir de la b√∫squeda sistem√°tica, se identificaron los siguientes par√°metros como √≥ptimos:
*   `reg_param = 0.0`: la ausencia de regularizaci√≥n sugiere que el modelo logr√≥ una buena capacidad de generalizaci√≥n sin necesidad de introducir restricciones adicionales sobre las matrices de covarianza. Esto tambi√©n implica que no se observaron problemas significativos de sobreajuste en el entrenamiento.
*   `store_covariance = True`: la opci√≥n de conservar las matrices de covarianza permite una inspecci√≥n posterior sobre la dispersi√≥n interna de las clases
*   `sampling_strategy = 0.8 (SMOTE)`: se aplic√≥ un sobremuestreo de la clase minoritaria hasta alcanzar el 80% del tama√±o de la clase mayoritaria. Esta t√©cnica mitig√≥ el desequilibrio de clases, mejorando la sensibilidad del modelo sin introducir sesgos artificiales significativos.

Durante la validaci√≥n cruzada, el modelo alcanz√≥ una precisi√≥n media del 96.88%, acompa√±ada de una desviaci√≥n est√°ndar de apenas ¬±0.99%. Este bajo nivel de variabilidad evidencia una alta estabilidad y consistencia del modelo frente a diferentes particiones de los datos, lo que refuerza su confiabilidad como herramienta predictiva.

Por lo que hace a la evaluaci√≥n sobre el conjunto de prueba, es decir, datos no utilizados durante el entrenamiento ni la validaci√≥n, mostr√≥ los siguientes resultados:
*   Exactitud general: 96.25%
*   F1-score: 96.91%, indicando un balance √≥ptimo entre precisi√≥n y recall.
*   Sensibilidad (recall): 94.00%, lo que refleja una muy buena capacidad para detectar correctamente los casos positivos (pacientes enfermos).
*   Precisi√≥n: 100.00%, lo cual implica que todas las predicciones positivas fueron correctas, sin falsos positivos.
*   Especificidad: 100.00%, confirmando la capacidad del modelo para identificar correctamente los casos negativos.

Tambi√©n hemos analizado el desempe√±o por clases:

<table>
    <thead>
      <tr>
        <th>Clase</th>
        <th>Precisi√≥n</th>
        <th>Recall</th>
        <th>F1-Score</th>
        <th>Soporte</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0 (No enfermo)</td>
        <td>90.91%</td>
        <td>100.00%</td>
        <td>95.24%</td>
        <td>30</td>
      </tr>
      <tr>
        <td>1 (Enfermo)</td>
        <td>100.00%</td>
        <td>94.00%</td>
        <td>96.91%</td>
        <td>50</td>
      </tr>
    </tbody>
  </table>

El modelo exhibe una ausencia completa de falsos positivos, lo cual es especialmente valioso en contextos m√©dicos, ya que evita alarmas innecesarias y garantiza la confiabilidad diagn√≥stica. Adem√°s, mantiene una alta sensibilidad (94%) en la clase positiva, lo que contribuye a una detecci√≥n eficaz de pacientes enfermos sin comprometer la especificidad.

Como conclusi√≥n, QDA ha demostrado un desempe√±o sobresaliente tanto en validaci√≥n cruzada como en datos no vistos, destac√°ndose por su alta precisi√≥n (100%) en predicciones positivas y su capacidad para detectar la gran mayor√≠a de los casos positivos (94% de sensibilidad). La robustez observada en distintas m√©tricas, junto con la ausencia de sobreajuste, lo posicionan como una opci√≥n confiable para escenarios cl√≠nicos donde crucial detectar todos los casos positivos sin comprometer la especificidads.
"""

plot_confusion_matrix(y_test, y_pred, model_name="Modelo QDA")

plot_roc_curve(y_test, y_proba, model_name='Modelo QDA')

"""Adem√°s de evaluar el rendimiento general de un modelo, es crucial entender qu√© variables est√°n contribuyendo m√°s a sus predicciones. Esto no solo ayuda a interpretar los resultados, sino tambi√©n a validar que el modelo est√© capturando relaciones l√≥gicas y relevantes en los datos.

En este apartado, realizamos un an√°lisis de la importancia de caracter√≠sticas usando el enfoque de importancia por permutaci√≥n. Esta t√©cnica mide c√≥mo se ve afectado el rendimiento del modelo al alterar aleatoriamente los valores de una caracter√≠stica: si una permutaci√≥n degrada significativamente la precisi√≥n, esa variable es considerada importante.

El procedimiento incluye:
*   Transformar los datos de entrada (excluyendo t√©cnicas como `SMOTE`, que solo se aplican en entrenamiento).
*   Aplicar la t√©cnica de `permutation_importance` al modelo QDA optimizado.
*   Visualizar las variables m√°s influyentes con un gr√°fico de barras.

Este enfoque es modelo-agn√≥stico, lo que significa que no depende de los coeficientes internos del modelo (como en la regresi√≥n lineal), sino del impacto real en el rendimiento predictivo. As√≠, permite una visi√≥n m√°s fiel y pr√°ctica de la contribuci√≥n de cada variable.
"""

# Importancia de la permutaci√≥n
def calculate_permutation_importance(model, X, y):
    """Calcular la importancia de las caracter√≠sticas mediante la importancia de la permutaci√≥n"""

    # Para el pipeline, primero debemos transformar los datos antes de dar importancia a la permutaci√≥n
    # Obtener datos transformados
    X_transformed = X.copy()
    if hasattr(model, 'named_steps'):
        if 'smote' in model.named_steps:
            # Omitir SMOTE para fines de evaluaci√≥n
            pass
        if 'scaler' in model.named_steps:
            X_transformed = model.named_steps['scaler'].transform(X_transformed)

    # Extraer el modelo QDA del pipeline
    qda_model = model.named_steps['qda'] if hasattr(model, 'named_steps') else model

    # Calcular la importancia de la permutaci√≥n
    result = permutation_importance(
        qda_model, X_transformed, y,
        n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1
    )

    # Crear un DataFrame para los puntajes de importancia
    importance_df = pd.DataFrame({
        'Caracter√≠stica': X.columns,
        'Importancia': result.importances_mean,
        'Std': result.importances_std
    })

    # Ordenar por importancia
    importance_df = importance_df.sort_values(by='Importancia', ascending=False)

    return importance_df


# Plot de la importancia de las caracter√≠sticas
def plot_feature_importance(importance_df, title, top_n=15):
    plt.figure(figsize=(8, 6))

    # Obtener las N mejores funciones
    if len(importance_df) > top_n:
        plot_df = importance_df.head(top_n)
    else:
        plot_df = importance_df

    # Crear un gr√°fico de barras horizontales
    sns.barplot(x='Importancia', y='Caracter√≠stica', data=plot_df, palette='viridis')
    plt.title(f'{title} (Top {len(plot_df)} caracter√≠sticas)', fontsize=12, fontweight='bold', pad=15)
    plt.xlabel('Puntuaci√≥n de importancia', fontsize=10, labelpad=12)
    plt.ylabel('Caracter√≠stica', fontsize=10, labelpad=12)
    plt.tight_layout()
    plt.show()
    print("\n")

    return plot_df

# Importancia de la permutaci√≥n en el conjunto de pruebas
print("\nüîπ Importancia de la permutaci√≥n:")
perm_importance = calculate_permutation_importance(qda_best, X_test, y_test)
print(perm_importance)
print("\n")
plot_feature_importance(perm_importance, 'Importancia de la caracter√≠stica de permutaci√≥n en el modelo QDA')

import joblib
joblib.dump(qda_best, 'qda_model.pkl')

"""Los resultados sugieren que un enfoque diagn√≥stico √≥ptimo para la enfermedad renal deber√≠a priorizar:


*   La evaluaci√≥n cuidadosa de albuminuria, como el marcador m√°s discriminativo
*   La medici√≥n de la gravedad espec√≠fica urinaria como complemento diagn√≥stico crucial
*  Los par√°metros hematol√≥gicos y la creatinina como indicadores complementarios

El modelo QDA, al capturar relaciones no lineales entre las variables, proporciona informaci√≥n valiosa sobre la complejidad de las interacciones entre estos biomarcadores en el contexto de la enfermedad renal.

### 6.4. M√°quinas de Vectores de Soporte (Sigmoidal)

Las M√°quinas de Vectores de Soporte (SVM) constituyen un enfoque robusto de aprendizaje supervisado, ampliamente utilizado tanto para tareas de clasificaci√≥n como, en menor medida, de regresi√≥n. Su objetivo principal es encontrar un hiperplano que separe de forma √≥ptima las clases en un espacio de caracter√≠sticas, maximizando el margen entre los ejemplos m√°s cercanos de cada clase, conocidos como vectores de soporte.

En el caso de una SVM con kernel sigmoidal, se emplea una transformaci√≥n no lineal de los datos de entrada mediante una funci√≥n kernel. Esto permite proyectar los datos a un espacio de mayor dimensi√≥n en el que la separaci√≥n lineal entre clases es factible, incluso si en el espacio original no lo es.

La funci√≥n sigmoidal utilizada como kernel se define de la siguiente manera:

$$K(x,x')=tanh(\gamma \cdot \left<x,x'\right>+r)$$

<p>Donde:</p>
<ul>
    <li><code>&lt;x, x'&gt;</code> representa el producto escalar entre los vectores <code>x</code> y <code>x'</code>.</li>
    <li><code>Œ≥</code> es un par√°metro que controla la escala del producto escalar.</li>
    <li><code>r</code> es un par√°metro de desplazamiento que ajusta el umbral de activaci√≥n de la funci√≥n.</li>
</ul>

Esta transformaci√≥n permite que la SVM construya un modelo no lineal en el espacio original, encontrando separaciones complejas entre clases de forma m√°s efectiva.

El objetivo de una SVM es maximizar el margen entre las clases, lo que se consigue al encontrar el hiperplano que maximiza la distancia entre las muestras m√°s cercanas de cada clase (vectores de soporte). La funci√≥n objetivo para SVM es minimizar una funci√≥n de p√©rdida, sujeta a restricciones de clasificaci√≥n correcta. La funci√≥n de p√©rdida utilizada es la funci√≥n de hinge (bisagra), que penaliza las clasificaciones incorrectas:

$$\underset{w,b}{min}\,\,\,\frac{1}{2}||W||^{2}\,\,\,\,\,\,\,\,sujeto\,\,a:\,\,\,\,\,y_{i}(w\cdot x_{i}+b)\geq 1,\,\,\,\,\,\,\,\forall i=1,...,n$$

<p>Donde:</p>
<ul>
    <li><code>w</code> es el vector normal al hiperplano de decisi√≥n.</li>
    <li><code>x_i</code> son los vectores de las caracter√≠sticas.</li>
    <li><code>y_i</code> son las etiquetas correspondientes a cada clase.</li>
</ul>

En presencia de un kernel, como el sigmoidal, esta optimizaci√≥n se lleva a cabo en un espacio transformado. El proceso suele implicar la resoluci√≥n de un problema cuadr√°tico mediante t√©cnicas como la programaci√≥n cuadr√°tica (QP) o m√©todos basados en gradientes.

Algunas consideraciones que debemos tener en mente son las siguientes:
*   Ventajas:
    *   Capacidad para capturar relaciones no lineales: permite a la SVM identificar patrones complejos en los datos que no pueden ser separados linealmente en el espacio original.
    *   Flexibilidad del modelo: los par√°metros `Œ≥` y `r` ofrecen control sobre la forma y complejidad del modelo, lo que posibilita una mejor adaptaci√≥n a distintos tipos de datos.
    *   Precisi√≥n: cuando se configura adecuadamente, este kernel puede lograr una separaci√≥n efectiva entre clases, favoreciendo un alto rendimiento en tareas de clasificaci√≥n.
*   Limitaciones:
    *   Sensibilidad a la configuraci√≥n de par√°metros: el rendimiento del modelo depende fuertemente de una adecuada selecci√≥n de `Œ≥` y `r`, lo cual puede requerir una b√∫squeda intensiva mediante validaci√≥n cruzada.
    *   Propensi√≥n al sobreajuste: en presencia de ruido o valores at√≠picos, el modelo puede sobreadaptarse, comprometiendo su capacidad de generalizaci√≥n.
    *   Coste computacional: la combinaci√≥n de no linealidad y necesidad de ajuste de m√∫ltiples par√°metros incrementa la carga computacional, especialmente en conjuntos de datos grandes.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.metrics import f1_score, recall_score, precision_score
from sklearn.inspection import permutation_importance
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as imba_Pipeline
import warnings
warnings.filterwarnings('ignore')

# Constantes
RANDOM_STATE = 42
TEST_SIZE = 0.2
FEATURES = ['win_sg', 'sqrt_hemo','sqrt_al','log_pcv','win_rc','log_sc']
CV_FOLDS = 5

# Preparar datos
X = df[FEATURES]
y = df['classification_ckd']

# Divisi√≥n de prueba para el entrenamiento
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Cree un pipeline de SMOTE y SVM solo con el kernel Sigmoid
smote = SMOTE(random_state=RANDOM_STATE)
svm_pipeline = imba_Pipeline([
    ('smote', smote),
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='sigmoid', probability=True, random_state=RANDOM_STATE))
])

# Cuadr√≠cula de hiperpar√°metros para SVM con kernel sigmoide
param_grid = {
    'svm__C': [0.1, 1, 10, 100],
    'svm__gamma': ['scale', 'auto', 0.01, 0.1, 1],
    'svm__coef0': [-1.0, 0.0, 1.0],  # Importante para el n√∫cleo sigmoideo
    'smote__sampling_strategy': [0.8, 1.0]
}

# B√∫squeda en cuadr√≠cula con validaci√≥n cruzada
grid_search = GridSearchCV(
    svm_pipeline, param_grid, cv=CV_FOLDS,
    scoring='f1', n_jobs=-1, verbose=1
)
grid_search.fit(X_train, y_train)

# Obtener el mejor modelo y par√°metros
svm_best = grid_search.best_estimator_
print(f"\nüîπ Mejores par√°metros: {grid_search.best_params_}")

# Validaci√≥n cruzada en el conjunto de entrenamiento
cv_scores = cross_val_score(svm_best, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')
print(f"\nüîπ Validaci√≥n cruzada (n={CV_FOLDS}):")
print(f"Precisi√≥n media: {cv_scores.mean():.4f}")
print(f"Desviaci√≥n est√°ndar: {cv_scores.std():.4f}")

# Evaluar en el conjunto de pruebas
y_pred = svm_best.predict(X_test)
y_proba = svm_best.predict_proba(X_test)[:, 1]

print_evaluation_report(y_test, y_pred)

"""El modelo de M√°quina de Vectores de Soporte (SVM) fue sometido a una evaluaci√≥n rigurosa mediante validaci√≥n cruzada estratificada con cinco particiones, explorando exhaustivamente un total de 120 combinaciones de hiperpar√°metros. Este proceso implic√≥ 600 ajustes individuales del modelo, lo que garantiz√≥ una b√∫squeda integral del espacio de par√°metros con el fin de maximizar el rendimiento predictivo.

A partir del proceso de validaci√≥n, se identificaron los siguientes valores √≥ptimos:
*   `C = 10`: un valor moderadamente alto que penaliza con firmeza los errores de clasificaci√≥n, conduciendo a m√°rgenes m√°s ajustados sin comprometer la generalizaci√≥n.
*   `Œ≥ = 0.1`: este par√°metro del kernel sigmoidal controla la influencia de cada instancia de entrenamiento. Un valor medio logra un equilibrio adecuado entre el ajuste local y la capacidad de generalizaci√≥n.
*   `coef0 = -1.0`: factor independiente en la funci√≥n de kernel (aplicable en kernels polinomial y sigmoidal), que afecta la forma de la frontera de decisi√≥n.
*   `sampling_strategy = 0.8 (SMOTE)`: la clase minoritaria fue sobremuestreada hasta alcanzar el 80% del tama√±o de la clase mayoritaria, mejorando el balance de clases en el conjunto de entrenamiento.

Adem√°s, durante la validaci√≥n cruzada, el modelo alcanz√≥ una precisi√≥n media del 97.50%, con una desviaci√≥n est√°ndar de ¬±1.59%, lo cual evidencia una excelente estabilidad y confiabilidad entre diferentes particiones del conjunto de datos.

En relaci√≥n al conjunto de prueba, el modelo SVM confirm√≥ su alto desempe√±o:
*   Exactitud: 97.50%
*   F1-score: 97.96%
*   Sensibilidad: 96.00%
*   Precisi√≥n: 100.00%
*   Especificidad: 100.00%

El informe de clasificaci√≥n detallado por clase mostr√≥:

<table>
    <thead>
      <tr>
        <th>Clase</th>
        <th>Precisi√≥n</th>
        <th>Recall</th>
        <th>F1-Score</th>
        <th>Soporte</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0 (No enfermo)</td>
        <td>96.77%</td>
        <td>100.00%</td>
        <td>98.36%</td>
        <td>30</td>
      </tr>
      <tr>
        <td>1 (Enfermo)</td>
        <td>100.00%</td>
        <td>98.00%</td>
        <td>98.99%</td>
        <td>50</td>
      </tr>
    </tbody>
  </table>

Estos resultados indican que el modelo logra una precisi√≥n perfecta (100%) en la detecci√≥n de casos positivos, sin falsos positivos, y con una alta sensibilidad, lo cual es esencial en escenarios cl√≠nicos donde la identificaci√≥n temprana de enfermedades es cr√≠tica.

En conclusi√≥n, el modelo SVM ha demostrado ser el m√°s robustos dentro del conjunto de modelos evaluados hasta ahora. Su capacidad para detectar correctamente ambas clases, junto con su ausencia de falsos positivos y elevada sensibilidad, lo convierten en una opci√≥n altamente fiable para tareas de predicci√≥n en el contexto de diagn√≥stico de enfermedad renal cr√≥nica ya que tanto la detecci√≥n temprana como la minimizaci√≥n de errores son factores cr√≠ticos.
"""

plot_confusion_matrix(y_test, y_pred, model_name='Modelo SVM')

plot_roc_curve(y_test, y_proba, model_name='Modelo SVM')

"""Para comprender mejor el funcionamiento interno del modelo SVM con n√∫cleo sigmoidal, hemos implementado una estrategia basada en la importancia de permutaci√≥n. Este m√©todo eval√∫a la relevancia de cada variable midiendo el impacto en el desempe√±o del modelo cuando los valores de dicha caracter√≠stica son aleatoriamente permutados. Es una t√©cnica agn√≥stica al modelo, lo que significa que no depende de c√≥mo est√© construido internamente.

En este caso, el an√°lisis se realiza posterior al entrenamiento del mejor modelo SVM y utilizando √∫nicamente el conjunto de prueba, con el fin de evitar sesgos y evaluar la importancia de las variables en datos no vistos. Adem√°s, se omite cualquier t√©cnica de sobremuestreo (como `SMOTE`) durante esta evaluaci√≥n para mantener la objetividad.
"""

# Utilizando √∫nicamente el m√©todo de importancia de permutaci√≥n para SVM
def calculate_permutation_importance(model, X, y):
    """Calcular la importancia de las caracter√≠sticas mediante la importancia de la permutaci√≥n"""

    # Para el pipeline, primero debemos transformar los datos antes de dar importancia a la permutaci√≥n.
    X_transformed = X.copy()
    if hasattr(model, 'named_steps'):
        if 'smote' in model.named_steps:
            # Omitir SMOTE para fines de evaluaci√≥n
            pass
        if 'scaler' in model.named_steps:
            X_transformed = model.named_steps['scaler'].transform(X_transformed)

    # Extraer el modelo SVM del pipeline
    svm_model = model.named_steps['svm'] if hasattr(model, 'named_steps') else model

    # Calcular la importancia de la permutaci√≥n
    result = permutation_importance(
        svm_model, X_transformed, y,
        n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1
    )

    # Create a DataFrame for the importance scores
    importance_df = pd.DataFrame({
        'Caracter√≠stica': X.columns,
        'Importancia': result.importances_mean,
        'Std': result.importances_std
    })

    # Ordenar por importancia
    importance_df = importance_df.sort_values(by='Importancia', ascending=False)

    return importance_df

# Plot de la importancia de las caracter√≠sticas
def plot_feature_importance(importance_df, title, top_n=15):
    plt.figure(figsize=(10, 8))

    # Obtener las N mejores funciones
    if len(importance_df) > top_n:
        plot_df = importance_df.head(top_n)
    else:
        plot_df = importance_df

    # Crear un gr√°fico de barras horizontales
    sns.barplot(x='Importancia', y='Caracter√≠stica', data=plot_df, palette='viridis')
    plt.title(f'{title} (Top {len(plot_df)} caracter√≠sticas)', fontsize=12, fontweight='bold', pad=15)
    plt.xlabel('Puntuaci√≥n de importancia', fontsize=10, labelpad=12)
    plt.ylabel('Caracter√≠stica', fontsize=10, labelpad=12)
    plt.tight_layout()
    plt.show()
    print("\n")

    return plot_df

# Calcular y graficar la importancia de las caracter√≠sticas
print("\nüîπ C√°lculo de la importancia de las caracter√≠sticas mediante la importancia de permutaci√≥n:")
perm_importance = calculate_permutation_importance(svm_best, X_test, y_test)
print(perm_importance)
print("\n")
plot_feature_importance(perm_importance, 'Importancia de las caracter√≠sticas de permutaci√≥n en SVM con n√∫cleo sigmoide')

import joblib
joblib.dump(svm_best, 'svm_sigmoid_model.pkl')

"""### 6.5. Comparaci√≥n de los modelos finales

Una vez entrenados y ajustados los distintos modelos de clasificaci√≥n, resulta fundamental realizar una evaluaci√≥n comparativa que permita identificar cu√°l de ellos presenta un mejor desempe√±o general frente al conjunto de prueba.

Para ello, lo primero que hemos decidido es emplear diversas m√©tricas de evaluaci√≥n: exactitud, precisi√≥n, exhaustividad (recall), especificidad y F1-score.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc
import joblib

# Constantes
FEATURES = ['win_sg', 'sqrt_hemo','sqrt_al','log_pcv','win_rc','log_sc']

# Definir constantes faltantes
TEST_SIZE = 0.25
RANDOM_STATE = 42

# Cargar los modelos pre-entrenados
try:
    lda_model = joblib.load('/content/lda_model.pkl')
    qda_model = joblib.load('/content/qda_model.pkl')
    svm_model = joblib.load('/content/svm_sigmoid_model.pkl')

    models = {
        'LDA': lda_model,
        'QDA': qda_model,
        'SVM (sigmoide)': svm_model
    }
    print("Todos los modelos se cargaron exitosamente.")

except Exception as e:
    print(f"Error al cargar modelos: {e}")
    exit(1)

X = df[FEATURES]
y = df['classification_ckd']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

print(f"\nüîπ Forma del conjunto de prueba: {X_test.shape}")

# Funci√≥n para evaluar todos los modelos
def evaluate_models(models, X_test, y_test):
    results = {}
    roc_data = {}

    print("\n"+"="*40)
    print("üìä Evaluaci√≥n de modelos")
    print("="*40)

    for name, model in models.items():
        print(f"\nüîç Evaluando: {name}")

        y_pred = model.predict(X_test)

        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)[:, 1]
        else:
            y_proba = None

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        specificity = recall_score(y_test, y_pred, pos_label=0)
        f1 = f1_score(y_test, y_pred)

        results[name] = {
            'Exactitud': round(accuracy, 4),
            'Precisi√≥n': round(precision, 4),
            'Exhaustividad': round(recall, 4),
            'Especificidad': round(specificity, 4),
            'F1 Score': round(f1, 4)
        }

        if y_proba is not None:
            fpr, tpr, _ = roc_curve(y_test, y_proba)
            roc_auc = auc(fpr, tpr)
            roc_data[name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}

        print("\nüìã Reporte de clasificaci√≥n:")
        print(classification_report(y_test, y_pred, digits=4))

    # Mostrar tabla comparativa mejorada
    df_results = pd.DataFrame(results).T
    df_results = df_results.sort_values(by="F1 Score", ascending=False)

    print("\nüîπ Comparaci√≥n de las m√©tricas de rendimiento de cada modelo:\n")

    # Mejorando el estilo de la tabla
    styled_table = df_results.style \
        .format("{:.4f}") \
        .set_properties(**{
            'text-align': 'center',
            'font-weight': 'bold',
            'border': '1px solid #ddd',
            'padding': '8px',
            'background-color': 'white',
            'color': 'black'
        }) \
        .set_table_styles([
            {'selector': 'th', 'props': [
                ('text-align', 'center'),
                ('background-color', '#457b9d'),
                ('color', 'white'),
                ('font-size', '12px'),
                ('border', '1px solid #000000')
            ]},
            {'selector': 'caption', 'props': [
                ('caption-side', 'top'),
                ('font-size', '16px'),
                ('color', '#000000')
            ]},
            {'selector': 'td', 'props': [
                ('border', '1px solid #000000'),
                ('color', 'black')
            ]}
        ])

    display(styled_table)
    print("\n")

    return results, roc_data

# Evaluar todos los modelos
results, roc_data = evaluate_models(models, X_test, y_test)

# Crear un DataFrame de comparaci√≥n
df_results = pd.DataFrame(results).T

# Determinar el mejor modelo seg√∫n la puntuaci√≥n F1
best_metric = 'F1 Score'
best_model = df_results[best_metric].idxmax()

def plot_comparison_bar_chart(df_results, title="Comparaci√≥n del rendimiento del modelo", y_min=0.5):
    plt.figure(figsize=(10, 6), facecolor='white')

    custom_palette = ['#f94144', '#f9844a', '#f9c74f', '#52b69a', '#1e6091']

    # Gr√°fico de barras con la paleta
    ax = df_results.plot(
        kind='bar',
        figsize=(10, 6),
        color=custom_palette,
        edgecolor='black',
        width=0.7
    )

    # Etiquetas de valores en las barras
    for container in ax.containers:
        ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=9, padding=2)

    plt.title(title, fontsize=16, weight='bold', pad=20)
    plt.ylabel('Puntuaci√≥n', fontsize=12, labelpad=12)
    plt.xlabel('Modelo', fontsize=12, labelpad=12)
    plt.xticks(rotation=0, fontsize=11, fontweight='bold')
    plt.yticks(fontsize=10)
    plt.ylim([y_min, 1.01])

    plt.grid(axis='y', linestyle='--', alpha=0.4)

    # Cambiar la posici√≥n de la leyenda para que ocupe todo el ancho en la parte inferior
    legend = plt.legend(
        title='M√©tricas',
        fontsize=10,
        loc='upper center',
        bbox_to_anchor=(0.5, -0.15),
        frameon=True,
        ncol=5,
        shadow=True,
        fancybox=True
    )

    legend.get_title().set_fontweight('bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Ajustar para dejar espacio a la leyenda
    plt.show()

# Generar visualizaciones de comparaci√≥n con los cambios solicitados
plot_comparison_bar_chart(df_results)

"""Adem√°s de las m√©tricas tradicionales, es importante analizar la capacidad discriminativa de los modelos mediante las curvas ROC. Estas curvas permiten visualizar el compromiso entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) para distintos umbrales de decisi√≥n. Acompa√±adas del valor del AUC, brindan una visi√≥n clara del rendimiento global de cada modelo."""

def plot_roc_curves(roc_data, title="Comparaci√≥n de la curva ROC"):
    plt.figure(figsize=(8, 6), facecolor='white')

    # Paleta de colores
    palette = ['#05668d', '#90be6d', '#c77dff']

    for (name, data), color in zip(roc_data.items(), palette):
        plt.plot(
            data['fpr'], data['tpr'],
            label=f'{name} (AUC = {data["auc"]:.3f})',
            linewidth=2.5,
            color=color
        )

    # L√≠nea de referencia diagonal
    plt.plot([0, 1], [0, 1], color='#9b2226', linestyle='--', lw=2,
             label='Rendimiento aleatorio')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Tasa de falsos positivos', fontsize=10, labelpad=12)
    plt.ylabel('Tasa de verdaderos positivos', fontsize=10, labelpad=12)
    plt.title(title, fontsize=15, weight='bold', pad=16)
    plt.grid(alpha=0.3, linestyle='--')
    plt.legend(loc='lower right', fontsize=10, frameon=True, shadow=True)

    plt.tight_layout()
    plt.show()

plot_roc_curves(roc_data)

"""Finalmente, para complementar el an√°lisis comparativo, se visualizan las matrices de confusi√≥n de cada modelo. Estas matrices permiten observar detalladamente la distribuci√≥n de aciertos y errores, diferenciando entre verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos."""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from matplotlib.colors import ListedColormap

def plot_confusion_matrices(models, X_test, y_test, class_names=None):
    if class_names is None:
        class_names = ['noCKD', 'CKD']

    n_models = len(models)
    fig, axes = plt.subplots(1, n_models, figsize=(n_models * 5.5, 5.5), facecolor='white')

    # Asegurar que axes sea iterable
    if n_models == 1:
        axes = [axes]

    for i, (name, model) in enumerate(models.items()):
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)

        ax = axes[i]

        colors = ['#b7b7a4', '#b7b7a4', '#588157', '#14213d']
        cmap = ListedColormap(colors)

        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False,
                    annot_kws={'size': 14, 'weight': 'bold', 'color': 'white'}, square=True,
                    xticklabels=class_names, yticklabels=class_names, ax=ax, linewidths=1, linecolor='white')

        ax.set_xlabel('Diagn√≥stico previsto', fontsize=12, labelpad=10)
        ax.set_ylabel('Diagn√≥stico real', fontsize=12, labelpad=10)
        ax.set_title(f'{name}', fontsize=14, weight='bold', pad=10)
        ax.tick_params(axis='both', which='major', labelsize=11)

    fig.suptitle("Matrices de confusi√≥n por modelo", fontsize=18, weight='bold', y=1.05)
    plt.tight_layout()
    plt.show()

plot_confusion_matrices(models, X_test, y_test)

"""Despu√©s de evaluar y comparar el desempe√±o de todos los modelos, hemos decido identificar el modelo con el mejor rendimiento seg√∫n la m√©trica F1 Score y mostrar un resumen detallado de sus principales m√©tricas. Proporcionando as√≠ una visi√≥n integral de su efectividad en la clasificaci√≥n."""

print("\n" + "="*46)
print(f"ü•á Resumen del mejor modelo: {best_model}")
print("="*46)
print(f"M√©tricas de rendimiento:")
for metric, value in df_results.loc[best_model].items():
    print(f"- {metric}: {value:.4f}")

if best_model in roc_data:
    print(f"- AUC: {roc_data[best_model]['auc']:.4f}")

"""El modelo SVM con kernel sigmoide demuestra ser el modelo m√°s efectivo para la clasificaci√≥n de ERC, destac√°ndose especialmente por su capacidad para identificar correctamente los casos positivos (recall), sin comprometer la precisi√≥n global del modelo. Esta caracter√≠stica resulta fundamental en contextos cl√≠nicos, donde los falsos negativos pueden tener consecuencias cr√≠ticas.

Si bien la diferencia de rendimiento entre los tres modelos evaluados fue estad√≠sticamente significativa, incluso el modelo m√°s simple (LDA) mostr√≥ resultados cl√≠nicamente √∫tiles, con una exactitud del 94% y una precisi√≥n perfecta en la detecci√≥n de casos positivos, lo que lo convierte en una opci√≥n v√°lida cuando se prioriza la interpretabilidad y eficiencia computacional.

QDA, por su parte, present√≥ un equilibrio notable entre simplicidad y desempe√±o, mostrando un rendimiento robusto en m√©tricas clave como accuracy, recall y F1-score. Su capacidad para modelar covarianzas distintas por clase le permiti√≥ adaptarse mejor a la heterogeneidad propia de los datos cl√≠nicos.

LDA, aunque limitado por suponer fronteras lineales y covarianzas iguales entre clases, constituye una base s√≥lida y f√°cilmente interpretable para la clasificaci√≥n inicial, siendo ideal para entornos con recursos computacionales limitados o cuando se requiere una explicaci√≥n clara del proceso diagn√≥stico.

**Consideraciones y perspectiva a futuro:**

La elecci√≥n final del modelo depender√° del balance entre rendimiento diagn√≥stico, interpretabilidad cl√≠nica y recursos computacionales disponibles en el entorno de aplicaci√≥n.

En escenarios cl√≠nicos donde la transparencia del modelo es prioritaria, LDA o QDA podr√≠an ser preferidos. Sin embargo, cuando el objetivo es maximizar la detecci√≥n de casos, como por ejemplo en fases tempranas del cribado poblacional, modelos m√°s sofisticados como SVM son altamente recomendables.

Adem√°s, consideramos que dado el s√≥lido rendimiento individual de cada modelo, una estrategia de ensamblado (ensemble), como  stacking, podr√≠a representar un paso evolutivo relevante. Este enfoque permitir√≠a combinar las fortalezas particulares de cada t√©cnica: la flexibilidad de QDA, la capacidad no lineal de SVM y la estabilidad interpretativa de LDA. En conjunto, ello podr√≠a mejorar a√∫n m√°s la capacidad predictiva, captando interacciones complejas y reduciendo la varianza del modelo final.

**Comparaci√≥n t√©cnica de los modelos:**

*   LDA: asume covarianzas iguales y separaci√≥n lineal entre clases. Su rendimiento se ve limitado en presencia de relaciones no lineales o clases con diferentes estructuras de dispersi√≥n.
*   QDA: relaja la restricci√≥n de covarianza, permitiendo bordes de decisi√≥n cuadr√°ticos. Esto le otorga mayor flexibilidad y lo hace m√°s adecuado para conjuntos de datos cl√≠nicos diversos.
*   SVM con kernel sigmoide: no impone supuestos sobre la distribuci√≥n de los datos. Construye fronteras no lineales √≥ptimas, lo que lo hace especialmente √∫til frente a solapamientos o presencia de valores at√≠picos.

En el contexto de la clasificaci√≥n de ERC, donde las relaciones entre variables pueden ser complejas, no lineales y heterog√©neas, es esperable que modelos como QDA y SVM superen consistentemente a LDA.

## 7. Ensemble learning

### 7.1. Stacking Ensemble (Ensamblado en Pilas)

El Stacking Ensemble, tambi√©n conocido como stacked generalization, es una t√©cnica de aprendizaje supervisado que combina m√∫ltiples modelos base para mejorar el rendimiento predictivo. Esta t√©cnica permite integrar modelos heterog√©neos, aprovechando sus fortalezas individuales para obtener una predicci√≥n final m√°s robusta, y se considera particularmente eficaz cuando se busca reducir tanto el sesgo como la varianza del modelo global.

El fundamento te√≥rico radica en que stacking se organiza en dos niveles:
1. Modelos de nivel base (nivel 0): diversos modelos (regresi√≥n lineal, SVM, √°rboles, etc.) se entrenan sobre el conjunto de entrenamiento original.
2. Modelo meta (nivel 1): un nuevo modelo (llamado meta-modelo o blender) se entrena usando como entrada las predicciones generadas por los modelos de nivel base. Este modelo aprende a combinar dichas predicciones para generar el resultado final.

La arquitectura general puede representarse as√≠:

<pre><code>Entrada ‚Üí Modelos base ‚Üí Predicciones base ‚Üí Meta-modelo ‚Üí Predicci√≥n final</code></pre>

Para evitar el sobreajuste durante el entrenamiento del meta-modelo, t√≠picamente se utiliza un esquema de validaci√≥n cruzada para generar las predicciones del nivel base. Esto garantiza que las predicciones utilizadas como entrada del modelo meta sean realistas, es decir, provengan de modelos entrenados en datos distintos a los evaluados.

Algunos factores que debemos considerar son los siguientes:
*   Ventajas:
    *   Combinaci√≥n de fortalezas: al incorporar modelos diferentes, stacking puede capturar patrones que un √∫nico modelo no detectar√≠a.
    *   Reducci√≥n del error general: mejora tanto el sesgo (por usar modelos complejos) como la varianza (por integrar modelos simples), equilibrando los errores t√≠picos del aprendizaje autom√°tico.
    *   Alta flexibilidad: permite usar cualquier combinaci√≥n de modelos base y cualquier tipo de meta-modelo.
*   Limitaciones:
    *   Complejidad computacional: entrenar varios modelos y coordinar sus salidas requiere mayor capacidad computacional y m√°s tiempo.
    *   Riesgo de sobreajuste: si no se maneja adecuadamente la validaci√≥n cruzada para generar las predicciones del nivel base, el meta-modelo puede sobreajustarse f√°cilmente a los datos de entrenamiento.
    *   Dif√≠cil interpretaci√≥n: debido a su arquitectura en m√∫ltiples capas, los modelos de stacking son menos interpretables que modelos individuales, lo cual puede ser una desventaja en contextos donde se requiere explicabilidad.

La elecci√≥n del enfoque Stacking se fundamenta en su capacidad para integrar modelos base de naturaleza diversa (como SVM, LDA y QDA), aprovechando de forma sin√©rgica sus respectivas fortalezas. Esta t√©cnica de ensamblado se caracteriza por su estructura jer√°rquica, en la que un modelo meta-aprendiz es entrenado para combinar las predicciones de los modelos base, con el objetivo de mejorar el rendimiento general del sistema.

El principal atractivo del stacking radica en su habilidad para capturar patrones complementarios que individualmente podr√≠an ser pasados por alto por modelos espec√≠ficos. Por ejemplo, como hemos mencionado anteriormente, en nuestro caso podriamos fusionar la flexibilidad de QDA, la capacidad no lineal de SVM y la estabilidad interpretativa de LDA.

Todas esas razones lo convierten en un enfoque especialmente valioso en problemas complejos y heterog√©neos como el que presenta nuesto contexto cl√≠nico.

Una vez comprendida la teor√≠a que sustenta el enfoque de Stacking Ensemble, procedemos a su implementaci√≥n pr√°ctica en el contexto de clasificaci√≥n binaria, utilizando modelos previamente entrenados: LDA, QDA y SVM con kernel sigmoidal. El objetivo es combinar estos clasificadores base mediante un meta-modelo (`GradientBoostingClassifier`) con el fin de mejorar la capacidad predictiva global.

El proceso completo de implementaci√≥n se estructura en varias etapas fundamentales:
1. Carga de modelos preentrenados: partimos de los tres clasificadores previamente entrenados y almacenados en disco. Estos modelos act√∫an como estimadores base dentro del ensamblado.
2. Preparaci√≥n del conjunto de datos: el conjunto de datos original es segmentado en caracter√≠sticas predictoras (X) y variable objetivo (y). Posteriormente, se realiza una partici√≥n estratificada en conjuntos de entrenamiento y prueba, asegurando que la proporci√≥n entre clases se mantenga constante en ambas divisiones.
3. Balanceo de clases: con el objetivo de mitigar el sesgo inducido por un desbalance de clases, se aplica la t√©cnica Synthetic Minority Over-sampling Technique (`SMOTE`). Este m√©todo genera muestras sint√©ticas de la clase minoritaria a partir de interpolaciones entre vecinos cercanos.
4. Normalizaci√≥n de caracter√≠sticas: antes del entrenamiento, las variables predictoras se escalan utilizando una normalizaci√≥n est√°ndar (`StandardScaler`), la cual transforma los datos para que cada caracter√≠stica tenga media cero y desviaci√≥n est√°ndar uno.
5. Construcci√≥n y entrenamiento del modelo de ensamblado: se construye un `StackingClassifier` utilizando los tres clasificadores base mencionados anteriormente como estimadores iniciales. La salida de estos modelos se utiliza para alimentar un modelo meta (en nuestro caso, una regresi√≥n log√≠stica) que aprende a combinar las predicciones de los modelos base de forma √≥ptima. El entrenamiento del ensamblado se realiza utilizando validaci√≥n cruzada interna (por defecto con 5 particiones), lo que permite generar metacaracter√≠sticas robustas sin requerir un conjunto de validaci√≥n expl√≠cito.
6. Evaluaci√≥n comparativa de modelos: finalmente, se realiza una evaluaci√≥n detallada de los modelos sobre el conjunto de prueba. En particular, se comparan las m√©tricas de desempe√±o del modelo SVM individual frente al ensamblado de stacking, permitiendo determinar si la combinaci√≥n de modelos mejora efectivamente la capacidad predictiva. Las m√©tricas consideradas incluyen exactitud, precisi√≥n, exhaustividad, especificidad, F1-score y el √°rea bajo la curva ROC (AUC).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, auc
import joblib
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import GradientBoostingClassifier

final_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=RANDOM_STATE)

# Constantes
RANDOM_STATE = 42
TEST_SIZE = 0.2
FEATURES = ['win_sg', 'sqrt_hemo','sqrt_al','log_pcv','win_rc','log_sc']

# Cargar los modelos pre-entrenados
try:
    lda_model = joblib.load('lda_model.pkl')
    qda_model = joblib.load('qda_model.pkl')
    svm_model = joblib.load('svm_sigmoid_model.pkl')
    print("Todos los modelos se cargaron correctamente.")
except Exception as e:
    print(f"Error al cargar modelos: {e}")
    exit(1)

# Preparando el dataset
X = df[FEATURES]
y = df['classification_ckd']

# Dividir datos con el mismo random state que se utiliz√≥ durante el entrenamiento
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

print(f"\nüîπ Forma del conjunto de entrenamiento: {X_train.shape}")
print(f"üîπ Forma del conjunto de prueba: {X_test.shape}")

# Extraer el clasificador real de los pipelines si es necesario
def extract_classifier(model):
    if hasattr(model, 'named_steps'):
        for step_name in ['lda', 'qda', 'svm']:
            if step_name in model.named_steps:
                return model.named_steps[step_name]
    return model

# Clasificadores de extracci√≥n
lda_clf = extract_classifier(lda_model)
qda_clf = extract_classifier(qda_model)
svm_clf = extract_classifier(svm_model)

# Aplicar preprocesamiento antes de apilar (en lugar de anidar pipelines)
# Aplicar SMOTE para el equilibrio de clases
smote = SMOTE(random_state=RANDOM_STATE)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Escalar las caracter√≠sticas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)  # Utilizar el mismo escalador para los datos de prueba

# Crear un conjunto de apilamiento directamente con datos preprocesados
stacking_clf = StackingClassifier(
    estimators=[
        ('lda', lda_clf),
        ('qda', qda_clf),
        ('svm', svm_clf)
    ],
    final_estimator=final_model,
    cv=5  # Validaci√≥n cruzada de 5 pasos para generar metacaracter√≠sticas
)

# Entrenar el conjunto de apilamiento
stacking_clf.fit(X_train_scaled, y_train_resampled)

# Evaluar modelos SVM y de apilamiento
def evaluate_model(model, name, X_data, y_test, preprocess=False):
    print("\n"+"="*35)
    print(f"üîé Evaluando: {name}")
    print("="*35)

    # Aplicar preprocesamiento si es necesario
    X_eval = X_data
    if preprocess and hasattr(model, 'predict'):
        y_pred = model.predict(X_eval)
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_eval)[:, 1]
        else:
            y_proba = None
    else:
        # Predicci√≥n directa (para apilar clasificadores que ya tienen datos preprocesados)
        y_pred = model.predict(X_eval)
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_eval)[:, 1]
        else:
            y_proba = None

    # Calcular m√©tricas
    results = {
        'Exactitud': accuracy_score(y_test, y_pred),
        'Precisi√≥n': precision_score(y_test, y_pred),
        'Exhaustividad': recall_score(y_test, y_pred),
        'Especificidad': recall_score(y_test, y_pred, pos_label=0),
        'F1 Score': f1_score(y_test, y_pred)
    }

    # Imprimir m√©tricas
    print("M√©tricas:")
    for metric, value in results.items():
        print(f"- {metric}: {value:.4f}")

    # Imprimir informe de clasificaci√≥n
    print(f"\nüìã Informe de clasificaci√≥n para {name}:")
    print(classification_report(y_test, y_pred))

    # Matriz de confusi√≥n
    cm = confusion_matrix(y_test, y_pred)

    # Datos ROC
    roc_data = None
    if y_proba is not None:
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        roc_data = {
            'fpr': fpr,
            'tpr': tpr,
            'auc': roc_auc
        }
        print(f"- AUC: {roc_auc:.4f}")

    return results, cm, roc_data

# Evaluar ambos modelos
svm_results, svm_cm, svm_roc = evaluate_model(svm_model, "SVM (sigmoidal)", X_test, y_test, preprocess=True)
stacking_results, stacking_cm, stacking_roc = evaluate_model(stacking_clf, "Stacking Ensemble", X_test_scaled, y_test)

def plot_confusion_matrices(qda_cm, stacking_cm):
    # Crear una figura con subparcelas para QDA y matrices de confusi√≥n de apilamiento
    fig, axes = plt.subplots(1, 2, figsize=(10, 7), facecolor='white')

    from matplotlib.colors import ListedColormap

    colors = ['#b7b7a4', '#b7b7a4', '#588157', '#14213d']
    cmap = ListedColormap(colors)

    # Matriz de confusi√≥n de QDA
    sns.heatmap(qda_cm, annot=True, fmt='d', cmap=cmap, cbar=False,
                annot_kws={'size': 14, 'weight': 'bold', 'color': 'white'}, square=True,
                ax=axes[0], linewidths=1, linecolor='white')
    axes[0].set_xlabel('Diagn√≥stico previsto', fontsize=12, labelpad=10)
    axes[0].set_ylabel('Diagn√≥stico real', fontsize=12, labelpad=10)
    axes[0].set_title('SVM', fontsize=14, weight='bold', pad=20)
    axes[0].tick_params(axis='both', labelsize=12)

    # Matriz de confusi√≥n de conjuntos de apilamiento
    sns.heatmap(stacking_cm, annot=True, fmt='d', cmap=cmap, cbar=False,
                annot_kws={'size': 14, 'weight': 'bold', 'color': 'white'}, square=True,
                ax=axes[1], linewidths=1, linecolor='white')
    axes[1].set_xlabel('Diagn√≥stico previsto', fontsize=12, labelpad=10)
    axes[1].set_ylabel('Diagn√≥stico real', fontsize=12, labelpad=10)
    axes[1].set_title('Stacking Ensemble', fontsize=14, weight='bold', pad=20)
    axes[1].tick_params(axis='both', labelsize=12)

    # Aplicar tick_params a cada eje individualmente, no al array completo
    for ax in axes:
        ax.tick_params(axis='both', which='major', labelsize=11)

    fig.suptitle("Comparaci√≥n de matrices de confusi√≥n", fontsize=18, weight='bold', y=0.87)
    plt.tight_layout()
    plt.subplots_adjust(top=0.9, wspace=0.35)
    plt.savefig('svm_vs_stacking_cm.png', dpi=300)
    plt.show()

plot_confusion_matrices(svm_cm, stacking_cm)

def plot_roc_curves(qda_roc, stacking_roc):
    plt.figure(figsize=(7, 6), facecolor='white')

    # Plot la curva ROC de QDA
    plt.plot(
        qda_roc['fpr'], qda_roc['tpr'],
        label=f'SVM (AUC = {qda_roc["auc"]:.3f})',
        color='#90be6d', linewidth=3, linestyle='-'
    )

    # Plot la curva ROC de apilamiento
    plt.plot(
        stacking_roc['fpr'], stacking_roc['tpr'],
        label=f'Stacking Ensemble (AUC = {stacking_roc["auc"]:.3f})',
        color='#168aad', linewidth=3, linestyle='-'
    )

    # L√≠nea de referencia diagonal
    plt.plot([0, 1], [0, 1], color='#9b2226', linestyle='--', lw=2,
             label='Rendimiento aleatorio')

    # Personalizar los l√≠mites y las etiquetas de los ejes
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Tasa de falsos positivos', fontsize=10, labelpad=12)
    plt.ylabel('Tasa de verdaderos positivos', fontsize=10, labelpad=12)
    plt.title('Comparaci√≥n de la curva ROC', fontsize=16, weight='bold', pad=18)

    # Personalizar la leyenda
    plt.legend(loc='lower right', fontsize=10, frameon=True, shadow=True)

    # Estilo de cuadr√≠cula para un mejor contraste visual
    plt.grid(alpha=0.3, linestyle='--')

    # A√±adir un dise√±o limpio y guarda el gr√°fico
    plt.tight_layout()
    plt.savefig('svm_vs_stacking_roc.png', dpi=300)
    plt.show()

plot_roc_curves(svm_roc, stacking_roc)

"""Adem√°s de las m√©tricas cl√°sicas de evaluaci√≥n, hemos incorporado una etapa de an√°lisis de importancia de caracter√≠sticas, espec√≠ficamente aplicada al modelo de Stacking Ensemble. Esta etapa permite identificar cu√°les variables tienen mayor influencia en el desempe√±o predictivo del modelo.

En este caso, hemos optado por utilizar el enfoque de `permutation_importance`, un m√©todo basado en la disminuci√≥n del rendimiento del modelo cuando se altera aleatoriamente una caracter√≠stica del conjunto de datos. Este es un modelo-agn√≥stica y mide directamente el impacto de cada variable en la m√©trica de inter√©s. En este an√°lisis, hemos utilizado la puntuaci√≥n F1 como criterio de evaluaci√≥n para capturar un equilibrio entre precisi√≥n y exhaustividad.


"""

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Establecer el estilo visual
sns.set_style("whitegrid")
plt.rcParams.update({'font.size': 11})

# Calcular la importancia de caracter√≠sticas mediante permutaci√≥n
def calculate_feature_importance(model, X, y, feature_names=None):
    """
    Calcula la importancia de las caracter√≠sticas usando permutation importance
    """

    # Realizar permutation importance
    result = permutation_importance(
        model, X, y,
        n_repeats=10,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        scoring='f1'  # Usar F1 score como m√©trica
    )

    # Crear un dataframe para facilitar el an√°lisis y visualizaci√≥n
    importances = pd.DataFrame(
        {'Feature': feature_names if feature_names is not None else [f'Feature {i}' for i in range(X.shape[1])],
         'Importance': result.importances_mean,
         'Std': result.importances_std
        })

    # Ordenar por importancia en orden descendente
    importances = importances.sort_values('Importance', ascending=False)

    return importances

# Calcular la importancia para el modelo stacking
stacking_importances = calculate_feature_importance(
    stacking_clf, X_test_scaled, y_test,
    feature_names=FEATURES
)

# Mostrar las caracter√≠sticas m√°s importantes
print("\nüîπ Top 10 caracter√≠sticas m√°s importantes:")
print(stacking_importances.head(10))
print("\n")

# Crear una paleta de colores basada en la importancia
n_features = min(15, len(stacking_importances))
importance_norm = (stacking_importances['Importance'][:n_features] -
                   stacking_importances['Importance'][:n_features].min()) / (
                   stacking_importances['Importance'][:n_features].max() -
                   stacking_importances['Importance'][:n_features].min())
colors = plt.cm.viridis(importance_norm)

# Visualizar la importancia de las caracter√≠sticas
plt.figure(figsize=(10, 8))

# Preparar los datos para la visualizaci√≥n (invertido para mostrar de mayor a menor de arriba a abajo)
features_reversed = stacking_importances['Feature'][:n_features][::-1]
importance_reversed = stacking_importances['Importance'][:n_features][::-1]

# Crear el gr√°fico de barras sin barras de error
bars = plt.barh(
    features_reversed,
    importance_reversed,
    color=colors[::-1],
    alpha=0.8,
    edgecolor='black',
    linewidth=1
)

# Agregar valores de importancia al final de cada barra
for i, (bar, importance) in enumerate(zip(bars, importance_reversed)):
    # Calcular la posici√≥n para el texto
    x_pos = bar.get_width() + 0.005  # Peque√±o margen
    y_pos = bar.get_y() + bar.get_height()/2
    plt.text(x_pos, y_pos,
             f'{importance:.4f}',
             va='center',
             fontweight='bold')

# A√±adir l√≠neas de cuadr√≠cula para facilitar la lectura
plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.xlabel('Importancia (disminuci√≥n en F1-score cuando se permuta la variable)', fontsize=12, labelpad=14)
plt.ylabel('Caracter√≠sticas', fontsize=12, labelpad=12)
plt.title('Importancia de caracter√≠sticas en el Modelo Stacking', fontsize=16, fontweight='bold', pad=20)


plt.tight_layout()
plt.margins(y=0.01)

"""A continuacion, haremos un an√°lisis de los resultados obtenidos. Lo primero que haremos es interpretar la importancia por caracter√≠stica:
*   `sqrt_al` (0.1551): la alb√∫mina (transformada mediante ra√≠z cuadrada) es la caracter√≠stica m√°s importante. Su permutaci√≥n causa una reducci√≥n del 15.51% en el F1-score, indicando que es crucial para el diagn√≥stico de enfermedad renal.
*   `win_sg` (0.1502): la gravedad espec√≠fica de la orina es casi tan importante como la alb√∫mina (15.02%), reflejando la capacidad de concentraci√≥n renal, deteriorada en la enfermedad.
*   `log_pcv` (0.0563): el volumen celular empaquetado (transformado logar√≠tmicamente) ocupa el tercer lugar con una importancia de 5.63%, reflejando la anemia asociada con enfermedad renal cr√≥nica.
*   `log_sc` (0.0521): la creatinina s√©rica tiene una importancia de 5.21%, siendo un biomarcador est√°ndar de la funci√≥n renal.
*   `sqrt_hemo` (0.0496): la hemoglobina tiene una importancia del 4.96%, complementando la informaci√≥n del PCV sobre el componente hematol√≥gico.
*   `win_rc` (0.0157): los recuentos celulares en orina muestran la menor importancia (1.57%), pero a√∫n contribuyen al modelo.

Por lo que hace al modelo Stacking, este combina las predicciones de varios modelos base mediante un "meta-modelo" que aprende a integrar estas predicciones. Este an√°lisis de importancia revela:
*   Consistencia con modelos individuales: la dominancia de alb√∫mina y gravedad espec√≠fica coincide con los hallazgos de QDA y otros modelos, lo que refuerza la robustez de estos biomarcadores como indicadores de enfermedad renal.
*   Distribuci√≥n bimodal de importancia: dos caracter√≠sticas dominantes (`sqrt_al` y `win_sg`) con aproximadamente 15% de importancia, un grupo intermedio (`log_pcv`, `log_sc`, `sqrt_hemo`) alrededor del 5%, y una caracter√≠stica de baja importancia (`win_rc`).
*   Integraci√≥n de informaci√≥n complementaria: el modelo valora tanto marcadores de da√±o glomerular (alb√∫mina) como de funci√≥n tubular (gravedad espec√≠fica), e incorpora par√°metros bioqu√≠micos (creatinina) y hematol√≥gicos (PCV, hemoglobina).

La utilidad cl√≠nica del modelo propuesto se manifiesta en varios aspectos fundamentales para el diagn√≥stico y la toma de decisiones m√©dicas en el contexto de la ERC:
*   Diagn√≥stico multidimensional: el an√°lisis confirma que un enfoque √≥ptimo para el diagn√≥stico de enfermedad renal debe incluir pruebas de albuminuria y evaluaci√≥n de la funci√≥n de concentraci√≥n renal.
*   Priorizaci√≥n de pruebas diagn√≥sticas: cuando los recursos son limitados, este an√°lisis sugiere priorizar la medici√≥n de alb√∫mina en orina y gravedad espec√≠fica. Los par√°metros hematol√≥gicos y la creatinina s√©rica proporcionan informaci√≥n complementaria importante.
*   Robustez del modelo ensamblado: la distribuci√≥n de importancia sugiere que el modelo Stacking aprovecha efectivamente m√∫ltiples dimensiones de informaci√≥n, mejorando la robustez de las predicciones, lo que puede explicar por qu√© los modelos de ensamblaje suelen superar a los modelos individuales en clasificaci√≥n m√©dica.

Finalmente, el an√°lisis destaca la importancia de un enfoque integral en la evaluaci√≥n de la enfermedad renal, con especial √©nfasis en los marcadores urinarios como pilares del diagn√≥stico, complementados por par√°metros bioqu√≠micos y hematol√≥gicos para una evaluaci√≥n completa.
"""

def plot_metrics_comparison(qda_results, stacking_results, y_min=0.5):
    # Crear un DataFrame para trazar gr√°ficos
    metrics_df = pd.DataFrame({
        'svm': qda_results,
        'Stacking Ensemble': stacking_results
    })

    plt.figure(figsize=(10, 6), facecolor='white')

    custom_palette = ['#90be6d', '#168aad']

    # Gr√°fico de barras con la paleta
    ax = metrics_df.plot(
        kind='bar',
        figsize=(10, 6),
        color=custom_palette,
        edgecolor='black',
        width=0.7
    )

    # Etiquetas de valores en las barras
    for container in ax.containers:
        ax.bar_label(container, fmt='%.2f', label_type='edge', fontsize=9, padding=2)

    plt.title('M√©tricas de rendimiento: SVM vs Stacking Ensemble', fontsize=15, weight='bold', pad=22)
    plt.ylabel('Puntuaci√≥n', fontsize=12, labelpad=12)
    plt.xlabel('M√©tricas', fontsize=12, labelpad=12)
    plt.xticks(rotation=0, fontsize=11, fontweight='bold')
    plt.yticks(fontsize=10)
    plt.ylim([y_min, 1.01])

    plt.grid(axis='y', linestyle='--', alpha=0.4)

    # Cambiar la posici√≥n de la leyenda para que ocupe todo el ancho en la parte inferior
    legend = plt.legend(
        title='Modelos',
        fontsize=10,
        loc='upper center',
        bbox_to_anchor=(0.5, -0.15),
        frameon=True,
        ncol=5,
        shadow=True,
        fancybox=True
    )

    legend.get_title().set_fontweight('bold')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Ajustar para dejar espacio a la leyenda
    plt.savefig('qda_vs_stacking_metrics.png', dpi=300)
    plt.show()

    return metrics_df

metrics_df = plot_metrics_comparison(svm_results, stacking_results)

# Imprimir tabla comparativa
print("\nüîπ Comparaci√≥n de m√©tricas de rendimiento:")
print(metrics_df)

"""Para finalizar el proyecto, hemos realizado una comparaci√≥n directa entre dos modelos de clasificaci√≥n: SVM con n√∫cleo sigmoidal y un modelo de Stacking Ensemble, ambos evaluados sobre el conjunto de prueba.

Analizando las matrices de confusi√≥n podemos observar que:
*   Support Vector Machine con kernel sigmoidal:
    *   Clasific√≥ correctamente todas las instancias de la clase negativa (0), es decir, no se produjeron falsos positivos.
    *   Sin embargo, present√≥ un √∫nico falso negativo, clasificando err√≥neamente una instancia de la clase positiva (paciente enfermo) como negativa.
    *   Resultado: 30 verdaderos negativos, 48 verdaderos positivos, 2 falsos negativos, 0 falsos positivos.
*   Stacking Ensemble:
    *   Clasific√≥ correctamente todas las instancias, tanto de la clase positiva como de la negativa.
    *   No se cometieron errores de clasificaci√≥n, lo cual refleja una capacidad de predicci√≥n excepcional.
    *   Resultado perfecto: 30 verdaderos negativos, 50 verdaderos positivos.

Aunque ambos modelos demostraron un rendimiento notable, el modelo Stacking Ensemble logr√≥ un desempe√±o perfecto, superando levemente al SVM al eliminar completamente los errores de clasificaci√≥n.

Por lo que hace a las m√©tricas de rendimiento:
*   El modelo SVM ofreci√≥ un rendimiento sobresaliente, destac√°ndose especialmente por su precisi√≥n, especificidad y AUC perfectas.
*   El modelo de Stacking Ensemble alcanz√≥ el 100% en todas las m√©tricas, lo que sugiere una capacidad de generalizaci√≥n superior y una mayor confiabilidad en contextos donde se deben evitar tanto falsos positivos como falsos negativos.

**Conclusi√≥n final:**

Ambos modelos presentaron un desempe√±o excepcional en la tarea de clasificaci√≥n binaria para la detecci√≥n de enfermedad renal cr√≥nica, evidenciando la calidad del conjunto de datos, el preprocesamiento aplicado y la selecci√≥n de caracter√≠sticas llevada a cabo en etapas anteriores del proyecto.

Sin embargo, el modelo de Stacking Ensemble demostr√≥ una ligera pero significativa ventaja al evitar completamente cualquier tipo de error de clasificaci√≥n. Esto lo convierte en la mejor opci√≥n para su aplicaci√≥n en entornos cl√≠nicos reales, donde la sensibilidad (identificaci√≥n de pacientes enfermos) y la precisi√≥n (evitar falsos positivos) son de vital importancia.

Adem√°s, al tratarse de un modelo basado en la combinaci√≥n de varios algoritmos base, el Stacking Ensemble logra capturar patrones complejos y complementar las debilidades de modelos individuales, ofreciendo un sistema m√°s robusto y preciso.

"""